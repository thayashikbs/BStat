[["index.html", "ビジネス統計 1 はじめに 1.1 コースの目的 1.2 統計学/統計的手法の学習について 1.3 ビジネス応用における統計学の最近の趨勢 1.4 本書内の記載の注意点", " ビジネス統計 林 高樹 2024-03-02 (内容は随時更新されます) 1 はじめに 1.1 コースの目的 近年の”データサイエンス”分野の発展の中で, その支柱の分野の一つとしての 統計学の重要性が高まっている. いまや, 簡単なデータ分析であれば, ChatGPTなどの生成AIが, 人間に代わって分析を行ったり, 分析のためのプログラムを書いたりしてくれる時代となった. しかしながら, 少なくとも現状の技術水準では生成AIの出力の 正確性は保証されておらず, 生成AIの出力の正しさを確認できるのは人間である. 当面は, 人間の手による分析, 人間の頭による分析方法論の正しい理解や結果の解釈が求められるだろう. AIブームより前に始まっていた”ビッグデータ”の時代において, 多種多様かつ大量の, 組織内・外のデータを分析する技術は 会社経営において決定的に重要となっている. 一方, 統計学やデータサイエンスの一人の初学者としては, いきなり 高度なITスキルを前提とする最新の機械学習系手法を駆使したビッグデータ“解析を行おうとするのでなく, まずは, 古典的な統計的方法論を正しく“スモール・データセット”に応用できるようになることが重要である. 本コースは, 多変量解析を中心にさまざまな統計的データ分析の手法を学び, これらの手法を学術研究や実務に応用できるようになるための基盤作り目指す. 統計ソフトウェアRを利用しながら学んでいく. 1.2 統計学/統計的手法の学習について 学習の目標 統計学/統計的手法は, サイエンスとアートの二つの側面があることから, 的確な応用を行うためには, 両面を同時に, バランスよく学ぶ必要がある. 統計理論を正確に理解する 数学的に正しい概念や手続きの理解 統計手法の実践 (運用法) を学ぶ 業務経験や知識, 統計的手法の経験則, 費用対効果等の判断 ※ 統計学は数学(の一分野)ではない (数学を道具として学問体系が作られいる) 一方の理解が不十分であると, 妥当な分析が行えず, 不正確あるいは間違った分析結果や解釈につながるリスクがある. 統計学/統計的手法の学習方法 3つの要素・ルート, それに対応した教科書・参考書がある. すなわち, 入門 (文章&amp;図表主体): 手法の概念, 用途, 特徴の大雑把な理解を図る 理論 (数式主体): 手法の理論的・技術的側面, 詳細の正確な理解を図る 実習 (コード主体): 手を動かすことで手法を経験, 実践力をつける 統計学を学習するにあたっては, 理解の段階に応じて, これらの要素を, 少しずつ万遍なく学びながら, “スパイラル”状に次の段階に進んでいくのが最も効果的であると筆者は考える. すなわち, 理論の学習を全くやらず, 入門と実習のみを学習するようなアプローチは, 理論的理解のないまま統計的分析を実践することになるため危険である. 筆者の経験上, プログラミングの得意な”エンジニア系”のデータサイエンティストにはそのような傾向を持つ人が少なからずいると感じている. 書籍ごとに目的や想定する読者層は異なり, それに対応するようにこれらの要素の割合が異なる. したがって, 学習者は自身の学習目的に照らして適切な本を選択する必要がある. 本書では, Rを用いながら代表的な統計手法を学んでいく. 統計学の教科書例 Rコードによる分析例を示しながら, 各手法や理論の解説を行っている書籍は 多数存在するが, バランス良くこれらを配置していると筆者が感じる教科書のタイトルを幾つか紹介する. 【統計学/R】 山田剛史, 杉澤武俊, 村井潤一郎 (2008), Rによるやさしい統計学, オーム社 【データ分析/R】 Kosuke Imai (2017), Quantitative Social Science: An Introduction, Princeton University Press (今井耕介(著), 粕谷祐子, 原田勝孝, 久保浩樹 (訳) (2018), 社会科学のためのデータ分析入門(上)(下), 岩波書店) 【機械学習/R】 R. James, G., Witten, D., Hastie, T., Tibshirani (2013), An Introduction to Statistical Learning: with Applications in R, Wiley. (James他(著), 落海浩, 首藤信通 (訳) (2018), Rによる統計的学習入門, 朝倉書店) 参考として, 次の書籍は, コードを載せずまた数式を使った説明も殆どなしに, 文章主体で (計量経済学の) 手法の概念や分析結果の解釈の仕方を平易に説明している良書である. 山本 勲 (2015), 実証分析のための計量経済学, 中央経済社 統計学/統計的手法の学習ステップ 入門・初級ステップ - レベル①: ソフトウェアを正しく動かせる - 目的に応じた適切な手法の選択 - 適切なデータの加工, ソフトウェアの操作 - 出力結果 (帳票, 図表) の正しい見方 - レベル②: 手法の背後にある理論を理解する - (②A) 概念や定義の正しい理解 (言葉やイメージ) - (②B) 数式による厳密な理解 ※ ①の達成度を高めるためには, ②の理解を高める必要 ① ⇒ ② ⇔ ① 中級ステップ - 特定の分野における (計量経済学, 心理学, 疫学, …) 統計的手法の理解と実践が出来るようになる 1.3 ビジネス応用における統計学の最近の趨勢 統計学, さらには中核分野として内包するデータサイエンス分野において扱う 対象データの特徴として以下のような傾向がみられる 大規模化 (“ビッグデータ”) レコード数 n → 大, 変数の数 p → 大 データ数より説明変数が多い場合も （“n&lt;p問題”) 従来の統計学: 「n 小・中規模」, かつ, 「n&gt;p」 高頻度・高速化 (従来) 四半期・月次… → 1日内, 秒, ミリ秒, …, リアルタイム 非構造化 画像, 音声, テキスト等 自動化 衛星画像, アクセスログ, IoTデータ, ウェアラブル・データ, … “マルチモーダル”化 テキスト・画像・音声・動画など複数の種類のデータを一括して処理 (AIによる)自動生成 一方, 経営(学)分野への応用の観点では次のような傾向がある. 文章や発言内容の自然言語処理・テキスト解析技術の重要性の高まり BERT, GPT-4, … 生成系AI技術の活用 文章 (ChatGPT, BARD), 画像 (DALLE, Stable Solution), 音楽 (Stable Audio, Suno), 等 複数のデータソースの有機的な組合せ活用の重要性 財務諸表等の“ハードデータ” × SNS等から得られた“ソフトデータ” 外部ソース・データ ×社内業務データ … 1.4 本書内の記載の注意点 読者への注) パス名は、各自のPC環境に応じて適宜変更すること "],["r言語の基本.html", "2 R言語の基本 2.1 Rの基本プログラミング 2.2 データの型や構造 2.3 データの操作・演算 2.4 R関数 2.5 データの可視化 2.6 ファイル入出力", " 2 R言語の基本 2.1 Rの基本プログラミング 主な参考書籍: 金, 『Rによるデータサイエンス』, 森北出版 山田他, 『Rによるやさしい統計学』, オーム社 http://minato.sip21c.org/swtips/R-jp-docs/R-intro-170.jp.pdf 本コースは, 基本的なRプログラミングにもっぱら限定 よりモダンなプログラミング (本コース終了後) → tidyverse 例. 松村他, 『RユーザーのためのRStudio[実践]入門]』, 技術評論社 (中級者以上) Rコーディングスタイルを気にすると良い 例. Google’s R Style Guide: https://google.github.io/styleguide/Rguide.html 2.1.1 基本操作 数値 (ベクトル), 演算の直接評価 2 + 3 ## [1] 5 c(1, 2, 3, 4) ## [1] 1 2 3 4 1:4 ## [1] 1 2 3 4 変数xに値を格納. 変数xに対する演算 - 基本形: 変数名 &lt;- 代入する値 x &lt;- c(1, 2, 3, 4, 5) x = c(1, 2, 3, 4, 5) x ## [1] 1 2 3 4 5 (x &lt;- c(1, 2, 3, 4, 5)) # 代入と表示を同時に実行 ## [1] 1 2 3 4 5 x^2 ## [1] 1 4 9 16 25 x**2 ## [1] 1 4 9 16 25 xに関数を適用 mean(x) ## [1] 3 var(x) ## [1] 2.5 sd(x) ## [1] 1.581139 summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1 2 3 3 4 5 # sqrt, summary, ... その他, R言語の基本 - 空白は無視される - Pythonと異なり, インデントは意味を持たない - 一行に二つのコマンドを入力する場合は, 間をセミコロン (;) で区切る - 行頭がシャープ (#) で始まる行は丸々無視される (コメント行) - 中括弧 {} は通常、コードのブロックを作成するために使用. 主に, 条件文、ループ、関数などのブロック構造を定義する際に使用. 2.1.2 基本構文 Rでは, データの加工や分析を行う際などに, 分析者自らの手で処理の手順をプログラミングをすることができる. forループ # 繰り返し処理 (forループ) for (変数名 in 変数のリスト){ 1回分の処理内容 } ``` #### if文 {-} ``` # 条件分岐 (if文) # if (条件式) 処理1 else 処理2 x &lt;- 0 for(i in 1:10){x &lt;- x + i} x # aaa &lt;- c(1, 3, 5) for (a in aaa) print (a) ## [1] 55 ## [1] 1 ## [1] 3 ## [1] 5 2.1.3 自作関数 同様な処理を”パラメータ”を変えながら何度も実行する場合は, 関数を作っておくと便利である. ※ 関数に付与する名前として, Rですでに使われている関数名や, Rで特別な意味を持つ値 (T, Fなど)は避けること # 自作関数の作成 関数名 &lt;- function(引数1, 引数2, ...){ 処理内容 } myfunc &lt;- function(y){ x &lt;- 0 for (i in 1:y) x &lt;- x + i return(x) } # 実行例 myfunc(10) myfunc2 &lt;- function(y){ if(y &gt; 10) print(&quot;yes&quot;) else print(&quot;no&quot;) } # 実行例 myfunc2(5) ## [1] 55 ## [1] &quot;no&quot; 2.1.4 パッケージのインストール &amp; 読み込み #lda # lda関数 → このままだエラー発生 library() # インストール済パッケージ一覧 search() # 読み込み済みパッケージ一覧 library(MASS) # MASSパッケージの読み込み(ロード) search() # アタッチされたパッケージのリスト表示 lda install.packages(&quot;DAAG&quot;) # http://cran.r-project.org # http://cran.r-project.org/web/packages/googleVis/index.html # パッケージインストローラー 2.1.5 ヘルプ 関数のヘルプ R関数helpを使うか, RStudioのプルダウンメニューやHelpペインを使用 help(&quot;fivenum&quot;) # 関数fivenumのヘルプ ?fivenum 2.2 データの型や構造 ここで, R言語の基礎を理解するのに重要な二つの概念について, 初心者を念頭に正確性を犠牲にしながら概要について述べる. 実際はここでの記載よりもはるかに複雑で, 技術的にも難易度が高い. 包括的かつ技術的に正確な内容については, 例えば, https://adv-r.hadley.nz を参照されたい. 2.2.1 データの値の種類 (“データ型”) Rでは, データの取る値の主要な種類 (type) として, 実数型 (double), 整数型 (integer), 文字列型 (character), 論理型 (logical) がある. また, 実数型, 整数型はまとめて数値型 (numeric) とも呼ばれる. 初心者は, 実数型と整数型の違いは気にしなくても良い. # 実数型 3.14 2.718 # 整数型 1L 5L # 文字列型 &quot;KBS&quot; &quot;日吉&quot; # 論理型 TRUE # または, T FALSE # または, F 次に, データの値の種類として, 上記以外に応用上知っておきたいものとして, 因子型 (factor),日付型 (Date)がある. 因子型は, カテゴリーデータに対して, 日付型は日付や時刻を表すデータに対して使うことができる. Rでは, カテゴリーデータ (ベクトル) を因子型としてオブジェクトに格納しておけば, その後の統計分析においてわざわざダミー変数を作る操作は (おおむね) 不要となる. また, 日付型として格納したデータは日付や時間に関する処理において効果を発揮する. 与えられたデータに対して, R組み込み関数である factor(), as.Date() を適用することでこれらの型に変換することができる. 少しだけ発展的な内容になるが, 因子型は整数型を値に持つベクトル, 日付型は実数型を値に持つベクトルとしてR内部で扱われる (ベクトルやオブジェクトについては次に述べる). # 因子型 factor(c(&quot;L&quot;, &quot;M&quot;, &quot;H&quot;, &quot;M&quot;, &quot;L&quot;, &quot;M&quot;)) # L/M/Hの3水準の因子型ベクトル (長さ5) # 日付型 as.Date(&quot;2023-10-02&quot;) 2.2.2 データの配列の仕方 (“データ構造”) Rでは, データの配置の仕方の種類の主要なものとして, ベクトル (vector), リスト (list), 行列 (matrix), 配列(array), データフレーム (data frame) などがある. 分析に応じて, 適切なデータの構造にして処理を行う必要がある. # ベクトル c(3.14, 2.718) c(&quot;KBS&quot;, &quot;日吉&quot;) # リスト list(&quot;KBS&quot;, 1962L, 1:10) # 行列 matrix(1:8, nrow = 2, byrow = T) # 配列 arra(1:12, c(2, 3, 2)) # データフレーム data.frame(name = c(&quot;Steve&quot;, &quot;Top&quot;), income = c(40000, 50000)) ちなみに, これらの”データ構造”には階層関係があり, 行列や配列はベクトルの特別な場合, リストはベクトルの特別な場合, データフレームはリストの特別な場合である. データフレームは, リスト (異なる種類のデータを同時に要素として持つ) でありながら, リストの各要素 (ベクトル) の長さが等しく, 2次元の行列の形式にデータが並べられたものである. R言語では, ベクトルが最も基本的な”データ構造”である. Rを用いた統計分析では, データフレームを用いるケースが非常に多いため, データフレームを使えるようになることが必須である. Rで分析を行う場合には, データや関数 (データ処理するための手続きを書いたコード) をオブジェクト (object) と呼ばれる”箱”に名前を付けて一旦格納し, その名前を呼び出す形で処理を実行するのが便利である. 量的変数や質的変数を同時に持つデータセットの分析には, データフレームが便利である. オブジェクトにはクラス (class) というオブジェクトの持つデータ構造の種類の属性が付与される. なお, Rには, type, class, modeの3つの”型”が存在し混乱しやすい. 初心者は違いを気にする必要はなく, 大雑把に, 上の”データ型”は関数 typeof(), “データ構造”は関数 class() により調べることができると知っていれば十分である. 興味のある読者は以下を参照: https://cran.r-project.org/doc/manuals/r-release/R-lang.html#Attributes 特別な値 Rにおける分析において注意や対処が必要な, データの取り得る特別な値として, - NA (欠損値) - NULL (非存在) - NaN (非数値) - Inf (無限大) これらの値をテストする関数が用意されている. is.na() is.null() is.nan() is.infinite() is.finite() # NAの含まれている例 x &lt;- c(1, NA, 3, 4, 5) x == NA ## [1] NA NA NA NA NA is.na(x) ## [1] FALSE TRUE FALSE FALSE FALSE mean(x) ## [1] NA mean(x, na.rm = T) ## [1] 3.25 # NULLの含まれている例 x &lt;- 1:5 names(x) &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;) x ## a b c d e ## 1 2 3 4 5 names(x) &lt;- NULL x ## [1] 1 2 3 4 5 # NaN, Infの発生例 0 / 0 ## [1] NaN 1 / 0 ## [1] Inf 上の”データ型”や”データ構造”を調べる関数も用意されている. # 整数値を持つ行列の例 abc &lt;- matrix(1:8, nrow = 2, byrow = T) is.numeric(abc) ## [1] TRUE is.integer(abc) ## [1] TRUE is.matrix(abc) ## [1] TRUE typeof(abc) ## [1] &quot;integer&quot; class(abc) ## [1] &quot;matrix&quot; &quot;array&quot; mode(abc) ## [1] &quot;numeric&quot; str(abc) ## int [1:2, 1:4] 1 5 2 6 3 7 4 8 2.2.3 ベクトル # 以下は, 互いに等価 aaa &lt;- c(2, 4, 6, 8) # 変数aaaに数値ベクトル(2,4,6,8)を割り当てる aaa = c(2, 4, 6, 8) aaa = seq(2, 8, 2) c(2, 4, 6, 8) -&gt; aaa aaa &lt;- 1:4 * 2 # assign(&quot;aaa&quot;, c(2, 4, 6, 8)) # 値を割り当てる際に, &quot;環境&quot;を指定することができる # ベクトルの長さ length(aaa) # 文字列ベクトル bbb &lt;- c(&quot;東京&quot;, &quot;埼玉&quot;, &quot;千葉&quot;, &quot;神奈川&quot;) # ベクトルの各要素に名前 (ラベル) を付与 names(aaa) &lt;- bbb # ベクトル要素の取り出し bbb[1] bbb[c(2, 4)] bbb[c(T, F, T, F)] # インデックスの値がT (TRUE) の要素の取り出し bbb[c(T, F, F)] # 注意 ## [1] 4 ## [1] &quot;東京&quot; ## [1] &quot;埼玉&quot; &quot;神奈川&quot; ## [1] &quot;東京&quot; &quot;千葉&quot; ## [1] &quot;東京&quot; &quot;神奈川&quot; 2.2.4 行列 matrix(0, 3, 4) # 全要素0の3x4-行列 matrix(0:4, 3, 4) # 行列の値に使うベクトル (0:4) の長さと 行数 (3)・列数 (4) が不一致 ## Warning in matrix(0:4, 3, 4): data length [5] is not a sub-multiple or multiple ## of the number of rows [3] ## [,1] [,2] [,3] [,4] ## [1,] 0 0 0 0 ## [2,] 0 0 0 0 ## [3,] 0 0 0 0 ## [,1] [,2] [,3] [,4] ## [1,] 0 3 1 4 ## [2,] 1 4 2 0 ## [3,] 2 0 3 1 ccc &lt;- matrix(c(3, 2, 1, 6, 5, 4), 2, 3) # 2x3-行列 ccc[1, 1] # (1, 1)成分 ## [1] 3 ccc[1, ] # 第1行(行ベクトル) ## [1] 3 1 5 ccc[, 1] # 第1列(列ベクトル) ## [1] 3 2 ccc[-2, ] # 2行目を除く → 2x4-行列 ## [1] 3 1 5 ccc[, -2] # 2列目を除く → 3x3-行列 ## [,1] [,2] ## [1,] 3 5 ## [2,] 2 4 dim(ccc); nrow(ccc); ncol(ccc) # セミコロン(;)により, 複数のコマンドを1行に収め, 順次実行 ## [1] 2 3 ## [1] 2 ## [1] 3 ccc[2, 3] &lt;- 10 # (2, 3)成分に値10を代入 # 行列にラベルを付与 colnames(ccc) &lt;- c(&quot;大阪&quot;, &quot;京都&quot;, &quot;名古屋&quot;) # 列ラベル rownames(ccc) &lt;- c(&quot;2012&quot;, &quot;2013&quot;) # 行ラベル ccc ## 大阪 京都 名古屋 ## 2012 3 1 5 ## 2013 2 6 10 t(ccc) # 転置 ## 2012 2013 ## 大阪 3 2 ## 京都 1 6 ## 名古屋 5 10 2.2.5 リスト ベクトル, 行列, 配列, リスト等の異なる型(&amp;異なる長さ)のオブジェクトを一つにまとめたオブジェクト L1 &lt;- list(rep(&quot;A&quot;, 3), 1:0, matrix(1:8, 2, 4)) L1[[1]] # 1番目の要素(変数)の取り出し k &lt;- list (name = &quot;Taro&quot;, salary = 50000, male = T) k2 &lt;- list (&quot;Taro&quot;, 50000, T) # 要素名 (タグ)なしの場合 k$sal # 要素名は省略形可 # リストはベクトルの一種 (recursive vector) # 一方, 通常のベクトルはatomic vector (それ以上分解できない) # vector()からリスト生成する場合 z &lt;- vector (mode = &quot;list&quot;) z[[&quot;abd&quot;]] &lt;- 5 k[1:2] # 元のリストの部分リスト k2 &lt;- k[2] class(k2); str(k2) k2a &lt;- k[[2]] # 2番目の要素(変数)の取り出し (要素の型を持つ結果を返す) # k[[1:2]] # --&gt; エラー class(k2a); str(k2a) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; ## [1] 50000 ## $name ## [1] &quot;Taro&quot; ## ## $salary ## [1] 50000 ## ## [1] &quot;list&quot; ## List of 1 ## $ salary: num 50000 ## [1] &quot;numeric&quot; ## num 50000 リストの要素追加・削除 z &lt;- list(a = &quot;abcd&quot;, b = 10) z$c &lt;- &quot;piano&quot; z[[4]] &lt;- 15 z[5:6] &lt;- c(TRUE, FALSE) z$b &lt;- NULL # xxx &lt;- 1:10 yyy &lt;- 0.5 * xxx + rnorm(10) lm_res &lt;- lm(yyy ~ xxx) is.list(lm_res) lm_res[[1]] lm_res$coef lm_res[&quot;coefficients&quot;] ## [1] TRUE ## (Intercept) xxx ## -0.1976077 0.5539141 ## (Intercept) xxx ## -0.1976077 0.5539141 ## $coefficients ## (Intercept) xxx ## -0.1976077 0.5539141 2.2.6 データフレーム リストの特別な場合 長さが等しい複数のベクトルを要素に持つリスト 数値と文字列などの異なるデータが混在するデータを行列のように扱える Rにおける様々な統計分析において多用される kids &lt;- c(&quot;taro&quot;, &quot;hanako&quot;) ages &lt;- c(10, 8) d &lt;- data.frame(kids, ages, stringsAsFactors = FALSE) # 注: stringsAsFactors = T: 文字ベクトルをfactorとして扱う d str(d) # 以下の3つは等価な操作 d[[1]] # データフレームの第1列 (リストの一番目の要素) を取り出す(→ 文字列ベクトル) d$kids # 変数(kids)のように取り出す d[, 1] # 行列のように操作 (--&gt; 便利) # ただし, d[1] # 第1列をデータフレーム (リスト) として取り出す df1 &lt;- data.frame(letters[1:3], 3:1) rownames(df1) &lt;- c(&quot;大阪&quot;, &quot;京都&quot;, &quot;名古屋&quot;) colnames(df1) &lt;- c(&quot;方言種類&quot;, &quot;順位&quot;) class(df1) is.vector(df1) ## kids ages ## 1 taro 10 ## 2 hanako 8 ## &#39;data.frame&#39;: 2 obs. of 2 variables: ## $ kids: chr &quot;taro&quot; &quot;hanako&quot; ## $ ages: num 10 8 ## [1] &quot;taro&quot; &quot;hanako&quot; ## [1] &quot;taro&quot; &quot;hanako&quot; ## [1] &quot;taro&quot; &quot;hanako&quot; ## kids ## 1 taro ## 2 hanako ## [1] &quot;data.frame&quot; ## [1] FALSE 2.3 データの操作・演算 2.3.1 ベクトルの結合, ソート vec1 = 1:4 vec2 = 2:5 rbind(vec1, vec2) # ベクトルの行方向への結合 cbind(vec1, vec2) # べクトルの列方向への結合 vec3 &lt;- c(2, 5, 1, 3) sort(vec3) # 昇順 rev(vec3) # 順番を逆転させる ccc[, order(ccc[&quot;2012&quot;, ])] ccc[, sort.list(ccc[&quot;2012&quot;, ])] ## [,1] [,2] [,3] [,4] ## vec1 1 2 3 4 ## vec2 2 3 4 5 ## vec1 vec2 ## [1,] 1 2 ## [2,] 2 3 ## [3,] 3 4 ## [4,] 4 5 ## [1] 1 2 3 5 ## [1] 3 1 5 2 ## 京都 大阪 名古屋 ## 2012 1 3 5 ## 2013 6 2 10 ## 京都 大阪 名古屋 ## 2012 1 3 5 ## 2013 6 2 10 2.3.2 二項演算 x &lt;- c(1, 3, 5, 2); y &lt;- c(-3, 1, -1, -2) x + y x * y x / y x ^ 2 x &lt; y ## [1] -2 4 4 0 ## [1] -3 3 -5 -4 ## [1] -0.3333333 3.0000000 -5.0000000 -1.0000000 ## [1] 1 9 25 4 ## [1] FALSE FALSE FALSE FALSE 2.3.3 論理演算 lx &lt;- c(T, T, F); ly &lt;- c(F, F, F) lx &amp; ly lx &amp;&amp; ly # 最初の要素間の論理演算が成り立つと, 以降の演算は行わない ## Warning in lx &amp;&amp; ly: &#39;length(x) = 3 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in lx &amp;&amp; ly: &#39;length(x) = 3 &gt; 1&#39; in coercion to &#39;logical(1)&#39; lx | ly lx || ly # 最初の要素間の論理演算が成り立つと, 以降の演算は行わない ## Warning in lx || ly: &#39;length(x) = 3 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## [1] FALSE FALSE FALSE ## [1] FALSE ## [1] TRUE TRUE FALSE ## [1] TRUE 2.3.4 条件式 # ==, &gt;, &lt;, &gt;=, &lt;= # &amp;&amp;, || 2.3.5 行列演算 A &lt;- matrix(c(1, 2, 3, 4, 5, 6), 3, 2) B &lt;- matrix(c(2, 1, -1, -2), 2, 2) A %*% B # 行列の積 # diag # 対角行列 # solve # 逆行列 ## [,1] [,2] ## [1,] 6 -9 ## [2,] 9 -12 ## [3,] 12 -15 2.4 R関数 2.4.1 数学基本関数 # sum; sqrt; abs # exp; log; log10; log2; sin; cos # round; ceiling; floor 2.4.2 基本統計量の計算 # mean, max; min; range; median; quantile # var; sd # summary # table # cov; cor 統計 &lt;- c(rep(&quot;好き&quot;, 8), rep(&quot;嫌い&quot;, 7)) 数学 &lt;- c(rep(&quot;好き&quot;, 6), rep(&quot;嫌い&quot;, 9)) table(統計, 数学) # クロス集計表, ベクトルは同一長 ## 数学 ## 統計 好き 嫌い ## 好き 6 2 ## 嫌い 0 7 2.4.3 確率分布 # dxxx(q) # 確率密度, q:確率点 # pxxx(q) # 累積確率, q:確率点 # qxxx(p) # 確率点, p:確率 # rxxx(n) # 乱数, n:個数 # ------------------------------------------------------------------ # xxx部分: # unif(x, min, max) # 一様分布 # norm(x, mean, sd) # 正規分布 # exp(x, rate) # 指数分布 # binom(x, size, prob) # ２項分布 # pois(x, lambda) # ポアソン分布 # t(x, df) # t分布 # chisq(x, df) # カイ2乗分布 # f(x, df1, df2) # F分布 curve(xを含んだ式, from = xの左端点, to = xの右端点) # 関数のグラフ描画 curve(dnorm(x, mean = 0, sd = 1), from = -4, to = 4) curve(dnorm(x, mean = 1, sd = 2), from = -4, to = 4, add = T) # 問：t分布(自由度4)の形状は? 2.4.4 その他便利な関数 # sweep # scale # ifelse ifelse(統計 == &quot;好き&quot;, 1, 0) # apply (X, MARGIN, FUN, ...) apply(ccc, 1, sum) apply(ccc, 2, sum) colMeans(ccc) rowMeans(ccc) ## [1] 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 ## 2012 2013 ## 9 18 ## 大阪 京都 名古屋 ## 5 7 15 ## 大阪 京都 名古屋 ## 2.5 3.5 7.5 ## 2012 2013 ## 3 6 2.5 データの可視化 # 棒グラフ barplot(ccc) barplot(ccc, beside = T) barplot(ccc, beside = T, col = c(&quot;lightblue&quot;, &quot;lavender&quot;), main = &quot;test&quot;) # apply(ccc, 1, pie) # pie # hist # 折れ線グラフ (行列の各列(変数)の同時プロット) matplot(ccc, type = &quot;l&quot;) matplot(t(ccc), type = &quot;l&quot;) # 箱ひげ図 boxplot(ccc) boxplot(t(ccc)) # 散布図 # plot pairs(ccc) #install.packages(&quot;car&quot;) #library(car) #scatterplot(ccc) # install.packages(&quot;scatterplot3d&quot;); library(scatterplot3d) # scatterplot3d # その他のグラフ # coplot; mosaic plot; stars; faces; persp; image; contour # その他 # windows() #新しいグラフィック・ウィンドウを開く # par(mfrow = c(2, 2)) # より洗練されたグラフ. やや難易度が高いがモダンなアプローチ # install.packages(&quot;ggplot2&quot;, dependencies = T) library(ggplot2) ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Petal.Length)) + geom_point(aes(colour = Species)) + geom_smooth(method = &quot;lm&quot;, colour = &quot;lightblue&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; 2.6 ファイル入出力 2.6.1 テキストファイル読み込み ファイル読み込み用のR関数には, ファイルの格納場所 (パス) とファイル名を知らせる必要がある パスを省略すると, 現在のディレクトリ (getwd関数で確認可能) 下でファイルを探す. もし, 存在しなければ, エラーとなる ここでは, あらかじめ, 各自のPCのデスクトップ上に, “BS_2023”という名前のフォルダ (ディレクトリ) を作成していると想定 ファイルは, カンマ(, )で区切られたcsv形式や, タブで区切られたtsv形式の テキストファイルであるとする. data1 &lt;- read.table(&quot;/[パス名]/ファイル名&quot;, header = T, row.names = 1) # オプション # header = T: 1行目が列ラベル # row.names = 1: 1列目が行ラベル または, file.path関数を使ってファイルの格納されているパス(経路)を指定しても良い. # ユーザー(yamada)が, デスクトップ(Desktop)フォルダの下に授業用フォルダ(BS_2023)を作成した場合のパスの指定 fpath &lt;- file.path(&quot;~&quot;, &quot;Desktop&quot;, &quot;BS_2023&quot;) # または fpath &lt;- file.path(&quot;Users&quot;, &quot;yamada&quot;, &quot;Desktop&quot;, &quot;BS_2023&quot;) # ifile &lt;- file.path(fpath, &quot;) # 例えば, # data1 &lt;- read.table(&quot;/Users/[アカウント名]/Desktop/BS_2023/data.txt&quot;, sep = &quot;, &quot;) # または # data1 &lt;- read.csv(&quot;/Users/[アカウント名]/Desktop/BS_2023/data.txt&quot;, header = T, row.names = 1) # 代替的に # data2 &lt;- scan(&quot;/Users/[アカウント名]/Desktop/BS_2023/data.txt&quot;, sep = &quot;, &quot;) #matrix(scan(&quot;/Users/[アカウント名]/Desktop/BS_2023/data.txt&quot;, sep = &quot;, &quot;), 3, 4, byrow = T) # matrix(scan(&quot;/Users/[アカウント名]/Desktop/BS_2023/data.txt&quot;, sep = &quot;, &quot;), 3, 4) # scan()において, 数値, 文字が混在している場合, 列ごとにデータ属性を指定する必要 # data2.txt # a 1 2 # b 2 3 # c 3 4 # data3 &lt;- scan(&quot;/Users/[アカウント名]/Desktop/BS_2023/data.txt&quot;, sep = &quot;, &quot;, list(x = &quot;&quot;, y = 0, z = 0)) # data.frame(data2) # データフレーム化 # パスを指定せずに, テキストファイルの置かれているフォルダに移動してから # ファイル名のみを使って読み込んでも良い # setwd(&quot;/Users/[アカウント名]/Desktop/BS_2023&quot;) data1 &lt;- read.table(&quot;data.txt&quot;, sep = &quot;, &quot;) # パッケージ&quot;foreign&quot;により, SAS, SPSS等のファイル形式のデータの読み込みが可能 2.6.2 テキストファイル書き出し write(data3, &quot;/[パス名]/ファイル名&quot;) # 例えば, # write.table(data3, &quot;/Users/[アカウント名]/Desktop/BS_2023/data3_out1.txt&quot;) # write.table(data3, &quot;/Users/[アカウント名]/Desktop/BS_2023/data3_out1.txt&quot;, append = T) # write.csv() # sink(&quot;/Users/[アカウント名]/Desktop/BS_2023/data3_out1.txt&quot;) data1; data2 sink() "],["仮説検定.html", "3 仮説検定 3.1 平均値の差の検定 3.2 カイ二乗検定 3.3 分析例: 統計テストデータ", " 3 仮説検定 3.1 平均値の差の検定 # t.test(x, y = NULL, # alternative = c(&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;), # mu = 0, paired = FALSE, var.equal = FALSE, # conf.level = 0.95, ...) # # # One Sample t-test # Performs one and two sample t-tests on vectors of data. 平均値の差の検定 (ペア検定) # sleep: Rにデフォルトで収録されているデータ 患者10名, 2種類の睡眠薬の比較 # (コントロールに対する睡眠時間の増加分) help(sleep) Data which show the effect # of two soporific drugs (increase in hours of sleep compared to control) on 10 # patients. (extra, group, ID) 20件, 患者(ID) 10名 head(sleep) #&gt; extra group ID #&gt; 1 0.7 1 1 #&gt; 2 -1.6 1 2 #&gt; 3 -0.2 1 3 #&gt; 4 -1.2 1 4 #&gt; 5 -0.1 1 5 #&gt; 6 3.4 1 6 tail(sleep) #&gt; extra group ID #&gt; 15 -0.1 2 5 #&gt; 16 4.4 2 6 #&gt; 17 5.5 2 7 #&gt; 18 1.6 2 8 #&gt; 19 4.6 2 9 #&gt; 20 3.4 2 10 attach(sleep) par(mfrow = c(1, 2)) plot(extra) hist(extra) # t.test(extra)\\t# 注) ペア検定ではない # 両側検定 t.test(extra ~ group, paired = T) # ペア検定 (両側検定 (デフォルト)) #&gt; #&gt; Paired t-test #&gt; #&gt; data: extra by group #&gt; t = -4.0621, df = 9, p-value = 0.002833 #&gt; alternative hypothesis: true mean difference is not equal to 0 #&gt; 95 percent confidence interval: #&gt; -2.4598858 -0.7001142 #&gt; sample estimates: #&gt; mean difference #&gt; -1.58 # t.test(extra[group == 1], extra[group == 2], paired = TRUE) # 片側検定 t.test(extra ~ group, paired = T, alternative = &quot;greater&quot;) # 片側 (右側) 検定 #&gt; #&gt; Paired t-test #&gt; #&gt; data: extra by group #&gt; t = -4.0621, df = 9, p-value = 0.9986 #&gt; alternative hypothesis: true mean difference is greater than 0 #&gt; 95 percent confidence interval: #&gt; -2.293005 Inf #&gt; sample estimates: #&gt; mean difference #&gt; -1.58 t.test(extra ~ group, paired = T, alternative = &quot;less&quot;) # 片側 (左側) 検定 #&gt; #&gt; Paired t-test #&gt; #&gt; data: extra by group #&gt; t = -4.0621, df = 9, p-value = 0.001416 #&gt; alternative hypothesis: true mean difference is less than 0 #&gt; 95 percent confidence interval: #&gt; -Inf -0.8669947 #&gt; sample estimates: #&gt; mean difference #&gt; -1.58 boxplot(sleep) # bad example # boxplot(extra) boxplot(extra ~ group) par(mfrow = c(1, 1)) 平均値の差の検定 # 口コミサイト(仮想データ) 2つの業種(A, B), 各20社 # 各企業に対する(元)従業員による平均評価点(1--5) x &lt;- read.table(&#39;dat_1-1.csv&#39;, # sep = &#39;,&#39;, header = T) x &lt;- read.csv(&quot;dat_1-1.csv&quot;, header = T) boxplot(x) attach(x) t.test(A, B) #&gt; #&gt; Welch Two Sample t-test #&gt; #&gt; data: A and B #&gt; t = -2.0822, df = 37.963, p-value = 0.04412 #&gt; alternative hypothesis: true difference in means is not equal to 0 #&gt; 95 percent confidence interval: #&gt; -1.097472 -0.015428 #&gt; sample estimates: #&gt; mean of x mean of y #&gt; 2.94290 3.49935 ## 正規性の検定 par(mfrow = c(1, 2)) # ks.test(A, &#39;pnorm&#39;); ks.test(B, &#39;pnorm&#39;)\\t\\t# コルモゴロフ・スミルノフ # (Kolmogorov-Smirnov) 検定 shapiro.test(A); shapiro.test(B)\\t# # シャピロ・ウィルク (Shapiro-Wilk) 検定 qqnorm(A); qqnorm(B)\\t# q-qプロット hist(A) hist(B) 平均値の差の検定 (ノンパラメトリック検定) 順位和検定: ノンパラメトリック検定の一手法 外れ値に対して頑強 # wilcox.test(x$A, x$B) wilcox.test(A, B) #&gt; #&gt; Wilcoxon rank sum exact test #&gt; #&gt; data: A and B #&gt; W = 139, p-value = 0.1022 #&gt; alternative hypothesis: true location shift is not equal to 0 # 対応のある場合 (例, 同一企業の職種A, B) plot(A, type = &quot;l&quot;, ylim = c(1, 5)) lines(B, lty = &quot;dotted&quot;) hist(A, col = &quot;red&quot;) hist(B, add = T, col = &quot;blue&quot;) t.test(A, B, paired = T) detach(x) 3.2 カイ二乗検定 # chisq.test(x, y = NULL, correct = TRUE, # p = rep(1 / length(x), length(x)), rescale.p = FALSE, simulate.p.value = FALSE, B = 2000) # chisq.test performs chi-squared contingency table tests and goodness-of-fit tests. 3.2.1 独立性検定 # 演習用データの作成 (実務では, ファイルを読み込む) d1 &lt;- matrix(c(rep(c(&quot;a1&quot;, &quot;b1&quot;), 76), rep(c(&quot;a1&quot;, &quot;b2&quot;), 15), rep(c(&quot;a1&quot;, &quot;b3&quot;), 41)), byrow = T, ncol = 2) d2 &lt;- matrix(c(rep(c(&quot;a2&quot;, &quot;b1&quot;), 95), rep(c(&quot;a2&quot;, &quot;b2&quot;), 30), rep(c(&quot;a2&quot;, &quot;b3&quot;), 85)), byrow = T, ncol = 2) d3 &lt;- matrix(c(rep(c(&quot;a3&quot;, &quot;b1&quot;), 135), rep(c(&quot;a3&quot;, &quot;b2&quot;), 70), rep(c(&quot;a3&quot;, &quot;b3&quot;), 95)), byrow = T, ncol = 2) d4 &lt;- matrix(c(rep(c(&quot;a4&quot;, &quot;b1&quot;), 69), rep(c(&quot;a4&quot;, &quot;b2&quot;), 10), rep(c(&quot;a4&quot;, &quot;b3&quot;), 29)), byrow = T, ncol = 2) data2 &lt;- rbind(d1, d2, d3, d4) colnames(data2) &lt;- c(&quot;A&quot;, &quot;B&quot;) # 商品種類(A), 販売チャネル(B) head(data2) #&gt; A B #&gt; [1,] &quot;a1&quot; &quot;b1&quot; #&gt; [2,] &quot;a1&quot; &quot;b1&quot; #&gt; [3,] &quot;a1&quot; &quot;b1&quot; #&gt; [4,] &quot;a1&quot; &quot;b1&quot; #&gt; [5,] &quot;a1&quot; &quot;b1&quot; #&gt; [6,] &quot;a1&quot; &quot;b1&quot; table(data2) # marginal totals #&gt; data2 #&gt; a1 a2 a3 a4 b1 b2 b3 #&gt; 132 210 300 108 375 125 250 (tbl &lt;- table(data2[, 1], data2[, 2])) # contingency table #&gt; #&gt; b1 b2 b3 #&gt; a1 76 15 41 #&gt; a2 95 30 85 #&gt; a3 135 70 95 #&gt; a4 69 10 29 chisq.test(data2[, &quot;A&quot;], data2[, &quot;B&quot;]) #&gt; #&gt; Pearson&#39;s Chi-squared test #&gt; #&gt; data: data2[, &quot;A&quot;] and data2[, &quot;B&quot;] #&gt; X-squared = 27.661, df = 6, p-value = 0.0001088 # chisq.test(data2[, 1], data2[, 2]) # または chisq.test(tbl) #&gt; #&gt; Pearson&#39;s Chi-squared test #&gt; #&gt; data: tbl #&gt; X-squared = 27.661, df = 6, p-value = 0.0001088 # A/Bテスト サイト導線A/Bとで, コンバージョンへの効果を比較 # \\t\\t\\t\\t\\t\\t有\\t\\t\\t\\t無 \\tサイト導線A\\t50\\t\\t\\t\\t131 # \\tサイト導線B\\t23\\t\\t\\t\\t35 ABdat &lt;- matrix(c(50, 131, 23, 35), ncol = 2, byrow = T) chisq.test(ABdat) #&gt; #&gt; Pearson&#39;s Chi-squared test with Yates&#39; continuity correction #&gt; #&gt; data: ABdat #&gt; X-squared = 2.4566, df = 1, p-value = 0.117 # 導線Aでコンバージョンしない人が10人増えた \\t\\t\\t\\t\\t\\t有\\t\\t\\t無 # \\tサイト導線A\\t50\\t\\t\\t141 \\tサイト導線B\\t23\\t\\t\\t 35 ABdat2 &lt;- matrix(c(50, 141, 23, 35), ncol = 2, byrow = T) chisq.test(ABdat2) #&gt; #&gt; Pearson&#39;s Chi-squared test with Yates&#39; continuity correction #&gt; #&gt; data: ABdat2 #&gt; X-squared = 3.2764, df = 1, p-value = 0.07028 # Fisherの正確確率検定 fisher.test(ABdat2) #&gt; #&gt; Fisher&#39;s Exact Test for Count Data #&gt; #&gt; data: ABdat2 #&gt; p-value = 0.06915 #&gt; alternative hypothesis: true odds ratio is not equal to 1 #&gt; 95 percent confidence interval: #&gt; 0.2796541 1.0568106 #&gt; sample estimates: #&gt; odds ratio #&gt; 0.5410443 3.2.2 適合度検定 # メンデルのデータ（エンドウの交雑実験） 種子の特徴(形質), 黄色・丸い, # 黄色・しわ, 緑色・丸い, 緑色・しわ obs &lt;- c(315, 101, 108, 32) # 観測度数 prob &lt;- c(9, 3, 3, 1)/16 # 理論確率分布 chisq.test(obs, p = prob) # obs と prob を用いたカイ二乗検定 #&gt; #&gt; Chi-squared test for given probabilities #&gt; #&gt; data: obs #&gt; X-squared = 0.47002, df = 3, p-value = 0.9254 # 確認用 ex &lt;- prob * sum(obs) chisq &lt;- sum((obs - ex)^2/ex) pval &lt;- 1 - pchisq(chisq, 3) 3.3 分析例: 統計テストデータ testdat &lt;- read.csv(&quot;BS_stattest.csv&quot;, header = F) # year(学年), MF(性別:男性1女性2), AS(文理:文系1その他2理系3), # math(数学履修年数), work(勤務年数), stat(統計学経験0-2), # MS(経営科学好き嫌い0-3), s4(4級相当得点), s3(3級相当得点), s2(2級相当得点) colnames(testdat) &lt;- c(&quot;year&quot;, &quot;MF&quot;, &quot;AS&quot;, &quot;math&quot;, &quot;work&quot;, &quot;stat&quot;, &quot;MS&quot;, &quot;s4&quot;, &quot;s3&quot;, &quot;s2&quot;) score &lt;- apply(testdat[, c(&quot;s4&quot;, &quot;s3&quot;, &quot;s2&quot;)], 1, sum) testdat2 &lt;- cbind(testdat, score) # モダンな方法 library(tidyverse) testdat2 &lt;- testdat %&gt;% mutate(score = s4 + # s3 + s2) # データの要約 attach()を使わない場合: table(testdat2$MF) table(testdat2[, # c(&#39;MF&#39;, &#39;AS&#39;)]) table(testdat2$s3) attach(testdat2) table(MF) #&gt; MF #&gt; 1 2 #&gt; 29 8 table(MF, AS) #&gt; AS #&gt; MF 1 2 3 #&gt; 1 17 3 9 #&gt; 2 4 1 3 table(s3) #&gt; s3 #&gt; 3 4 5 6 7 8 9 #&gt; 3 2 5 10 5 10 2 summary(score) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 10.00 16.00 18.00 17.97 20.00 24.00 fivenum(score) #&gt; [1] 10 16 18 20 24 hist(score) # 相関係数 cor(testdat[, c(&#39;math&#39;, &#39;s2&#39;)]) cor(testdat[, c(&#39;math&#39;, &#39;s3&#39;)]) # cor(testdat[, c(&#39;math&#39;, &#39;s4&#39;)]) cor(math, s2) #&gt; [1] 0.04241976 cor(math, s3) #&gt; [1] 0.2481946 cor(math, s4) #&gt; [1] 0.3205779 pairs(testdat[, 8:10]) # cor(testdat[, c(&#39;MS&#39;, &#39;s4&#39;, &#39;s3&#39;, &#39;s2&#39;)]) cor(cbind(MS, s4, s3, s2)) #&gt; MS s4 s3 s2 #&gt; MS 1.0000000 0.2592955 0.4922877 0.2600562 #&gt; s4 0.2592955 1.0000000 0.1315443 0.1245631 #&gt; s3 0.4922877 0.1315443 1.0000000 0.6435060 #&gt; s2 0.2600562 0.1245631 0.6435060 1.0000000 cor(MS, math) #&gt; [1] 0.2864297 # 箱ひげ図 boxplot(score ~ factor(MF)) # 2-level factor boxplot(score ~ factor(MS)) # 4-level factor # 平均値の差の検定 score_MF &lt;- split(score, factor(MF)) t.test(score_MF$&quot;1&quot;, score_MF$&quot;2&quot;) #&gt; #&gt; Welch Two Sample t-test #&gt; #&gt; data: score_MF$&quot;1&quot; and score_MF$&quot;2&quot; #&gt; t = 2.8923, df = 13.276, p-value = 0.01237 #&gt; alternative hypothesis: true difference in means is not equal to 0 #&gt; 95 percent confidence interval: #&gt; 0.884667 6.063609 #&gt; sample estimates: #&gt; mean of x mean of y #&gt; 18.72414 15.25000 # score2 &lt;- testdat[, &#39;s2&#39;] score3 &lt;- testdat[, &#39;s3&#39;] score4 &lt;- testdat[, &#39;s4&#39;] # t.test(score4, score3, paired = T, alternative = &#39;greater&#39;) t.test(score3, # score2, paired = T, alternative = &#39;greater&#39;) # ペア検定 t.test(s4, s3, paired = T, alternative = &quot;greater&quot;) #&gt; #&gt; Paired t-test #&gt; #&gt; data: s4 and s3 #&gt; t = 0.87426, df = 36, p-value = 0.1939 #&gt; alternative hypothesis: true mean difference is greater than 0 #&gt; 95 percent confidence interval: #&gt; -0.2516528 Inf #&gt; sample estimates: #&gt; mean difference #&gt; 0.2702703 t.test(s3, s2, paired = T, alternative = &quot;greater&quot;) #&gt; #&gt; Paired t-test #&gt; #&gt; data: s3 and s2 #&gt; t = 5.305, df = 36, p-value = 2.95e-06 #&gt; alternative hypothesis: true mean difference is greater than 0 #&gt; 95 percent confidence interval: #&gt; 0.9212852 Inf #&gt; sample estimates: #&gt; mean difference #&gt; 1.351351 # 得点の差のt値の計算 (確認用) m &lt;- mean(s3 - s2) v &lt;- var(s3 - s2) tt &lt;- m/sqrt(v/(length(s3))) # var.test(x, y, ratio = 1, # alternative = c(&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;), # conf.level = 0.95, ...) # Performs an F test to compare the variances of two samples from normal populations. # aaa &lt;- table(testdat[, c(&#39;MF&#39;, &#39;AS&#39;)]) aaa &lt;- table(MF, AS) chisq.test(aaa) #&gt; #&gt; Pearson&#39;s Chi-squared test #&gt; #&gt; data: aaa #&gt; X-squared = 0.18986, df = 2, p-value = 0.9094 # aaa &lt;- table(testdat[, c(&#39;MS&#39;, &#39;AS&#39;)]) aaa &lt;- table(MS, AS) chisq.test(aaa) #&gt; #&gt; Pearson&#39;s Chi-squared test #&gt; #&gt; data: aaa #&gt; X-squared = 16.044, df = 6, p-value = 0.01352 detach(testdat2) "],["分散分析-anova.html", "4 分散分析 (ANOVA) 4.1 1元配置ANOVA 4.2 2元配置ANOVA 4.3 分析例", " 4 分散分析 (ANOVA) 4.1 1元配置ANOVA # 仮想データセットの作成 (数値例1) グループ数: k=3 k &lt;- 3 ttt1 &lt;- c(8, 7, 9, 6, 8) ttt2 &lt;- c(7, 5, 4) ttt3 &lt;- c(6, 2, 1, 3) # 縦型に整形 dat1 &lt;- data.frame(grp = c(rep(&quot;ttt1&quot;, length(ttt1)), rep(&quot;ttt2&quot;, length(ttt2)), rep(&quot;ttt3&quot;, length(ttt3))), resp = c(ttt1, ttt2, ttt3)) # ANOVAの実行 res_aov &lt;- aov(resp ~ grp, data = dat1) # 実行結果の表示 summary(res_aov) #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; grp 2 47.13 23.567 8.887 0.0074 ** #&gt; Residuals 9 23.87 2.652 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # 確認用 (飛ばしてOK) library(tidyverse) # 全体平方和 (SST) の計算 avg &lt;- mean(dat1$resp) # 全体平均 # (mean(dat1$resp^2) - avg^2) * length(dat1$resp) sst &lt;- sum((dat1$resp - avg)^2) # 全体平方和 # 群内平方和 (SSW) の計算 (※) 発展的なコード library(tidyverse) ssw_j &lt;- dat1 %&gt;% group_by(grp) %&gt;% summarize(ssq = (mean(resp^2) - mean(resp)^2) * length(resp)) ssw &lt;- sum(ssw_j$ssq) # 群内平方和 # 群間平方和 (SSB) の計算 n_vec &lt;- c(length(ttt1), length(ttt2), length(ttt3)) avg_j &lt;- dat1 %&gt;% group_by(grp) %&gt;% summarize(avg = mean(resp)) ssb &lt;- sum((avg_j$avg - avg)^2 * n_vec) # 群間平方和 # または, ssb &lt;- sst - ssw # 群間平方和 # 自由度 (df) SST: length(dat1$resp) - 1, SSB: K - 1, SSW: length(dat1$resp) - # K # 平均平方和の計算 msb &lt;- ssb/(k - 1) msw &lt;- ssw/(length(dat1$resp) - k) # F値の計算 f_val &lt;- msb/msw # p値の計算 pf(f_val, k - 1, length(dat1$resp) - k, lower.tail = F) #&gt; [1] 0.007402874 4.2 2元配置ANOVA # 仮想データセットの作成 (数値例2) fctA_val &lt;- c(&quot;a1&quot;, &quot;a2&quot;, &quot;a3&quot;, &quot;a4&quot;) fctB_val &lt;- c(&quot;b1&quot;, &quot;b2&quot;, &quot;b3&quot;) val &lt;- matrix(c(8, 7, 6, 7, 5, 2, 6, 4, 3, 8, 6, 2), nrow = 4, byrow = T) colnames(val) &lt;- fctB_val rownames(val) &lt;- fctA_val dat2 &lt;- as.data.frame(val) %&gt;% rownames_to_column(var = &quot;fctA&quot;) %&gt;% pivot_longer(cols = b1:b3, names_to = &quot;fctB&quot;, values_to = &quot;resp&quot;) %&gt;% data.frame() # SST sum((val - mean(val))^2) # ANOVAの実行 res_aov &lt;- aov(resp ~ fctA + fctB, data = dat2) # 実行結果の表示 summary(res_aov) #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; fctA 3 12.67 4.222 4.343 0.05988 . #&gt; fctB 2 32.17 16.083 16.543 0.00362 ** #&gt; Residuals 6 5.83 0.972 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # interactionなし res_aov &lt;- aov(resp ~ fctA * fctB, data = dat2) # summary(res_aov) 4.3 分析例 データ1: 2業種口コミサイト(仮想データ) t検定と結果の比較を行う. データの読み込み, 並べ替え x &lt;- read.csv(&quot;dat_1-1.csv&quot;) # 口コミサイト(仮想データ) 2つの業種(A, B), 各20社 # 各企業に対する(元)従業員による平均評価点(1--5) head(x) #&gt; A B #&gt; 1 2.257 4.065 #&gt; 2 4.273 4.771 #&gt; 3 4.205 2.793 #&gt; 4 3.251 3.003 #&gt; 5 1.534 3.250 #&gt; 6 3.327 3.390 # xの整形 (横型 → 縦型) xvec &lt;- as.vector(as.matrix(x)) # xをベクトル化 yvec &lt;- c(rep(&quot;A&quot;, 20), rep(&quot;B&quot;, 20)) xdf &lt;- data.frame(score = xvec, type = yvec) str(xdf) #&gt; &#39;data.frame&#39;: 40 obs. of 2 variables: #&gt; $ score: num 2.26 4.27 4.21 3.25 1.53 ... #&gt; $ type : chr &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ... xdf$score #&gt; [1] 2.257 4.273 4.205 3.251 1.534 3.327 3.596 3.421 2.544 2.110 2.262 2.550 #&gt; [13] 2.790 2.033 3.688 3.440 3.797 3.644 2.906 1.230 4.065 4.771 2.793 3.003 #&gt; [25] 3.250 3.390 2.834 2.171 4.654 3.051 4.789 4.373 3.325 3.028 4.127 2.796 #&gt; [37] 2.015 4.379 3.380 3.793 xdf[, &quot;score&quot;] #&gt; [1] 2.257 4.273 4.205 3.251 1.534 3.327 3.596 3.421 2.544 2.110 2.262 2.550 #&gt; [13] 2.790 2.033 3.688 3.440 3.797 3.644 2.906 1.230 4.065 4.771 2.793 3.003 #&gt; [25] 3.250 3.390 2.834 2.171 4.654 3.051 4.789 4.373 3.325 3.028 4.127 2.796 #&gt; [37] 2.015 4.379 3.380 3.793 xdf[, 1] #&gt; [1] 2.257 4.273 4.205 3.251 1.534 3.327 3.596 3.421 2.544 2.110 2.262 2.550 #&gt; [13] 2.790 2.033 3.688 3.440 3.797 3.644 2.906 1.230 4.065 4.771 2.793 3.003 #&gt; [25] 3.250 3.390 2.834 2.171 4.654 3.051 4.789 4.373 3.325 3.028 4.127 2.796 #&gt; [37] 2.015 4.379 3.380 3.793 # 代替的方法 stack(x) ## library(tidyverse) gather関数(横型→縦型), spread関数(縦型→横型) 1元ANOVAの実行 # 1元ANOVA attach(xdf) res_aov &lt;- aov(score ~ type) summary(res_aov) ### 代替アプローチ (1) oneway.test(score ~ type) # デフォルト：等分散を仮定しない oneway.test(score ~ type, var.equal = T) # 等分散を仮定 ### 代替アプローチ (2) res_lm &lt;- lm(score ~ type) res_anova &lt;- anova(res_lm) res_anova # summary()を使わずに出力 detach(xdf) # 等分散検定 var.test(x$A, x$B) boxplot(x) # t検定との比較 t.test(x$A, x$B) t.test(x$A, x$B, var.equal = T) ## t.test(x$A, x$B, paired=T) データ2: 統計小テストデータ year(学年), MF(性別:男性1女性2), AS(文理:文系1その他2理系3), math(数学履修年数), work(勤務年数), stat(統計学経験0-2), MS(経営科学好き嫌い0-3), s4(4級相当得点), s3(3級相当得点), s2(2級相当得点) testdat &lt;- read.csv(&quot;BS_stattest.csv&quot;, header = F) colnames(testdat) &lt;- c(&quot;year&quot;, &quot;MF&quot;, &quot;AS&quot;, &quot;math&quot;, &quot;work&quot;, &quot;stat&quot;, &quot;MS&quot;, &quot;s4&quot;, &quot;s3&quot;, &quot;s2&quot;) # 総合得点の計算, 列に追加 score &lt;- apply(testdat[, c(&quot;s4&quot;, &quot;s3&quot;, &quot;s2&quot;)], 1, sum) testdat2 &lt;- cbind(testdat, score) # モダンな方法 library(tidyverse) testdat2 &lt;- testdat %&gt;% mutate(score = s4 + # s3 + s2) str(testdat2) #&gt; &#39;data.frame&#39;: 37 obs. of 11 variables: #&gt; $ year : int 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ MF : int 1 1 1 1 2 1 1 1 1 1 ... #&gt; $ AS : int 1 2 1 1 1 1 1 1 1 3 ... #&gt; $ math : int 13 6 8 7 5 5 5 6 8 10 ... #&gt; $ work : int 7 6 4 4 1 5 4 7 0 8 ... #&gt; $ stat : int 1 0 1 0 0 0 0 0 0 0 ... #&gt; $ MS : int 2 3 1 1 1 1 0 1 1 2 ... #&gt; $ s4 : int 7 8 7 7 7 5 5 7 8 7 ... #&gt; $ s3 : int 8 8 6 9 5 8 3 7 6 8 ... #&gt; $ s2 : int 7 8 6 7 4 7 2 6 7 5 ... #&gt; $ score: int 22 24 19 23 16 20 10 20 21 20 ... # → 得点以外の変数も数値(整数)で入力されている # 分割表(クロス集計表) table(testdat2[, c(&quot;MF&quot;, &quot;AS&quot;)]) #&gt; AS #&gt; MF 1 2 3 #&gt; 1 17 3 9 #&gt; 2 4 1 3 # 相関係数 cor(testdat2[, c(&quot;math&quot;, &quot;s2&quot;)]) #&gt; math s2 #&gt; math 1.00000000 0.04241976 #&gt; s2 0.04241976 1.00000000 cor(testdat2[, c(&quot;MS&quot;, &quot;s4&quot;, &quot;s3&quot;, &quot;s2&quot;)]) #&gt; MS s4 s3 s2 #&gt; MS 1.0000000 0.2592955 0.4922877 0.2600562 #&gt; s4 0.2592955 1.0000000 0.1315443 0.1245631 #&gt; s3 0.4922877 0.1315443 1.0000000 0.6435060 #&gt; s2 0.2600562 0.1245631 0.6435060 1.0000000 cor(testdat2[, c(&quot;MS&quot;, &quot;math&quot;)]) #&gt; MS math #&gt; MS 1.0000000 0.2864297 #&gt; math 0.2864297 1.0000000 # 箱ひげ図 attach(testdat2) par(mfrow = c(1, 2)) boxplot(score ~ factor(MF)) # 2-level factor boxplot(score ~ factor(MS)) # 4-level factor aaa &lt;- table(testdat2[, c(&quot;MF&quot;, &quot;AS&quot;)]) chisq.test(aaa) #&gt; #&gt; Pearson&#39;s Chi-squared test #&gt; #&gt; data: aaa #&gt; X-squared = 0.18986, df = 2, p-value = 0.9094 aaa &lt;- table(testdat2[, c(&quot;MS&quot;, &quot;AS&quot;)]) chisq.test(aaa) #&gt; #&gt; Pearson&#39;s Chi-squared test #&gt; #&gt; data: aaa #&gt; X-squared = 16.044, df = 6, p-value = 0.01352 1元ANOVA # 1-way ANOVA summary(aov(score ~ factor(MS))) #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; factor(MS) 3 108.2 36.07 3.282 0.0329 * #&gt; Residuals 33 362.7 10.99 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # 代替的アプローチ anova(lm(score ~ factor(MS))) #&gt; Analysis of Variance Table #&gt; #&gt; Response: score #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; factor(MS) 3 108.22 36.074 3.2817 0.03293 * #&gt; Residuals 33 362.75 10.992 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 2元ANOVA ### 2-way ANOVA 交互作用プロット interaction.plot(factor(MS), factor(MF), score) summary(aov(score ~ factor(MS) + factor(MF))) # 主効果項のみ (交互作用項なし) #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; factor(MS) 3 108.2 36.07 3.571 0.0246 * #&gt; factor(MF) 1 39.5 39.49 3.909 0.0567 . #&gt; Residuals 32 323.3 10.10 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(aov(score ~ factor(MS) * factor(MF))) # 交互作用項有 #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; factor(MS) 3 108.22 36.07 3.727 0.0217 * #&gt; factor(MF) 1 39.49 39.49 4.080 0.0524 . #&gt; factor(MS):factor(MF) 2 32.92 16.46 1.701 0.1997 #&gt; Residuals 30 290.35 9.68 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # 代替的アプローチ anova(lm(score ~ factor(MS) + factor(AS))) # 主効果項のみ (交互作用項なし) #&gt; Analysis of Variance Table #&gt; #&gt; Response: score #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; factor(MS) 3 108.22 36.074 3.3147 0.03267 * #&gt; factor(AS) 2 25.38 12.689 1.1659 0.32493 #&gt; Residuals 31 337.37 10.883 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(lm(score ~ factor(AS) + factor(MS))) # 分解順の影響 #&gt; Analysis of Variance Table #&gt; #&gt; Response: score #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; factor(AS) 2 3.50 1.748 0.1607 0.85229 #&gt; factor(MS) 3 130.10 43.368 3.9849 0.01642 * #&gt; Residuals 31 337.37 10.883 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(lm(score ~ factor(MS) * factor(AS))) # 交互作用項有 #&gt; Analysis of Variance Table #&gt; #&gt; Response: score #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; factor(MS) 3 108.22 36.074 3.3063 0.03397 * #&gt; factor(AS) 2 25.38 12.689 1.1629 0.32672 #&gt; factor(MS):factor(AS) 2 20.96 10.478 0.9603 0.39460 #&gt; Residuals 29 316.42 10.911 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 lm(score ~ factor(MS) + factor(MF)) #&gt; #&gt; Call: #&gt; lm(formula = score ~ factor(MS) + factor(MF)) #&gt; #&gt; Coefficients: #&gt; (Intercept) factor(MS)1 factor(MS)2 factor(MS)3 factor(MF)2 #&gt; 15.552 2.693 3.820 8.448 -2.604 detach(testdat2) (自主課題) 対応のある分散分析の実行: 山田・杉澤・村井 (2008)『Rによるやさしい統計学』「第7章 分散分析」を参照せよ. データ3: 経営科学アンケート グループ(group), 負荷(workload), 難易度(difficulty), 有用性(usefulness), 貢献度(contribution), 数学スキル(Math), エクセルスキル(Excel), 文理(AS), 業務経験(Work), 性別(MF) testdat &lt;- read.csv(&quot;FB_dist.csv&quot;, header = T) attach(testdat) # 実行1 aov(difficulty ~ Math) #&gt; Call: #&gt; aov(formula = difficulty ~ Math) #&gt; #&gt; Terms: #&gt; Math Residuals #&gt; Sum of Squares 20.02254 52.62250 #&gt; Deg. of Freedom 1 113 #&gt; #&gt; Residual standard error: 0.6824118 #&gt; Estimated effects may be unbalanced # anova(lm(difficulty ~ Math)) # 実行2 aov(difficulty ~ factor(Math)) #&gt; Call: #&gt; aov(formula = difficulty ~ factor(Math)) #&gt; #&gt; Terms: #&gt; factor(Math) Residuals #&gt; Sum of Squares 21.81657 50.82847 #&gt; Deg. of Freedom 4 110 #&gt; #&gt; Residual standard error: 0.6797624 #&gt; Estimated effects may be unbalanced # anova(lm(difficulty ~ factor(Math))) detach(testdat) (自主課題) 対応のある分散分析の実行を試みよ. "],["線形回帰分析.html", "5 線形回帰分析 5.1 重回帰分析の基本操作 5.2 変数の選択 5.3 説明変数に質的変数を含む回帰 5.4 時系列データ同士の回帰 5.5 線形モデル", " 5 線形回帰分析 5.1 重回帰分析の基本操作 データ1: 1ルーム賃貸マンション - 1ルーム賃貸マンション, 家賃データ, 50件 (仮想データ) - rent: 月額家賃 (円) - area: 専有面積 (平米) - yrs: 築後年数 (年) - dist: 最寄駅からの徒歩距離 (m) データの読み込み, 回帰実行 rentdat &lt;- read.csv(&quot;rentdat.csv&quot;, header = T) head(rentdat) #&gt; rent area yrs dist #&gt; 1 60000 18.45 8.73 837.46 #&gt; 2 61000 19.84 13.33 520.86 #&gt; 3 74000 22.45 8.26 433.77 #&gt; 4 77000 26.81 5.94 1192.32 #&gt; 5 59000 17.62 3.85 815.17 #&gt; 6 86000 26.68 4.19 373.87 pairs(rentdat) cor(rentdat) #&gt; rent area yrs dist #&gt; rent 1.0000000 0.84098526 -0.16885266 -0.36727009 #&gt; area 0.8409853 1.00000000 0.05454398 -0.02291733 #&gt; yrs -0.1688527 0.05454398 1.00000000 -0.05812975 #&gt; dist -0.3672701 -0.02291733 -0.05812975 1.00000000 res_lm &lt;- lm(rent ~ ., data = rentdat) summary(res_lm) #&gt; #&gt; Call: #&gt; lm(formula = rent ~ ., data = rentdat) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -6732 -2379 -1016 2286 7256 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 32261.469 3652.405 8.833 1.81e-11 *** #&gt; area 2397.144 142.744 16.793 &lt; 2e-16 *** #&gt; yrs -745.440 159.275 -4.680 2.55e-05 *** #&gt; dist -12.443 1.733 -7.180 4.89e-09 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 3531 on 46 degrees of freedom #&gt; Multiple R-squared: 0.8838, Adjusted R-squared: 0.8762 #&gt; F-statistic: 116.6 on 3 and 46 DF, p-value: &lt; 2.2e-16 モデル診断 plot(res_lm$fitted.values, rentdat$rent) # モデル診断: y観測値 vs y適合値 abline(a = 0, b = 1) plot(res_lm$fitted.values, res_lm$residuals) # モデル診断: y適合値 vs 残差 abline(h = 0) par(mfrow = c(2, 2)) plot(res_lm) # → resid(res_lm) # # 標準(化)回帰係数、p.211 srentdat &lt;- scale(rentdat)\\t\\t# # scale()の返り値はリスト型 → データフレームへ変換 srentdat &lt;- # data.frame(srentdat) # sres_lm &lt;- lm(rent ~ area + yrs + dist, data = # srentdat) summary(sres_lm) # # 偏回帰係数 vs 標準(化)偏回帰係数 summary(res_lm)[&#39;coefficients&#39;] # summary(sres_lm)[&#39;coefficients&#39;] # # 確認 attach(rentdat) y_sd &lt;- sd(rent) # x_sd &lt;- apply(rentdat[, -1], 2, sd) res_lm$coef[ -1] * x_sd / y_sd モデルを使った予測 専有面積=18.8平米, 築後年数=13年, 駅距離=800m, または100mの物件の賃料は? new &lt;- data.frame(area = 18.8, dist = c(800, 100), yrs = 13) predict(res_lm, newdata = new) #&gt; 1 2 #&gt; 57682.32 66392.71 # predict.lm(res_lm, newdata = new) res_lm$residuals\\t\\t# resid(res_lm) 5.2 変数の選択 データ2: ボストン市内住宅物件価格データ - Boston Housingデータ - crim: 町ごとの一人当たり犯罪率 - zn: 25,000平方フィート以上の住宅用地の割合 - indus: 町ごとの非小売業の土地の割合 - chas: チャールズ川のダミー変数 (川に接している場合は1, さもなくば0) - nox: 窒素酸化物濃度（1000万ppm） - rm: 1住居当たりの平均部屋数 - age: 1940年以前に建設された住戸の持ち家比率 - dis: ボストンの5つの雇用センターまでの距離の加重平均 - rad: 放射状高速道路へのアクセス指数 - tax: 10,000米ドル当たりの固定資産税率 - ptratio: 町ごとの生徒数・教師数比率 - b: 1000(B - 0.63)^2 (Bは町ごとの黒人の割合) - lstat: 低所得者層の割合 - medv: 持ち家住宅の中央値（1000ドル単位） - 506件 x 14変数 (オリジナル版) - source: http://lib.stat.cmu.edu/datasets/boston housing &lt;- read.csv(&quot;boston_housing.csv&quot;, header = T) # 散布図行列 pairs(housing[, -4]) # chas(バイナリ)を除去 round(cor(housing[, c(&quot;crim&quot;, &quot;rm&quot;, &quot;tax&quot;, &quot;lstat&quot;)]), 2) # chas(バイナリ)を除去 #&gt; crim rm tax lstat #&gt; crim 1.00 -0.22 0.58 0.46 #&gt; rm -0.22 1.00 -0.29 -0.61 #&gt; tax 0.58 -0.29 1.00 0.54 #&gt; lstat 0.46 -0.61 0.54 1.00 library(corrplot) corrplot(cor(housing[, -4])) # corrplot 4変数に絞り込み res_lm1 &lt;- lm(medv ~ crim + rm + tax + lstat, data = housing) summary(res_lm1) #&gt; #&gt; Call: #&gt; lm(formula = medv ~ crim + rm + tax + lstat, data = housing) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -16.383 -3.497 -1.149 1.825 30.716 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -1.414928 3.178364 -0.445 0.6564 #&gt; crim -0.061579 0.035562 -1.732 0.0840 . #&gt; rm 5.248721 0.439664 11.938 &lt;2e-16 *** #&gt; tax -0.005018 0.001922 -2.611 0.0093 ** #&gt; lstat -0.534835 0.050258 -10.642 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.458 on 501 degrees of freedom #&gt; Multiple R-squared: 0.6506, Adjusted R-squared: 0.6478 #&gt; F-statistic: 233.2 on 4 and 501 DF, p-value: &lt; 2.2e-16 anova(res_lm1) #&gt; Analysis of Variance Table #&gt; #&gt; Response: medv #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; crim 1 6440.8 6440.8 216.206 &lt; 2.2e-16 *** #&gt; rm 1 16709.7 16709.7 560.915 &lt; 2.2e-16 *** #&gt; tax 1 1267.3 1267.3 42.542 1.693e-10 *** #&gt; lstat 1 3373.6 3373.6 113.247 &lt; 2.2e-16 *** #&gt; Residuals 501 14924.8 29.8 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # update関数でモデル更新: 変数ptratio追加 res_lm2 &lt;- update(res_lm1, . ~ . + ptratio) summary(res_lm2) #&gt; #&gt; Call: #&gt; lm(formula = medv ~ crim + rm + tax + lstat + ptratio, data = housing) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -14.3602 -3.1111 -0.9237 1.6569 30.4116 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 16.7488084 4.0001180 4.187 3.34e-05 *** #&gt; crim -0.0593795 0.0339830 -1.747 0.0812 . #&gt; rm 4.6349234 0.4292367 10.798 &lt; 2e-16 *** #&gt; tax -0.0008196 0.0019328 -0.424 0.6717 #&gt; lstat -0.5280046 0.0480346 -10.992 &lt; 2e-16 *** #&gt; ptratio -0.8731668 0.1251429 -6.977 9.59e-12 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.215 on 500 degrees of freedom #&gt; Multiple R-squared: 0.6816, Adjusted R-squared: 0.6784 #&gt; F-statistic: 214.1 on 5 and 500 DF, p-value: &lt; 2.2e-16 anova(res_lm2) #&gt; Analysis of Variance Table #&gt; #&gt; Response: medv #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; crim 1 6440.8 6440.8 236.784 &lt; 2.2e-16 *** #&gt; rm 1 16709.7 16709.7 614.301 &lt; 2.2e-16 *** #&gt; tax 1 1267.3 1267.3 46.591 2.540e-11 *** #&gt; lstat 1 3373.6 3373.6 124.026 &lt; 2.2e-16 *** #&gt; ptratio 1 1324.2 1324.2 48.684 9.589e-12 *** #&gt; Residuals 500 13600.6 27.2 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # 変数zn追加 res_lm3 &lt;- update(res_lm2, . ~ . + zn) summary(res_lm3) #&gt; #&gt; Call: #&gt; lm(formula = medv ~ crim + rm + tax + lstat + ptratio + zn, data = housing) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -14.4790 -3.1374 -0.8754 1.6871 30.3185 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 17.3073953 4.0780517 4.244 2.62e-05 *** #&gt; crim -0.0584021 0.0340274 -1.716 0.0867 . #&gt; rm 4.6460026 0.4297290 10.811 &lt; 2e-16 *** #&gt; tax -0.0008832 0.0019358 -0.456 0.6484 #&gt; lstat -0.5354553 0.0491813 -10.887 &lt; 2e-16 *** #&gt; ptratio -0.8958719 0.1291910 -6.934 1.27e-11 *** #&gt; zn -0.0081367 0.0114124 -0.713 0.4762 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.218 on 499 degrees of freedom #&gt; Multiple R-squared: 0.6819, Adjusted R-squared: 0.6781 #&gt; F-statistic: 178.3 on 6 and 499 DF, p-value: &lt; 2.2e-16 anova(res_lm, res_lm3) #&gt; Analysis of Variance Table #&gt; #&gt; Response: rent #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; area 1 3490224482 3490224482 279.906 &lt; 2.2e-16 *** #&gt; yrs 1 228207060 228207060 18.302 9.440e-05 *** #&gt; dist 1 642861374 642861374 51.556 4.893e-09 *** #&gt; Residuals 46 573587084 12469284 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # 変数nox追加, zn除去 res_lm4 &lt;- update(res_lm3, . ~ . + nox - zn) summary(res_lm4) #&gt; #&gt; Call: #&gt; lm(formula = medv ~ crim + rm + tax + lstat + ptratio + nox, #&gt; data = housing) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -14.2389 -3.1372 -0.9454 1.6680 30.4687 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 17.2649269 4.2731659 4.040 6.18e-05 *** #&gt; crim -0.0596990 0.0340256 -1.755 0.080 . #&gt; rm 4.6382386 0.4297223 10.794 &lt; 2e-16 *** #&gt; tax -0.0004089 0.0022705 -0.180 0.857 #&gt; lstat -0.5216846 0.0514382 -10.142 &lt; 2e-16 *** #&gt; ptratio -0.8844707 0.1294545 -6.832 2.44e-11 *** #&gt; nox -1.0363053 2.9989281 -0.346 0.730 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.22 on 499 degrees of freedom #&gt; Multiple R-squared: 0.6817, Adjusted R-squared: 0.6779 #&gt; F-statistic: 178.1 on 6 and 499 DF, p-value: &lt; 2.2e-16 # 変数lstatの逆数を新変数invlstatとして定義し, モデルに追加 data2 &lt;- data.frame(housing, invlstat = 1/housing$lstat) res_lm5 &lt;- lm(medv ~ crim + rm + tax + ptratio + invlstat, data = data2) plot(housing$medv, 1/housing$lstat) summary(res_lm5) #&gt; #&gt; Call: #&gt; lm(formula = medv ~ crim + rm + tax + ptratio + invlstat, data = data2) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -14.9062 -2.6032 -0.5276 2.1041 31.2592 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 6.665018 3.295520 2.022 0.0437 * #&gt; crim -0.119564 0.030121 -3.969 8.26e-05 *** #&gt; rm 3.609393 0.394880 9.140 &lt; 2e-16 *** #&gt; tax -0.002272 0.001693 -1.342 0.1802 #&gt; ptratio -0.665188 0.114156 -5.827 1.01e-08 *** #&gt; invlstat 60.465938 3.762069 16.073 &lt; 2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 4.719 on 500 degrees of freedom #&gt; Multiple R-squared: 0.7393, Adjusted R-squared: 0.7367 #&gt; F-statistic: 283.6 on 5 and 500 DF, p-value: &lt; 2.2e-16 # AIC, BICの計算 AIC(res_lm5, res_lm2) #&gt; df AIC #&gt; res_lm5 7 3014.149 #&gt; res_lm2 7 3115.379 BIC(res_lm5, res_lm2) #&gt; df BIC #&gt; res_lm5 7 3043.735 #&gt; res_lm2 7 3144.965 # 追加 (除去) した変数群の有意性 (例) anova(res_lm1, res_lm3, test = &quot;F&quot;) # F検定 #&gt; Analysis of Variance Table #&gt; #&gt; Model 1: medv ~ crim + rm + tax + lstat #&gt; Model 2: medv ~ crim + rm + tax + lstat + ptratio + zn #&gt; Res.Df RSS Df Sum of Sq F Pr(&gt;F) #&gt; 1 501 14925 #&gt; 2 499 13587 2 1338.1 24.572 6.636e-11 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(res_lm3, res_lm1, test = &quot;F&quot;) # 実質的に同一 #&gt; Analysis of Variance Table #&gt; #&gt; Model 1: medv ~ crim + rm + tax + lstat + ptratio + zn #&gt; Model 2: medv ~ crim + rm + tax + lstat #&gt; Res.Df RSS Df Sum of Sq F Pr(&gt;F) #&gt; 1 499 13587 #&gt; 2 501 14925 -2 -1338.1 24.572 6.636e-11 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # anova(res_lm1, res_lm2, test = &#39;LRT&#39;) # 尤度比検定 ステップワイズ法による変数選択 # step(); AICによって決定 # scope: モデルサーチの範囲(追加や削除を検討するべき変数を指定) # scope指定ない場合: # - directionのデフォルトは, 変数減少法(後方削除) # - モデルサーチ上限(upper)は, 初期モデル # scope指定ある場合: # - directionのデフォルトは, 変数増減法 # - scopeがリストでなく, 単一式で与えらている場合, upperモデルと解釈 (lowerは欠損) res_lm_all &lt;- lm(medv ~ ., data = housing) # → 13変数 res_lm_all_2 &lt;- lm(medv ~ 1, data = housing) # → y切片のみ (変数なし) step(res_lm5) # 変数減少法 (scopeない場合のデフォルト) #&gt; Start: AIC=1576.18 #&gt; medv ~ crim + rm + tax + ptratio + invlstat #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - tax 1 40.1 11175 1576.0 #&gt; &lt;none&gt; 11134 1576.2 #&gt; - crim 1 350.9 11485 1589.9 #&gt; - ptratio 1 756.1 11891 1607.4 #&gt; - rm 1 1860.5 12995 1652.4 #&gt; - invlstat 1 5752.7 16887 1784.9 #&gt; #&gt; Step: AIC=1576 #&gt; medv ~ crim + rm + ptratio + invlstat #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; &lt;none&gt; 11175 1576.0 #&gt; - crim 1 643.8 11818 1602.3 #&gt; - ptratio 1 951.0 12126 1615.3 #&gt; - rm 1 1847.9 13023 1651.4 #&gt; - invlstat 1 6153.6 17328 1796.0 #&gt; #&gt; Call: #&gt; lm(formula = medv ~ crim + rm + ptratio + invlstat, data = data2) #&gt; #&gt; Coefficients: #&gt; (Intercept) crim rm ptratio invlstat #&gt; 6.6395 -0.1399 3.5960 -0.7113 61.4172 # step(res_lm5, direction = &#39;forward&#39;)\\t# 変数増加法 (上限は初期モデル) # step(res_lm5, direction = &#39;both&#39;)\\t# 変数増減法 (上限は初期モデル) # 採用する変数の上限・下限の指定 step(res_lm_all, scope = list(lower = ~crim + rm)) # 下限のモデルを指定. 変数増減法 #&gt; Start: AIC=1589.64 #&gt; medv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + #&gt; tax + ptratio + b + lstat #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - age 1 0.06 11079 1587.7 #&gt; - indus 1 2.52 11081 1587.8 #&gt; &lt;none&gt; 11079 1589.6 #&gt; - chas 1 218.97 11298 1597.5 #&gt; - tax 1 242.26 11321 1598.6 #&gt; - zn 1 257.49 11336 1599.3 #&gt; - b 1 270.63 11349 1599.8 #&gt; - rad 1 479.15 11558 1609.1 #&gt; - nox 1 487.16 11566 1609.4 #&gt; - ptratio 1 1194.23 12273 1639.4 #&gt; - dis 1 1232.41 12311 1641.0 #&gt; - lstat 1 2410.84 13490 1687.3 #&gt; #&gt; Step: AIC=1587.65 #&gt; medv ~ crim + zn + indus + chas + nox + rm + dis + rad + tax + #&gt; ptratio + b + lstat #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - indus 1 2.52 11081 1585.8 #&gt; &lt;none&gt; 11079 1587.7 #&gt; - chas 1 219.91 11299 1595.6 #&gt; - tax 1 242.24 11321 1596.6 #&gt; - zn 1 260.32 11339 1597.4 #&gt; - b 1 272.26 11351 1597.9 #&gt; - rad 1 481.09 11560 1607.2 #&gt; - nox 1 520.87 11600 1608.9 #&gt; - ptratio 1 1200.23 12279 1637.7 #&gt; - dis 1 1352.26 12431 1643.9 #&gt; - lstat 1 2718.88 13798 1696.7 #&gt; #&gt; Step: AIC=1585.76 #&gt; medv ~ crim + zn + chas + nox + rm + dis + rad + tax + ptratio + #&gt; b + lstat #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; &lt;none&gt; 11081 1585.8 #&gt; - chas 1 227.21 11309 1594.0 #&gt; - zn 1 257.82 11339 1595.4 #&gt; - b 1 270.82 11352 1596.0 #&gt; - tax 1 273.62 11355 1596.1 #&gt; - rad 1 500.92 11582 1606.1 #&gt; - nox 1 541.91 11623 1607.9 #&gt; - ptratio 1 1206.45 12288 1636.0 #&gt; - dis 1 1448.94 12530 1645.9 #&gt; - lstat 1 2723.48 13805 1695.0 #&gt; #&gt; Call: #&gt; lm(formula = medv ~ crim + zn + chas + nox + rm + dis + rad + #&gt; tax + ptratio + b + lstat, data = housing) #&gt; #&gt; Coefficients: #&gt; (Intercept) crim zn chas nox rm #&gt; 36.341145 -0.108413 0.045845 2.718716 -17.376023 3.801579 #&gt; dis rad tax ptratio b lstat #&gt; -1.492711 0.299608 -0.011778 -0.946525 0.009291 -0.522553 step(res_lm_all_2, scope = list(upper = ~crim + rm)) # 上限のモデルを指定. 変数増減法 #&gt; Start: AIC=2246.51 #&gt; medv ~ 1 #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; + rm 1 20654.4 22062 1914.2 #&gt; + crim 1 6440.8 36276 2165.8 #&gt; &lt;none&gt; 42716 2246.5 #&gt; #&gt; Step: AIC=1914.19 #&gt; medv ~ rm #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; + crim 1 2496.1 19566 1855.4 #&gt; &lt;none&gt; 22062 1914.2 #&gt; - rm 1 20654.4 42716 2246.5 #&gt; #&gt; Step: AIC=1855.43 #&gt; medv ~ rm + crim #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; &lt;none&gt; 19566 1855.4 #&gt; - crim 1 2496.1 22062 1914.2 #&gt; - rm 1 16709.7 36276 2165.8 #&gt; #&gt; Call: #&gt; lm(formula = medv ~ rm + crim, data = housing) #&gt; #&gt; Coefficients: #&gt; (Intercept) rm crim #&gt; -29.2447 8.3911 -0.2649 step(res_lm1, scope = list(upper = ~crim + rm + tax + lstat + ptratio + b, lower = ~crim + rm)) #&gt; Start: AIC=1722.43 #&gt; medv ~ crim + rm + tax + lstat #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; + ptratio 1 1324.2 13601 1677.4 #&gt; + b 1 255.6 14669 1715.7 #&gt; &lt;none&gt; 14925 1722.4 #&gt; - tax 1 203.1 15128 1727.3 #&gt; - lstat 1 3373.6 18298 1823.5 #&gt; #&gt; Step: AIC=1677.41 #&gt; medv ~ crim + rm + tax + lstat + ptratio #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; + b 1 306.0 13295 1667.9 #&gt; - tax 1 4.9 13606 1675.6 #&gt; &lt;none&gt; 13601 1677.4 #&gt; - ptratio 1 1324.2 14925 1722.4 #&gt; - lstat 1 3286.7 16887 1784.9 #&gt; #&gt; Step: AIC=1667.9 #&gt; medv ~ crim + rm + tax + lstat + ptratio + b #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - tax 1 3.06 13298 1666.0 #&gt; &lt;none&gt; 13295 1667.9 #&gt; - b 1 306.02 13601 1677.4 #&gt; - ptratio 1 1374.66 14669 1715.7 #&gt; - lstat 1 2849.76 16144 1764.2 #&gt; #&gt; Step: AIC=1666.01 #&gt; medv ~ crim + rm + lstat + ptratio + b #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; &lt;none&gt; 13298 1666.0 #&gt; + tax 1 3.06 13295 1667.9 #&gt; - b 1 307.85 13606 1675.6 #&gt; - ptratio 1 1478.71 14776 1717.4 #&gt; - lstat 1 3001.77 16299 1767.0 #&gt; #&gt; Call: #&gt; lm(formula = medv ~ crim + rm + lstat + ptratio + b, data = housing) #&gt; #&gt; Coefficients: #&gt; (Intercept) crim rm lstat ptratio b #&gt; 11.615006 -0.038921 4.788176 -0.495139 -0.877249 0.009593 # 上限・下限を同時に指定. 変数増減法 ----------------------------------------# 5.3 説明変数に質的変数を含む回帰 データセット#3: 高速道路事故データ - Hoffstedt’s Highway accident data - adt：1日の平均交通量（単位：千台) - trks：総交通量に占めるトラック交通量の割合 - lane：交通の総車線数 - acpt：1マイルあたりのアクセスポイント数 - sigs: 1マイルあたりの信号付きインターチェンジの数 - itg：1マイルあたりの高速道路型インターチェンジの数 - slim：1973年の制限速度 - lwid: 車線幅（フィート単位） - shld: 車道の外側路肩の幅（フィート単位) - htype: 道路の種類または道路の財源を示す指標: &quot;mc&quot;: メジャーコレクター (major collector), &quot;fai&quot;: 州間 (interstate) 高速道路, &quot;pa&quot;: 地域・都市間主要幹線 (principal arterial) 道路, &quot;ma&quot;; 地域・都市内主要幹線 (major arterial) 道路 - rate: 1973年の事故発生率（百万車両マイル当たり） - 注) htypeは4-水準因子 - 参考文献: Weisberg (2014), Applied Linear Regression, 4th Ed., Wiley. library(alr4) data(Highway) str(Highway) #&gt; &#39;data.frame&#39;: 39 obs. of 12 variables: #&gt; $ adt : int 69 73 49 61 28 30 46 25 43 23 ... #&gt; $ trks : int 8 8 10 13 12 6 8 9 12 7 ... #&gt; $ lane : int 8 4 4 6 4 4 4 4 4 4 ... #&gt; $ acpt : num 4.6 4.4 4.7 3.8 2.2 24.8 11 18.5 7.5 8.2 ... #&gt; $ sigs : num 0 0 0 0 0 1.84 0.7 0.38 1.39 1.21 ... #&gt; $ itg : num 1.2 1.43 1.54 0.94 0.65 0.34 0.47 0.38 0.95 0.12 ... #&gt; $ slim : int 55 60 60 65 70 55 55 55 50 50 ... #&gt; $ len : num 4.99 16.11 9.75 10.65 20.01 ... #&gt; $ lwid : int 12 12 12 12 12 12 12 12 12 12 ... #&gt; $ shld : int 10 10 10 10 10 10 8 10 4 5 ... #&gt; $ htype: Factor w/ 4 levels &quot;mc&quot;,&quot;fai&quot;,&quot;pa&quot;,..: 2 2 2 2 2 3 3 3 3 3 ... #&gt; $ rate : num 4.58 2.86 3.02 2.29 1.61 6.87 3.85 6.12 3.29 5.88 ... library(corrplot) cor_hw &lt;- cor(cbind(Highway$rate, Highway[, -(11:12)])) # htypeを除去 corrplot.mixed(cor_hw) 多重共線性のチェック (VIF) # VIF install.packages(&#39;car&#39;)\\t\\t# or RStudio, Tools → Install packages, library(car) # &#39;Companion to Applied Regression&#39; package vif(res_lm) #&gt; area yrs dist #&gt; 1.003378 1.006251 1.003784 # OLS回帰 res_lm &lt;- lm(rate ~ ., data = Highway) summary(res_lm) #&gt; #&gt; Call: #&gt; lm(formula = rate ~ ., data = Highway) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -1.99564 -0.62039 -0.05676 0.61741 2.54998 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 13.658212 6.872719 1.987 0.0579 . #&gt; adt -0.004038 0.033945 -0.119 0.9063 #&gt; trks -0.100150 0.114726 -0.873 0.3910 #&gt; lane 0.026675 0.283834 0.094 0.9259 #&gt; acpt 0.066588 0.042569 1.564 0.1303 #&gt; sigs 0.713644 0.525213 1.359 0.1864 #&gt; itg -0.475478 1.282742 -0.371 0.7140 #&gt; slim -0.123778 0.081683 -1.515 0.1422 #&gt; len -0.064751 0.033369 -1.940 0.0637 . #&gt; lwid -0.133813 0.597917 -0.224 0.8247 #&gt; shld 0.014113 0.162174 0.087 0.9313 #&gt; htypefai 0.543592 1.728270 0.315 0.7557 #&gt; htypepa -1.009777 1.105612 -0.913 0.3698 #&gt; htypema -0.548025 0.975623 -0.562 0.5793 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.198 on 25 degrees of freedom #&gt; Multiple R-squared: 0.7605, Adjusted R-squared: 0.636 #&gt; F-statistic: 6.107 on 13 and 25 DF, p-value: 5.733e-05 # ステップワイズ回帰 res_step &lt;- step(res_lm) #&gt; Start: AIC=24.76 #&gt; rate ~ adt + trks + lane + acpt + sigs + itg + slim + len + lwid + #&gt; shld + htype #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - htype 3 3.1001 38.994 21.994 #&gt; - shld 1 0.0109 35.905 22.775 #&gt; - lane 1 0.0127 35.906 22.777 #&gt; - adt 1 0.0203 35.914 22.785 #&gt; - lwid 1 0.0719 35.966 22.841 #&gt; - itg 1 0.1973 36.091 22.977 #&gt; - trks 1 1.0941 36.988 23.934 #&gt; &lt;none&gt; 35.894 24.763 #&gt; - sigs 1 2.6508 38.544 25.542 #&gt; - slim 1 3.2969 39.191 26.190 #&gt; - acpt 1 3.5130 39.407 26.405 #&gt; - len 1 5.4061 41.300 28.235 #&gt; #&gt; Step: AIC=21.99 #&gt; rate ~ adt + trks + lane + acpt + sigs + itg + slim + len + lwid + #&gt; shld #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - adt 1 0.0131 39.007 20.007 #&gt; - lane 1 0.0296 39.023 20.023 #&gt; - itg 1 0.0431 39.037 20.037 #&gt; - lwid 1 0.9119 39.906 20.895 #&gt; - sigs 1 0.9652 39.959 20.948 #&gt; - shld 1 0.9680 39.962 20.950 #&gt; - slim 1 1.3365 40.330 21.308 #&gt; - trks 1 1.7899 40.784 21.744 #&gt; &lt;none&gt; 38.994 21.994 #&gt; - len 1 5.2726 44.266 24.940 #&gt; - acpt 1 11.8141 50.808 30.315 #&gt; #&gt; Step: AIC=20.01 #&gt; rate ~ trks + lane + acpt + sigs + itg + slim + len + lwid + #&gt; shld #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - lane 1 0.0780 39.085 18.085 #&gt; - itg 1 0.2582 39.265 18.264 #&gt; - lwid 1 0.8988 39.906 18.895 #&gt; - shld 1 0.9652 39.972 18.960 #&gt; - sigs 1 0.9786 39.986 18.973 #&gt; - slim 1 1.4501 40.457 19.430 #&gt; - trks 1 1.7771 40.784 19.744 #&gt; &lt;none&gt; 39.007 20.007 #&gt; - len 1 5.3262 44.333 22.999 #&gt; - acpt 1 11.9079 50.915 28.397 #&gt; #&gt; Step: AIC=18.08 #&gt; rate ~ trks + acpt + sigs + itg + slim + len + lwid + shld #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - itg 1 0.7717 39.857 16.847 #&gt; - lwid 1 0.8615 39.946 16.935 #&gt; - shld 1 0.8874 39.972 16.960 #&gt; - sigs 1 1.3821 40.467 17.440 #&gt; - slim 1 1.5009 40.586 17.554 #&gt; - trks 1 1.8331 40.918 17.872 #&gt; &lt;none&gt; 39.085 18.085 #&gt; - len 1 5.2498 44.335 21.000 #&gt; - acpt 1 11.9551 51.040 26.493 #&gt; #&gt; Step: AIC=16.85 #&gt; rate ~ trks + acpt + sigs + slim + len + lwid + shld #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - shld 1 0.5989 40.456 15.429 #&gt; - lwid 1 0.7972 40.654 15.620 #&gt; - slim 1 1.5121 41.369 16.300 #&gt; - sigs 1 1.7001 41.557 16.476 #&gt; - trks 1 1.7312 41.588 16.506 #&gt; &lt;none&gt; 39.857 16.847 #&gt; - len 1 6.3017 46.158 20.572 #&gt; - acpt 1 11.3358 51.192 24.609 #&gt; #&gt; Step: AIC=15.43 #&gt; rate ~ trks + acpt + sigs + slim + len + lwid #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - lwid 1 0.4727 40.928 13.882 #&gt; - trks 1 1.4809 41.936 14.831 #&gt; - sigs 1 1.4984 41.954 14.847 #&gt; &lt;none&gt; 40.456 15.429 #&gt; - slim 1 5.6245 46.080 18.506 #&gt; - len 1 5.7031 46.159 18.572 #&gt; - acpt 1 11.5064 51.962 23.191 #&gt; #&gt; Step: AIC=13.88 #&gt; rate ~ trks + acpt + sigs + slim + len #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - trks 1 1.4051 42.333 13.198 #&gt; - sigs 1 1.5118 42.440 13.297 #&gt; &lt;none&gt; 40.928 13.882 #&gt; - len 1 5.2305 46.159 16.573 #&gt; - slim 1 6.0861 47.014 17.289 #&gt; - acpt 1 11.6253 52.554 21.633 #&gt; #&gt; Step: AIC=13.2 #&gt; rate ~ acpt + sigs + slim + len #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; &lt;none&gt; 42.333 13.198 #&gt; - sigs 1 2.5132 44.847 13.448 #&gt; - slim 1 6.3349 48.668 16.637 #&gt; - len 1 9.1881 51.521 18.859 #&gt; - acpt 1 12.5355 54.869 21.314 summary(res_step) #&gt; #&gt; Call: #&gt; lm(formula = rate ~ acpt + sigs + slim + len, data = Highway) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -1.7505 -0.8659 0.1051 0.6618 2.5116 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 8.81443 2.60435 3.385 0.00181 ** #&gt; acpt 0.08940 0.02818 3.173 0.00319 ** #&gt; sigs 0.48538 0.34164 1.421 0.16450 #&gt; slim -0.09599 0.04255 -2.256 0.03064 * #&gt; len -0.06856 0.02524 -2.717 0.01030 * #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.116 on 34 degrees of freedom #&gt; Multiple R-squared: 0.7176, Adjusted R-squared: 0.6843 #&gt; F-statistic: 21.6 on 4 and 34 DF, p-value: 6.112e-09 ## 除かれた変数群の有意性 anova(res_step, res_lm) # anova(res_lm, res_step) 5.4 時系列データ同士の回帰 データセット#4: 株式市場 - TOPIX: 東証株価指数, 月次終値, 2011年12月--2017年4月 - X1--X4, ある指標 topixmat &lt;- read.csv(&quot;topix_X.csv&quot;) attach(topixmat) res_lm1 &lt;- lm(topix ~ X1) summary(res_lm1) #&gt; #&gt; Call: #&gt; lm(formula = topix ~ X1) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -391.79 -173.11 -16.77 82.05 452.93 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -170.600 219.329 -0.778 0.44 #&gt; X1 11.635 1.805 6.448 1.82e-08 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 220.2 on 63 degrees of freedom #&gt; Multiple R-squared: 0.3975, Adjusted R-squared: 0.388 #&gt; F-statistic: 41.57 on 1 and 63 DF, p-value: 1.815e-08 par(mfrow = c(1, 1)) plot(X1, topix) cor(X1, topix) #&gt; [1] 0.6305084 par(mfrow = c(2, 1)) plot(topix, type = &quot;l&quot;) plot(X1, type = &quot;l&quot;) res_lm2 &lt;- lm(topix ~ X2) summary(res_lm2) #&gt; #&gt; Call: #&gt; lm(formula = topix ~ X2) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -396.10 -182.85 -57.86 198.61 599.41 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 153.656 243.855 0.63 0.531 #&gt; X2 8.739 1.960 4.46 3.45e-05 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 247.3 on 63 degrees of freedom #&gt; Multiple R-squared: 0.2399, Adjusted R-squared: 0.2279 #&gt; F-statistic: 19.89 on 1 and 63 DF, p-value: 3.451e-05 plot(X2, topix) cor(X2, topix) #&gt; [1] 0.4898424 par(mfrow = c(2, 1)) plot(topix, type = &quot;l&quot;) plot(X2, type = &quot;l&quot;) # 残差プロット plot(res_lm1); plot(res_lm2) # Durbin - Watson検定 library(lmtest) dwtest(res_lm1) #&gt; #&gt; Durbin-Watson test #&gt; #&gt; data: res_lm1 #&gt; DW = 0.17012, p-value &lt; 2.2e-16 #&gt; alternative hypothesis: true autocorrelation is greater than 0 dwtest(res_lm2) #&gt; #&gt; Durbin-Watson test #&gt; #&gt; data: res_lm2 #&gt; DW = 0.1226, p-value &lt; 2.2e-16 #&gt; alternative hypothesis: true autocorrelation is greater than 0 5.5 線形モデル 本セクションでは, 線形モデルと関数lmの出力結果との対応関係の確認を行う. 具体的には, 目的変数が量的変数で, いずれもカテゴリー変数である2つの説明変数を持つ線形モデル (2因子モデル) を取り上げる. サプリメント売上データに対して”回帰分析” (関数lm()を適用) することで得られる 回帰係数の推定値を調べることで, 線形モデルの持つパラメータの意味を理解する. さらに, Rの環境設定のための関数options()のパラメータの一つであるコントラスト(“contrasts”)を変えることで, パラメータの持つ意味が, すなわち, 解釈が変わることも確認する. 2因子モデルの例 (交互作用なし) データ: サプリメント売上データ (1) (仮想) - sales1_10.txt, sales2_10.txt (過去10分売上) - 性別 (因子A) - 年代 (因子B) - 売上 (万円) # 因子A (性別) 因子B (30代以下, 40代以上) サプリメント売上(万円) # 過去n週間データ (n = 10, 50) ### n = 10のケース sales &lt;- read.csv(&quot;sales1_10.txt&quot;) # sales &lt;- read.csv(&#39;sales2_10.txt&#39;) sales #&gt; 性別 年代 売上 #&gt; 1 A1 B1 14.06 #&gt; 2 A1 B1 13.45 #&gt; 3 A1 B1 9.89 #&gt; 4 A1 B1 12.20 #&gt; 5 A1 B1 14.88 #&gt; 6 A1 B1 15.17 #&gt; 7 A1 B1 10.38 #&gt; 8 A1 B1 12.91 #&gt; 9 A1 B1 9.12 #&gt; 10 A1 B1 13.23 #&gt; 11 A1 B2 21.31 #&gt; 12 A1 B2 20.27 #&gt; 13 A1 B2 17.29 #&gt; 14 A1 B2 20.96 #&gt; 15 A1 B2 20.22 #&gt; 16 A1 B2 18.27 #&gt; 17 A1 B2 15.14 #&gt; 18 A1 B2 17.41 #&gt; 19 A1 B2 20.78 #&gt; 20 A1 B2 19.45 #&gt; 21 A2 B1 22.21 #&gt; 22 A2 B1 17.44 #&gt; 23 A2 B1 21.98 #&gt; 24 A2 B1 17.64 #&gt; 25 A2 B1 20.20 #&gt; 26 A2 B1 22.88 #&gt; 27 A2 B1 21.94 #&gt; 28 A2 B1 21.38 #&gt; 29 A2 B1 23.69 #&gt; 30 A2 B1 23.24 #&gt; 31 A2 B2 22.44 #&gt; 32 A2 B2 27.77 #&gt; 33 A2 B2 30.91 #&gt; 34 A2 B2 28.55 #&gt; 35 A2 B2 23.86 #&gt; 36 A2 B2 23.69 #&gt; 37 A2 B2 29.09 #&gt; 38 A2 B2 22.72 #&gt; 39 A2 B2 27.03 #&gt; 40 A2 B2 26.05 # クロス集計 table(sales[, -3]) #&gt; 年代 #&gt; 性別 B1 B2 #&gt; A1 10 10 #&gt; A2 10 10 # 各グループの平均売上計算 aggregate(売上 ~ 性別 + 年代, data = sales, FUN = mean) # 性別・年代別平均 #&gt; 性別 年代 売上 #&gt; 1 A1 B1 12.529 #&gt; 2 A2 B1 21.260 #&gt; 3 A1 B2 19.110 #&gt; 4 A2 B2 26.211 主効果モデル (交互作用なし) # lm関数の実行 res_lm1 &lt;- lm(売上 ~ 性別 + 年代, data = sales) summary(res_lm1) #&gt; #&gt; Call: #&gt; lm(formula = 売上 ~ 性別 + 年代, data = sales) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -4.1785 -1.6985 0.5205 1.9345 4.2915 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 12.9365 0.6417 20.161 &lt; 2e-16 *** #&gt; 性別A2 7.9160 0.7409 10.684 7.33e-13 *** #&gt; 年代B2 5.7660 0.7409 7.782 2.64e-09 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 2.343 on 37 degrees of freedom #&gt; Multiple R-squared: 0.8252, Adjusted R-squared: 0.8158 #&gt; F-statistic: 87.35 on 2 and 37 DF, p-value: 9.677e-15 2因子モデルの例 (交互作用有り) 飽和モデル (主効果 + 交互作用効果) res_lm2 &lt;- lm(売上 ~ 性別 * 年代, data = sales) summary(res_lm2) #&gt; #&gt; Call: #&gt; lm(formula = 売上 ~ 性別 * 年代, data = sales) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.9700 -1.9023 0.6905 1.6325 4.6990 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 12.5290 0.7388 16.959 &lt; 2e-16 *** #&gt; 性別A2 8.7310 1.0448 8.357 5.96e-10 *** #&gt; 年代B2 6.5810 1.0448 6.299 2.78e-07 *** #&gt; 性別A2:年代B2 -1.6300 1.4775 -1.103 0.277 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 2.336 on 36 degrees of freedom #&gt; Multiple R-squared: 0.8309, Adjusted R-squared: 0.8169 #&gt; F-statistic: 58.98 on 3 and 36 DF, p-value: 5.697e-14 anova(res_lm2) #&gt; Analysis of Variance Table #&gt; #&gt; Response: 売上 #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; 性別 1 626.63 626.63 114.813 9.506e-13 *** #&gt; 年代 1 332.47 332.47 60.916 2.975e-09 *** #&gt; 性別:年代 1 6.64 6.64 1.217 0.2773 #&gt; Residuals 36 196.48 5.46 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # 交互作用プロット attach(sales) interaction.plot(年代, 性別, response = 売上) データ: サプリメント売上データ (2) (仮想) 上と同じデータ項目を持つが, 件数が\\(n=10 \\rightarrow 50\\)に増加したデータセット. - sales1_50.txt, sales2_50.txt (過去50分売上) - 性別 (因子A) - 年代 (因子B) - 売上 (万円) ### n = 50のケース sales2 &lt;- read.csv(&quot;sales1_50.txt&quot;) # sales2 &lt;- read.csv(&#39;sales2_50.txt&#39;) # クロス集計 table(sales2[, -3]) #&gt; 年代 #&gt; 性別 B1 B2 #&gt; A1 50 50 #&gt; A2 50 50 # 各グループの平均売上計算 aggregate(売上 ~ 性別 + 年代, data = sales2, FUN = mean) # 性別・年代別平均 #&gt; 性別 年代 売上 #&gt; 1 A1 B1 12.9766 #&gt; 2 A2 B1 24.0718 #&gt; 3 A1 B2 18.2048 #&gt; 4 A2 B2 27.3582 主効果モデル (交互作用なし) # 主効果モデル res_lm1 &lt;- lm(売上 ~ 性別 + 年代, data = sales2) summary(res_lm1) #&gt; #&gt; Call: #&gt; lm(formula = 売上 ~ 性別 + 年代, data = sales2) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -6.8037 -2.2375 0.0243 2.2664 6.9406 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 13.4621 0.3516 38.29 &lt;2e-16 *** #&gt; 性別A2 10.1243 0.4060 24.94 &lt;2e-16 *** #&gt; 年代B2 4.2573 0.4060 10.49 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 2.871 on 197 degrees of freedom #&gt; Multiple R-squared: 0.7879, Adjusted R-squared: 0.7858 #&gt; F-statistic: 365.9 on 2 and 197 DF, p-value: &lt; 2.2e-16 飽和モデル (主効果 + 交互作用効果) # 飽和モデル (主効果+交互作用効果) res_lm2 &lt;- lm(売上 ~ 性別 * 年代, data = sales2) summary(res_lm2) #&gt; #&gt; Call: #&gt; lm(formula = 売上 ~ 性別 * 年代, data = sales2) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -6.3182 -2.0093 0.2117 2.1238 6.8418 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 12.9766 0.4011 32.355 &lt;2e-16 *** #&gt; 性別A2 11.0952 0.5672 19.561 &lt;2e-16 *** #&gt; 年代B2 5.2282 0.5672 9.217 &lt;2e-16 *** #&gt; 性別A2:年代B2 -1.9418 0.8021 -2.421 0.0164 * #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 2.836 on 196 degrees of freedom #&gt; Multiple R-squared: 0.7941, Adjusted R-squared: 0.7909 #&gt; F-statistic: 251.9 on 3 and 196 DF, p-value: &lt; 2.2e-16 anova(res_lm2) #&gt; Analysis of Variance Table #&gt; #&gt; Response: 売上 #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; 性別 1 5125.1 5125.1 637.21 &lt;2e-16 *** #&gt; 年代 1 906.2 906.2 112.67 &lt;2e-16 *** #&gt; 性別:年代 1 47.1 47.1 5.86 0.0164 * #&gt; Residuals 196 1576.4 8.0 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(res_lm1, res_lm2, test = &quot;F&quot;) #&gt; Analysis of Variance Table #&gt; #&gt; Model 1: 売上 ~ 性別 + 年代 #&gt; Model 2: 売上 ~ 性別 * 年代 #&gt; Res.Df RSS Df Sum of Sq F Pr(&gt;F) #&gt; 1 197 1623.6 #&gt; 2 196 1576.4 1 47.132 5.86 0.0164 * #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # 交互作用プロット interaction.plot(年代, 性別, response = 売上) 予測 (理論値) sales_pred1 &lt;- data.frame(sales2, predict(res_lm1)) head(sales_pred1, 10) #&gt; 性別 年代 売上 predict.res_lm1. #&gt; 1 A1 B1 14.06 13.46205 #&gt; 2 A1 B1 13.45 13.46205 #&gt; 3 A1 B1 9.89 13.46205 #&gt; 4 A1 B1 12.20 13.46205 #&gt; 5 A1 B1 14.88 13.46205 #&gt; 6 A1 B1 15.17 13.46205 #&gt; 7 A1 B1 10.38 13.46205 #&gt; 8 A1 B1 12.91 13.46205 #&gt; 9 A1 B1 9.12 13.46205 #&gt; 10 A1 B1 13.23 13.46205 sales_pred2 &lt;- data.frame(sales2, predict(res_lm2)) head(sales_pred2, 10) #&gt; 性別 年代 売上 predict.res_lm2. #&gt; 1 A1 B1 14.06 12.9766 #&gt; 2 A1 B1 13.45 12.9766 #&gt; 3 A1 B1 9.89 12.9766 #&gt; 4 A1 B1 12.20 12.9766 #&gt; 5 A1 B1 14.88 12.9766 #&gt; 6 A1 B1 15.17 12.9766 #&gt; 7 A1 B1 10.38 12.9766 #&gt; 8 A1 B1 12.91 12.9766 #&gt; 9 A1 B1 9.12 12.9766 #&gt; 10 A1 B1 13.23 12.9766 “コントラスト”の設定変更 関数options()のパラメータの一つであるコントラスト(“contrasts”)を変えることで, パラメータの持つ意味が, すなわち, 解釈が変わる. Rのデフォルトは, 処置対比 (“contr.treatment”). #options(&quot;contrasts&quot;) #options(&quot;contrasts&quot; = c(&quot;contr.treatment&quot;, &quot;contr.poly&quot;)) # デフォルト #options(&quot;contrasts&quot; = c(&quot;contr.sum&quot;, &quot;contr.poly&quot;)) 零和対比 (“contr.sum) に変更した場合. - 主効果モデル (交互作用なし) options(contrasts = c(&quot;contr.sum&quot;, &quot;contr.poly&quot;)) # 主効果モデル res_lm1 &lt;- lm(売上 ~ 性別 + 年代, data = sales2) summary(res_lm1) #&gt; #&gt; Call: #&gt; lm(formula = 売上 ~ 性別 + 年代, data = sales2) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -6.8037 -2.2375 0.0243 2.2664 6.9406 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 20.653 0.203 101.74 &lt;2e-16 *** #&gt; 性別1 -5.062 0.203 -24.94 &lt;2e-16 *** #&gt; 年代1 -2.129 0.203 -10.49 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 2.871 on 197 degrees of freedom #&gt; Multiple R-squared: 0.7879, Adjusted R-squared: 0.7858 #&gt; F-statistic: 365.9 on 2 and 197 DF, p-value: &lt; 2.2e-16 飽和モデル (主効果 + 交互作用効果) # 飽和モデル (主効果+交互作用効果) res_lm2 &lt;- lm(売上 ~ 性別 * 年代, data = sales2) summary(res_lm2) #&gt; #&gt; Call: #&gt; lm(formula = 売上 ~ 性別 * 年代, data = sales2) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -6.3182 -2.0093 0.2117 2.1239 6.8418 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 20.6528 0.2005 102.988 &lt;2e-16 *** #&gt; 性別1 -5.0622 0.2005 -25.243 &lt;2e-16 *** #&gt; 年代1 -2.1286 0.2005 -10.615 &lt;2e-16 *** #&gt; 性別1:年代1 -0.4855 0.2005 -2.421 0.0164 * #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 2.836 on 196 degrees of freedom #&gt; Multiple R-squared: 0.7941, Adjusted R-squared: 0.7909 #&gt; F-statistic: 251.9 on 3 and 196 DF, p-value: &lt; 2.2e-16 "],["ロジットブロビット回帰分析.html", "6 ロジット/ブロビット回帰分析 6.1 ロジット回帰分析の基本操作 6.2 データ分析例 6.3 疑似R2の計算 6.4 関数factor()について", " 6 ロジット/ブロビット回帰分析 6.1 ロジット回帰分析の基本操作 ロジットモデル: シミュレーションデータ シミュレーションデータの生成 set.seed(1) n &lt;- 100 p &lt;- 2 a &lt;- 1.2 b &lt;- c(0.5, 1.5) X &lt;- matrix(runif(n * p, -5, 5), ncol = p) # 予測変数 (X1, X2) colnames(X) &lt;- paste0(&quot;X&quot;, 1:p) mu &lt;- a + X %*% b # 線形予測子 pi &lt;- exp(mu)/(1 + exp(mu)) # ロジスティック変換 y &lt;- rbinom(n, 1, pi) # 発生頻度 (ランダム) plot(X[, 1], y) plot(X[, 2], y) ロジット回帰分析の実行 # ロジット回帰 res_glm &lt;- glm(y ~ X, family = binomial) summary(res_glm) #&gt; #&gt; Call: #&gt; glm(formula = y ~ X, family = binomial) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -1.79161 -0.08440 0.00084 0.09128 2.36307 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 2.5815 0.8647 2.985 0.002831 ** #&gt; XX1 1.1494 0.3434 3.347 0.000818 *** #&gt; XX2 2.7630 0.7509 3.680 0.000233 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 133.750 on 99 degrees of freedom #&gt; Residual deviance: 34.478 on 97 degrees of freedom #&gt; AIC: 40.478 #&gt; #&gt; Number of Fisher Scoring iterations: 8 予測 # 予測 (内挿) mu_hat &lt;- predict(res_glm) # μ p_hat &lt;- predict(res_glm, type = &quot;response&quot;) # 発生頻度 head(data.frame(y, p_hat)) #&gt; y p_hat #&gt; 1 1 0.9846543 #&gt; 2 0 0.0500027 #&gt; 3 0 0.0507452 #&gt; 4 1 1.0000000 #&gt; 5 1 0.9448650 #&gt; 6 0 0.3178520 plot(mu_hat, p_hat) 係数の信頼区間 # 信頼区間 confint(res_glm) # 95%信頼区間 (デフォルト) #&gt; 2.5 % 97.5 % #&gt; (Intercept) 1.1914472 4.659417 #&gt; XX1 0.5901329 1.974012 #&gt; XX2 1.5989191 4.620681 confint(res_glm, level = 0.9) # 90%信頼区間 #&gt; 5 % 95 % #&gt; (Intercept) 1.3787136 4.269921 #&gt; XX1 0.6664132 1.818782 #&gt; XX2 1.7488603 4.265779 6.2 データ分析例 データセット (1): 販売プロモーション・データ (仮想) - convdat.csv, 8件 - 商品 (product): A/B - 世代 (genZ): Yes/No - K-pop好き (Kpop): Yes/No - DM送付数 (n.tot): 人 - 購入人数 (n.conv): 人 - 注) product--Kpopは, 関数read.csv()でそのまま読み込むと文字型変数となり, 因子型にはならない. データ読み込み conv_dat1 &lt;- read.csv(&quot;convdat.txt&quot;, skip = 1) # 注) デフォルトはstringsAsFactors = F (文字列を因子型変数に変換せずに読み込む) # conv_dat1 &lt;- read.csv(&#39;convdat.txt&#39;, skip = 2, stringsAsFactors = T) attach(conv_dat1) ロジット回帰実行 異なるデータ形式への対応 # データ形式-1 &#39;成功回数&#39;、&#39;失敗回数&#39;の2列 conv_tbl &lt;- cbind(n.conv, n.tot - n.conv) res_glm1 &lt;- glm(conv_tbl ~ product + genZ + Kpop, family = binomial) summary(res_glm1) #&gt; #&gt; Call: #&gt; glm(formula = conv_tbl ~ product + genZ + Kpop, family = binomial) #&gt; #&gt; Deviance Residuals: #&gt; 1 2 3 4 5 6 7 8 #&gt; 0.11781 0.18151 0.15625 -1.18807 -0.35661 -0.40158 0.78239 0.09386 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -2.07205 0.33431 -6.198 5.72e-10 *** #&gt; productB -0.07844 0.26926 -0.291 0.77082 #&gt; genZYes 0.82333 0.27607 2.982 0.00286 ** #&gt; KpopYes 0.61585 0.35132 1.753 0.07961 . #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 15.2547 on 7 degrees of freedom #&gt; Residual deviance: 2.3921 on 4 degrees of freedom #&gt; AIC: 36.198 #&gt; #&gt; Number of Fisher Scoring iterations: 4 res_glm2 &lt;- glm(conv_tbl ~ genZ + Kpop, binomial) summary(res_glm2) #&gt; #&gt; Call: #&gt; glm(formula = conv_tbl ~ genZ + Kpop, family = binomial) #&gt; #&gt; Deviance Residuals: #&gt; 1 2 3 4 5 6 7 8 #&gt; 0.16285 0.20322 0.28582 -1.22028 -0.54755 -0.32113 0.66014 0.01463 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -2.0904 0.3286 -6.361 2e-10 *** #&gt; genZYes 0.8235 0.2760 2.983 0.00285 ** #&gt; KpopYes 0.6100 0.3507 1.739 0.08200 . #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 15.2547 on 7 degrees of freedom #&gt; Residual deviance: 2.4775 on 5 degrees of freedom #&gt; AIC: 34.284 #&gt; #&gt; Number of Fisher Scoring iterations: 4 res_glm0 &lt;- glm(conv_tbl ~ 1, binomial) # 切片項のみ summary(res_glm0) #&gt; #&gt; Call: #&gt; glm(formula = conv_tbl ~ 1, family = binomial) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -1.6946 -1.0354 -0.5135 0.7712 2.2549 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -1.3950 0.1205 -11.58 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 15.255 on 7 degrees of freedom #&gt; Residual deviance: 15.255 on 7 degrees of freedom #&gt; AIC: 43.061 #&gt; #&gt; Number of Fisher Scoring iterations: 4 anova(res_glm2, test = &quot;Chisq&quot;) # カイ2乗検定 (test = &#39;LRT&#39;でも可) #&gt; Analysis of Deviance Table #&gt; #&gt; Model: binomial, link: logit #&gt; #&gt; Response: conv_tbl #&gt; #&gt; Terms added sequentially (first to last) #&gt; #&gt; #&gt; Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) #&gt; NULL 7 15.2547 #&gt; genZ 1 9.4371 6 5.8175 0.002126 ** #&gt; Kpop 1 3.3400 5 2.4775 0.067615 . #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(res_glm2, res_glm0, test = &quot;Chisq&quot;) # 同 #&gt; Analysis of Deviance Table #&gt; #&gt; Model 1: conv_tbl ~ genZ + Kpop #&gt; Model 2: conv_tbl ~ 1 #&gt; Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) #&gt; 1 5 2.4775 #&gt; 2 7 15.2547 -2 -12.777 0.001681 ** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # データ形式-2 &#39;成功率&#39;の指定 prop_conv &lt;- n.conv/n.tot res_glm1_2 &lt;- glm(prop_conv ~ product + genZ + Kpop, binomial, weights = n.tot) summary(res_glm1_2) #&gt; #&gt; Call: #&gt; glm(formula = prop_conv ~ product + genZ + Kpop, family = binomial, #&gt; weights = n.tot) #&gt; #&gt; Deviance Residuals: #&gt; 1 2 3 4 5 6 7 8 #&gt; 0.11781 0.18151 0.15625 -1.18807 -0.35661 -0.40158 0.78239 0.09386 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -2.07205 0.33431 -6.198 5.72e-10 *** #&gt; productB -0.07844 0.26926 -0.291 0.77082 #&gt; genZYes 0.82333 0.27607 2.982 0.00286 ** #&gt; KpopYes 0.61585 0.35132 1.753 0.07961 . #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 15.2547 on 7 degrees of freedom #&gt; Residual deviance: 2.3921 on 4 degrees of freedom #&gt; AIC: 36.198 #&gt; #&gt; Number of Fisher Scoring iterations: 4 # プロビット回帰 res_glm2_p &lt;- glm(conv_tbl ~ genZ + Kpop, family = binomial(link = &quot;probit&quot;)) # probit summary(res_glm2_p) #&gt; #&gt; Call: #&gt; glm(formula = conv_tbl ~ genZ + Kpop, family = binomial(link = &quot;probit&quot;)) #&gt; #&gt; Deviance Residuals: #&gt; 1 2 3 4 5 6 7 8 #&gt; 0.17291 0.15015 0.27674 -1.24447 -0.55358 -0.29829 0.67558 0.02009 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -1.2285 0.1742 -7.053 1.75e-12 *** #&gt; genZYes 0.4814 0.1644 2.929 0.0034 ** #&gt; KpopYes 0.3343 0.1886 1.773 0.0763 . #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 15.255 on 7 degrees of freedom #&gt; Residual deviance: 2.530 on 5 degrees of freedom #&gt; AIC: 34.336 #&gt; #&gt; Number of Fisher Scoring iterations: 3 # ロジット回帰 (再実行) res_glm2 &lt;- glm(conv_tbl ~ genZ + Kpop, binomial) # simpler, logit summary(res_glm2) #&gt; #&gt; Call: #&gt; glm(formula = conv_tbl ~ genZ + Kpop, family = binomial) #&gt; #&gt; Deviance Residuals: #&gt; 1 2 3 4 5 6 7 8 #&gt; 0.16285 0.20322 0.28582 -1.22028 -0.54755 -0.32113 0.66014 0.01463 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -2.0904 0.3286 -6.361 2e-10 *** #&gt; genZYes 0.8235 0.2760 2.983 0.00285 ** #&gt; KpopYes 0.6100 0.3507 1.739 0.08200 . #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 15.2547 on 7 degrees of freedom #&gt; Residual deviance: 2.4775 on 5 degrees of freedom #&gt; AIC: 34.284 #&gt; #&gt; Number of Fisher Scoring iterations: 4 # モデル診断 plot(res_glm2) 予測 # 予測 predict(res_glm2) # 対数オッズ #&gt; 1 2 3 4 5 6 7 8 #&gt; -2.090427 -1.266936 -1.480454 -1.266936 -1.480454 -0.656963 -0.656963 -2.090427 p_hat &lt;- predict(res_glm2, type = &quot;response&quot;) # 確率 data.frame(conv_dat1, p_hat) #&gt; product genZ Kpop n.tot n.conv p_hat #&gt; 1 A No No 60 7 0.1100307 #&gt; 2 A Yes No 8 2 0.2197822 #&gt; 3 A No Yes 186 36 0.1853588 #&gt; 4 B Yes No 3 0 0.2197822 #&gt; 5 B No Yes 86 14 0.1853588 #&gt; 6 A Yes Yes 50 16 0.3414222 #&gt; 7 B Yes Yes 22 9 0.3414222 #&gt; 8 B No No 18 2 0.1100307 # 注) 説明変数が因子型(factor)でない場合, 以下エラーとなる (read.csvで, # stringsAsFactors = Tを指定していた場合はOK) plot(genZ, predict(res_glm2, type # = &#39;response&#39;)) plot(cat, predict(res_glm2, type = &#39;response&#39;)) すなわち, # 文字列型のままの場合, は因子型に一旦変換が必要 plot(factor(genZ), # predict(res_glm2, type = &#39;response&#39;)) plot(factor(cat), predict(res_glm2, # type = &#39;response&#39;)) # 説明変数が数値型変数ならば、logistic曲線を描く 係数の信頼区間 # 信頼区間 confint(res_glm1_2) # 95%信頼区間 (デフォルト) #&gt; 2.5 % 97.5 % #&gt; (Intercept) -2.78200808 -1.4598384 #&gt; productB -0.61946141 0.4396708 #&gt; genZYes 0.27372310 1.3592877 #&gt; KpopYes -0.03750972 1.3520115 confint(res_glm1_2, level = 0.9) # 90%信頼区間 #&gt; 5 % 95 % #&gt; (Intercept) -2.65972908 -1.5530190 #&gt; productB -0.53045473 0.3574811 #&gt; genZYes 0.36344375 1.2736384 #&gt; KpopYes 0.06339516 1.2264225 detach() データセット (2): 個人ローン・デフォルト・データ (仮想) - default.csv, 100件 - デフォルト (1/0) - ローン残高 (万円) - 収入 (万円) - 職種 (A/B) データ読み込み default &lt;- read.csv(&quot;default.csv&quot;) attach(default) ロジット回帰 # ロジット回帰 res_glm &lt;- glm(デフォルト ~ ローン残高 + 収入 + 職種, family = binomial) summary(res_glm) #&gt; #&gt; Call: #&gt; glm(formula = デフォルト ~ ローン残高 + 収入 + 職種, #&gt; family = binomial) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.20685 -0.21788 -0.04981 -0.00258 2.84704 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -1.336e+01 3.505e+00 -3.811 0.000138 *** #&gt; ローン残高 4.708e-03 1.224e-03 3.846 0.000120 *** #&gt; 収入 -9.227e-04 2.345e-03 -0.394 0.693925 #&gt; 職種B 9.713e-01 1.445e+00 0.672 0.501434 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 102.791 on 99 degrees of freedom #&gt; Residual deviance: 46.971 on 96 degrees of freedom #&gt; AIC: 54.971 #&gt; #&gt; Number of Fisher Scoring iterations: 8 plot(ローン残高, predict(res_glm, type = &quot;response&quot;)) plot(収入, predict(res_glm, type = &quot;response&quot;)) # plot(職種, predict(res_glm, type = &#39;response&#39;)) プロビット回帰 # プロビット回帰 res_glm_p &lt;- glm(デフォルト ~ ローン残高 + 収入 + 職種, family = binomial(link = &quot;probit&quot;)) summary(res_glm_p) #&gt; #&gt; Call: #&gt; glm(formula = デフォルト ~ ローン残高 + 収入 + 職種, #&gt; family = binomial(link = &quot;probit&quot;)) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.10245 -0.22078 -0.02316 -0.00001 2.80240 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -7.0233825 1.6903093 -4.155 3.25e-05 *** #&gt; ローン残高 0.0024920 0.0005893 4.229 2.35e-05 *** #&gt; 収入 -0.0006478 0.0013082 -0.495 0.620 #&gt; 職種B 0.6549554 0.8035424 0.815 0.415 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 102.79 on 99 degrees of freedom #&gt; Residual deviance: 47.81 on 96 degrees of freedom #&gt; AIC: 55.81 #&gt; #&gt; Number of Fisher Scoring iterations: 8 ## 結果の比較: glm vs lm res_glm_normal &lt;- glm(デフォルト ~ ローン残高 + 収入 + ## 職種, family = gaussian) summary(res_glm_normal) res_lm &lt;- lm(デフォルト ~ ## ローン残高 + 収入 + 職種) anova(res_lm) 予測 # 予測 (新しいデータセットに対して) newdat &lt;- data.frame(ローン残高 = c(100, 500, 1000, 10000), 収入 = 30000, 職種 = &quot;A&quot;) predict(res_glm, newdata = newdat, type = &quot;response&quot;) #&gt; 1 2 3 4 #&gt; 2.220446e-16 2.220446e-16 2.220446e-16 9.976404e-01 係数の信頼区間 # 信頼区間 confint(res_glm) # 95%信頼区間 (デフォルト) #&gt; 2.5 % 97.5 % #&gt; (Intercept) -21.713657994 -7.721969240 #&gt; ローン残高 0.002743950 0.007642011 #&gt; 収入 -0.005498298 0.003877354 #&gt; 職種B -1.917922786 3.862413416 detach() 6.3 疑似R2の計算 # ロジット回帰 res_glm &lt;- glm(デフォルト ~ ローン残高 + 収入 + 職種, family = binomial, data = default) # summary(res_glm) パッケージDescToolsの利用 DescTools::PseudoR2() - usage: PseudoR2(x, which = NULL) - which: 計算したい疑似R2. 選択肢: &quot;McFadden&quot;(デフォルト), &quot;McFaddenAdj&quot;, &quot;CoxSnell&quot;, &quot;Nagelkerke&quot;, &quot;AldrichNelson&quot;, &quot;VeallZimmermann&quot;, &quot;Efron&quot;, &quot;McKelveyZavoina&quot;, &quot;Tjur&quot;, &quot;all&quot;. # 疑似R2の計算 library(DescTools) PseudoR2(res_glm) # McFadden (デフォルト) #&gt; McFadden #&gt; 0.5430468 PseudoR2(res_glm, which = &quot;CoxSnell&quot;) # Cox-Snell #&gt; CoxSnell #&gt; 0.4277647 PseudoR2(res_glm, which = &quot;Nagelkerke&quot;) # Nagelkerke #&gt; Nagelkerke #&gt; 0.6660436 PseudoR2(res_glm, which = &quot;all&quot;) #&gt; McFadden McFaddenAdj CoxSnell Nagelkerke AldrichNelson #&gt; 0.5430468 0.4652192 0.4277647 0.6660436 0.3582359 #&gt; VeallZimmermann Efron McKelveyZavoina Tjur AIC #&gt; 0.7067438 0.5736644 0.8515845 0.5589555 54.9708332 #&gt; BIC logLik logLik0 G2 #&gt; 65.3915139 -23.4854166 -51.3956671 55.8205010 パッケージpsclの利用 pscl::pR2() - 以下を出力: - llh: The log-likelihood from the fitted model - llhNull: The log-likelihood from the intercept-only restricted model - G2: Minus two times the difference in the log-likelihoods - McFadden: McFadden&#39;s pseudo r-squared - r2ML: Maximum likelihood pseudo r-squared - r2CU: Cragg and Uhler&#39;s pseudo r-squared # install.packages(&#39;pscl&#39;) library(pscl) # 疑似R2の計算 pscl::pR2(res_glm) #&gt; fitting null model for pseudo-r2 #&gt; llh llhNull G2 McFadden r2ML r2CU #&gt; -23.4854166 -51.3956671 55.8205010 0.5430468 0.4277647 0.6660436 パッケージperformanceの利用 performance::r2() - モデルに応じて適切な疑似R2を選んで出力: - Logistic models: Tjur&#39;s R2 - General linear models: Nagelkerke&#39;s R2 - Multinomial Logit: McFadden&#39;s R2 - Models with zero-inflation: R2 for zero-inflated models - Mixed models: Nakagawa&#39;s R2 - Bayesian models: R2 bayes # install.packages(&#39;performance&#39;) library(performance) # 疑似R2の計算 performance::r2(res_glm) #&gt; # R2 for Logistic Regression #&gt; Tjur&#39;s R2: 0.559 6.4 関数factor()について Rの初心者向けに関数factor()の使い方に関する簡単な説明を行う. Rでのfactor関数の使い方 関数factor()は, カテゴリーデータ（文字列や整数を値に持つベクトル）を因子型（factor）に変換するために使用される. 1. ベクトルを因子に変換する # サンプルのカテゴリーデータ gender &lt;- c(&quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;male&quot;, &quot;male&quot;) # ベクトルを因子に変換する gender_factor &lt;- factor(gender) # 変換後の因子の表示 print(gender_factor) #&gt; [1] male female female male male #&gt; Levels: female male 2. 水準（カテゴリー）の指定 # 水準（カテゴリー）を指定して因子に変換する gender_factor &lt;- factor(gender, levels = c(&quot;male&quot;, &quot;female&quot;)) # 変換後の因子の表示 print(gender_factor) #&gt; [1] male female female male male #&gt; Levels: male female 3. 因子ラベルの指定 # 因子ラベルを指定しながら因子に変換する gender_factor &lt;- factor(gender, levels = c(&quot;m&quot;, &quot;f&quot;), labels = c(&quot;male&quot;, &quot;female&quot;)) # 変換後の因子の表示 print(gender_factor) #&gt; [1] &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; Levels: male female 4. 因子の利用 Rのデータフレーム (data.frame) は, 行列の形をしているが, 実際は, 長さは等しいものの 異なる”データ型”のベクトルを要素に持つリスト (list) である. 因子に変換されたデータは, データフレーム内でカテゴリー変数として保持することで, 統計解析や可視化などで利用される. # サンプルのカテゴリーデータ gender &lt;- c(&quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;male&quot;, &quot;male&quot;) age &lt;- c(25, 30, 35, 40, 45) # データフレームの作成 df &lt;- data.frame(gender = factor(gender), age = age) df #&gt; gender age #&gt; 1 male 25 #&gt; 2 female 30 #&gt; 3 female 35 #&gt; 4 male 40 #&gt; 5 male 45 # データフレームの要約 summary(df) #&gt; gender age #&gt; female:2 Min. :25 #&gt; male :3 1st Qu.:30 #&gt; Median :35 #&gt; Mean :35 #&gt; 3rd Qu.:40 #&gt; Max. :45 # 箱ひげ図 plot(age ~ gender, data = df) Rにおける標準的な統計解析や可視化を行う場合には, 関数factor()を使うことで, カテゴリー変数はR内部で適正に処理されるため 一変数のままで使用することができる. すなわち, 通常, カテゴリーの水準に対応するダミー変数を作る作業は不要である. 関数lm()やglm()等においては, 文字列を値に持つカテゴリー変数は明示的にfactor()を使って因子型に 変換せずとも, Rは因子型と解釈して関数を実行するが, もしそのカテゴリー変数が整数値 (例, gender &lt;- c(1, 2, 2, 1, 1)) を持つ場合には, factor()を使って 変換しないと, 意図とは異なる結果やエラーを生じることになる # カテゴリーデータが整数値で記録されている場合 gender &lt;- c(1, 2, 2, 1, 1) age &lt;- c(25, 30, 35, 40, 45) # データフレームの作成 df &lt;- data.frame(gender = gender, age = age) df #&gt; gender age #&gt; 1 1 25 #&gt; 2 2 30 #&gt; 3 2 35 #&gt; 4 1 40 #&gt; 5 1 45 # データフレームの要約? summary(df) #&gt; gender age #&gt; Min. :1.0 Min. :25 #&gt; 1st Qu.:1.0 1st Qu.:30 #&gt; Median :1.0 Median :35 #&gt; Mean :1.4 Mean :35 #&gt; 3rd Qu.:2.0 3rd Qu.:40 #&gt; Max. :2.0 Max. :45 # 箱ひげ図? plot(age ~ gender, data = df) # 適切な処理: 関数factor()の使用 df &lt;- data.frame(gender = factor(gender, levels = c(&quot;1&quot;, &quot;2&quot;), labels = c(&quot;male&quot;, &quot;female&quot;)), age = age) df #&gt; gender age #&gt; 1 male 25 #&gt; 2 female 30 #&gt; 3 female 35 #&gt; 4 male 40 #&gt; 5 male 45 # データフレームの要約 summary(df) #&gt; gender age #&gt; male :3 Min. :25 #&gt; female:2 1st Qu.:30 #&gt; Median :35 #&gt; Mean :35 #&gt; 3rd Qu.:40 #&gt; Max. :45 # 箱ひげ図 plot(age ~ gender, data = df) "],["判別分析.html", "7 判別分析 7.1 線形判別 &amp; 2次判別 - パッケージMASSの利用 7.2 kNN法 - パッケージclassの利用 7.3 データセット分割・学習・判別・評価 7.4 他パッケージの利用", " 7 判別分析 7.1 線形判別 &amp; 2次判別 - パッケージMASSの利用 手初めに, データセットを学習データ (訓練データ) とテストデータに分けずに, 標準パッケージMASS内の線形判別, 2次判別を実行する関数 lda, qdaを適用した結果 (内挿予測) を眺めてみる. データセット (1): ビジネススクールの入学許可データ - admission.csv - GPA - GMAT - De (意思決定): &quot;admit&quot;, &quot;border&quot;, &quot;notadmit&quot; - source: https://rstudio-pubs-static.s3.amazonaws.com/35817_2552e05f1d4e4db8ba87b334101a43da.html # url &lt;- &#39;http://www.biz.uiowa.edu/faculty/jledolter/DataMining/admission.csv&#39; # admit &lt;- read.csv(url) admdat &lt;- read.csv(&quot;admission.csv&quot;, skip = 2, stringsAsFactors = T) # 注: stringsAsFactors = F(デフォルト) → エラー head(admdat) # admdat = data.frame(admdat) plot(admdat$GPA, admdat$GMAT, col = admdat$De) #&gt; GPA GMAT De #&gt; 1 2.96 596 admit #&gt; 2 3.14 473 admit #&gt; 3 3.22 482 admit #&gt; 4 3.29 527 admit #&gt; 5 3.69 505 admit #&gt; 6 3.46 693 admit 線形判別 (LDA) 線形判別の実行は, パッケージMASSの関数ldaにより行うことができる. library(MASS) # LDAの実行 adm_lda &lt;- lda(De ~ ., admdat) # adm_lda 線形判別 (lda) や2次判別 (qda) の出力オブジェクト (適合結果) を 関数predictに入力することで, 判別結果 (class), およびその判断に使用した 事後確率 (posterior) が出力される. これら (特に前者) が, 判別分析における主要な出力である. # 判別結果 (予測結果) prd_lda &lt;- predict(adm_lda) head(prd_lda$class) #&gt; [1] admit border border admit admit admit #&gt; Levels: admit border notadmit head(prd_lda$posterior) #&gt; admit border notadmit #&gt; 1 0.6274441 3.677500e-01 4.805886e-03 #&gt; 2 0.1400290 8.578453e-01 2.125682e-03 #&gt; 3 0.4070245 5.925494e-01 4.261541e-04 #&gt; 4 0.9111531 8.883071e-02 1.618774e-05 #&gt; 5 0.9989964 1.003605e-03 6.361722e-10 #&gt; 6 0.9999849 1.506768e-05 6.281005e-11 # → 出力: $class, $posterior, $x 外挿予測 &amp; 精度評価 predict(adm_lda, newdata = data.frame(GPA = 3.21, GMAT = 497)) (tbl_lda &lt;- table(predict(adm_lda)$class, admdat$De)) #&gt; $class #&gt; [1] admit #&gt; Levels: admit border notadmit #&gt; #&gt; $posterior #&gt; admit border notadmit #&gt; 1 0.5180421 0.4816015 0.0003563717 #&gt; #&gt; $x #&gt; LD1 LD2 #&gt; 1 1.252409 0.318194 #&gt; #&gt; #&gt; admit border notadmit #&gt; admit 28 1 0 #&gt; border 3 24 2 #&gt; notadmit 0 1 26 2次判別 (QDA) 線形判別の実行は, パッケージMASSの関数qdaにより行うことができる. # QDAの実行 adm_qda &lt;- qda(De ~ ., admdat) # adm_qda # 判別結果 (予測結果) prd_qda &lt;- predict(adm_qda) head(prd_qda$class) #&gt; [1] admit border admit admit admit admit #&gt; Levels: admit border notadmit head(prd_qda$posterior) #&gt; admit border notadmit #&gt; 1 0.9827310 5.740445e-03 1.152851e-02 #&gt; 2 0.3098756 6.880045e-01 2.119862e-03 #&gt; 3 0.8168026 1.826903e-01 5.070901e-04 #&gt; 4 0.9995494 4.307666e-04 1.985923e-05 #&gt; 5 1.0000000 2.322074e-09 4.254151e-10 #&gt; 6 1.0000000 3.974095e-20 1.887372e-10 # → 出力: $class, $posterior\\t 外挿予測 &amp; 精度評価 predict(adm_qda, newdata = data.frame(GPA = 3.21, GMAT = 497)) (tbl_qda &lt;- table(predict(adm_qda)$class, admdat$De)) #&gt; $class #&gt; [1] admit #&gt; Levels: admit border notadmit #&gt; #&gt; $posterior #&gt; admit border notadmit #&gt; 1 0.9226763 0.0768693 0.0004544468 #&gt; #&gt; #&gt; admit border notadmit #&gt; admit 30 1 0 #&gt; border 1 25 1 #&gt; notadmit 0 0 27 なお, 学習済モデルのパラメータ推定値は ldaやqdaの出力オブジェクトに格納され, これらから 線形判別関数や2次判別関数や, 判別境界の形状を知ることができる (ここでは省略). 7.2 kNN法 - パッケージclassの利用 kNN法を実行する関数knnはパッケージclassに含まれている. 入学許可データに対して, 上と同様, 判別 (内挿予測) を試みる. # kNN法 library(class) pred_knn &lt;- knn(admdat[, 1:2], admdat[, 1:2], cl = admdat[, 3], k = 5) (tbl_knn &lt;- table(pred_knn, admdat[, 3])) # confusionMatrix(knn_tab, mode = &#39;prec_recall&#39;) #&gt; #&gt; pred_knn admit border notadmit #&gt; admit 24 1 2 #&gt; border 4 17 7 #&gt; notadmit 3 8 19 7.3 データセット分割・学習・判別・評価 実際の判別分析においては, まず, データセットを学習データ (訓練データ) とテストデータに分け, 前者でモデルを推定 (学習) し, 後者でパフォーマンスを評価する必要がある. データセット2: 個人ローン・デフォルト・データ (仮想) - default1.csv, 7500件 - ローン残高 (万円) - 収入 (万円) - 職種 (A/B) -デフォルト (1/0) dat &lt;- read.csv(&quot;default1.csv&quot;) nsize &lt;- nrow(dat) データセットをランダムに, 2/3を学習用, 残りをテスト用に分割する. # 学習用データ, テスト用データに分割 id_train &lt;- sample(1:nsize, round(nsize/3 * 2)) dat_train &lt;- dat[id_train, ] dat_test &lt;- dat[-id_train, ] データセットの分割は通常ランダムに行うが, 学習済モデルの判別パフォーマンスはデータセットの分割の仕方に依存する. そこで, そのような分割への依存性を軽減し, モデルによる外挿予測の精度評価の信頼性を高めるために交互検証 (クロス・バリデーション) と呼ばれる方法を行うことが多い (ここでは省略する). 線形判別 # 線形判別 library(MASS) dat_lda &lt;- lda(デフォルト ~ ローン残高 + 収入, data = dat_train) # dat_lda dat_lda$scaling\\t\\t\\t\\t# 線形判別関数の係数 # hist(predict(dat_lda)$x)\\t\\t# 判別得点 dat_lda$means\\t\\t\\t\\t\\t# グループ平均 # dat_lda$prior\\t\\t\\t\\t\\t# 事前確率 学習済モデルのパフォーマンス評価は, テストデータを使って判別させ (外挿予測), 正解と比較することにより行う. # 予測 (テストデータに対する判別), パフォーマンス評価 lda_tab &lt;- table(predict(dat_lda, dat_test)$class, dat_test[, &quot;デフォルト&quot;]) # library(caret) confusionMatrix(lda_tab, mode = &#39;prec_recall&#39;) kNN法 # kNN法 library(class) pred_knn &lt;- knn(dat_train[, c(&quot;ローン残高&quot;, &quot;収入&quot;)], dat_test[, c(&quot;ローン残高&quot;, &quot;収入&quot;)], cl = dat_train[, &quot;デフォルト&quot;], k = 3) (tbl_knn &lt;- table(pred_knn, dat_test[, &quot;デフォルト&quot;])) # confusionMatrix(knn_tab, mode = &#39;prec_recall&#39;) #&gt; #&gt; pred_knn 0 1 #&gt; 0 2392 80 #&gt; 1 14 14 7.4 他パッケージの利用 パッケージklaR 関数partimat()は, 判別分析やそれ以外の多様なクラス分類手法をサポートしている. - 引数methodで指定: “lda”, “qda”, “rpart”, “naiveBayes”, “rda”, “sknn”, “svmlight” # &#39;klaR&#39;パッケージの利用 install.packages(&#39;klaR&#39;) library(klaR) partimat(De ~ ., data = admdat, method = &quot;lda&quot;) # 線形判別 partimat(De ~ ., data = admdat, method = &quot;qda&quot;) # 2次判別 partimat(De ~ ., data = admdat, method = &quot;sknn&quot;) # Simple kNN法 (デフォルトk = 3) partimat(De ~ ., data = admdat, method = &quot;naiveBayes&quot;) # ナイーブベイズ法 変数間のスケールを統一 (標準化) した後にkNN法を再実行してみる. adm2 &lt;- data.frame(GPA = scale(admdat$GPA), GMAT = scale(admdat$GMAT), De = admdat$De) partimat(De ~ ., data = adm2, method = &quot;sknn&quot;) # Simple kNN法 (デフォルトk = 3), 変数標準化後 パッケージklaR内には, ナイーブベイズ法を実行する関数NaiveBayes()も用意されている. # ナイーブベイズ法 adm_NB &lt;- NaiveBayes(De ~ ., data = admdat) # usekernel = F(デフォルト) → 正規分布 # predict(adm_NB) # → 出力: $class, $posterior (tbl_nb &lt;- table(predict(adm_NB)$class, admdat$De)) #&gt; #&gt; admit border notadmit #&gt; admit 28 1 0 #&gt; border 3 23 2 #&gt; notadmit 0 2 26 "],["決定木分析.html", "8 決定木分析 8.1 導入: 回帰木 vs 線形回帰 8.2 回帰木 8.3 分類木", " 8 決定木分析 8.1 導入: 回帰木 vs 線形回帰 データセット1: 自動車の制動距離 - cars - speed: 自動車の速度 - dist: ブレーキを踏んだ後で自動車が止まるまでの距離 - Rの標準データセット # cars head(cars) #&gt; speed dist #&gt; 1 4 2 #&gt; 2 4 10 #&gt; 3 7 4 #&gt; 4 7 22 #&gt; 5 8 16 #&gt; 6 9 10 tail(cars) #&gt; speed dist #&gt; 45 23 54 #&gt; 46 24 70 #&gt; 47 24 92 #&gt; 48 24 93 #&gt; 49 24 120 #&gt; 50 25 85 # cars[&#39;speed&#39;] plot(cars) cor(cars[&quot;speed&quot;], cars[&quot;dist&quot;]) #&gt; dist #&gt; speed 0.8068949 単回帰分析 # 単回帰分析 cars_lm &lt;- lm(dist ~ speed, data = cars) summary(cars_lm) #&gt; #&gt; Call: #&gt; lm(formula = dist ~ speed, data = cars) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -29.069 -9.525 -2.272 9.215 43.201 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -17.5791 6.7584 -2.601 0.0123 * #&gt; speed 3.9324 0.4155 9.464 1.49e-12 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 15.38 on 48 degrees of freedom #&gt; Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 #&gt; F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12 # 回帰係数の取り出し cars_lm$coef #&gt; (Intercept) speed #&gt; -17.579095 3.932409 coefficients(cars_lm) #&gt; (Intercept) speed #&gt; -17.579095 3.932409 # 回帰直線の図示 plot(cars) abline(cars_lm) # モデル診断（残差プロット等) plot(cars_lm)\\t\\t 4枚表示: 残差vs Y適合値, 残差 # vs Q - Qプロット, 残差平方根 vs Y適合値, 残差 vs 影響力(てこ値とCook距離) 予測 # 学習データに対する適合値 (内挿予測) cars_pred &lt;- predict(cars_lm) # 残差 cars_resd &lt;- residuals(cars_lm) # 予測値 vs 残差 plot(cars_pred, cars_resd) abline(h = 0, lty = 2) data.frame(cars, cars_pred, cars_resd) #&gt; speed dist cars_pred cars_resd #&gt; 1 4 2 -1.849460 3.849460 #&gt; 2 4 10 -1.849460 11.849460 #&gt; 3 7 4 9.947766 -5.947766 #&gt; 4 7 22 9.947766 12.052234 #&gt; 5 8 16 13.880175 2.119825 #&gt; 6 9 10 17.812584 -7.812584 #&gt; 7 10 18 21.744993 -3.744993 #&gt; 8 10 26 21.744993 4.255007 #&gt; 9 10 34 21.744993 12.255007 #&gt; 10 11 17 25.677401 -8.677401 #&gt; 11 11 28 25.677401 2.322599 #&gt; 12 12 14 29.609810 -15.609810 #&gt; 13 12 20 29.609810 -9.609810 #&gt; 14 12 24 29.609810 -5.609810 #&gt; 15 12 28 29.609810 -1.609810 #&gt; 16 13 26 33.542219 -7.542219 #&gt; 17 13 34 33.542219 0.457781 #&gt; 18 13 34 33.542219 0.457781 #&gt; 19 13 46 33.542219 12.457781 #&gt; 20 14 26 37.474628 -11.474628 #&gt; 21 14 36 37.474628 -1.474628 #&gt; 22 14 60 37.474628 22.525372 #&gt; 23 14 80 37.474628 42.525372 #&gt; 24 15 20 41.407036 -21.407036 #&gt; 25 15 26 41.407036 -15.407036 #&gt; 26 15 54 41.407036 12.592964 #&gt; 27 16 32 45.339445 -13.339445 #&gt; 28 16 40 45.339445 -5.339445 #&gt; 29 17 32 49.271854 -17.271854 #&gt; 30 17 40 49.271854 -9.271854 #&gt; 31 17 50 49.271854 0.728146 #&gt; 32 18 42 53.204263 -11.204263 #&gt; 33 18 56 53.204263 2.795737 #&gt; 34 18 76 53.204263 22.795737 #&gt; 35 18 84 53.204263 30.795737 #&gt; 36 19 36 57.136672 -21.136672 #&gt; 37 19 46 57.136672 -11.136672 #&gt; 38 19 68 57.136672 10.863328 #&gt; 39 20 32 61.069080 -29.069080 #&gt; 40 20 48 61.069080 -13.069080 #&gt; 41 20 52 61.069080 -9.069080 #&gt; 42 20 56 61.069080 -5.069080 #&gt; 43 20 64 61.069080 2.930920 #&gt; 44 22 66 68.933898 -2.933898 #&gt; 45 23 54 72.866307 -18.866307 #&gt; 46 24 70 76.798715 -6.798715 #&gt; 47 24 92 76.798715 15.201285 #&gt; 48 24 93 76.798715 16.201285 #&gt; 49 24 120 76.798715 43.201285 #&gt; 50 25 85 80.731124 4.268876 # テストデータ(未学習データ)に対する予測 (外挿予測) testdat &lt;- data.frame(speed = c(5, 6, 21)) head(predict(cars_lm, newdata = testdat)) #&gt; 1 2 3 #&gt; 2.082949 6.015358 65.001489 8.1.1 基本操作: 回帰木 回帰木, 分類木ともに, パッケージrpartの関数rpart()を適用する. library(rpart) cars_rp &lt;- rpart(dist ~ speed, data = cars) summary(cars_rp) # ==&gt; 葉3枚 #&gt; Call: #&gt; rpart(formula = dist ~ speed, data = cars) #&gt; n= 50 #&gt; #&gt; CP nsplit rel error xerror xstd #&gt; 1 0.4676398 0 1.0000000 1.0156762 0.2160453 #&gt; 2 0.1104944 1 0.5323602 0.6292953 0.1398402 #&gt; 3 0.0100000 2 0.4218658 0.4974150 0.1239476 #&gt; #&gt; Variable importance #&gt; speed #&gt; 100 #&gt; #&gt; Node number 1: 50 observations, complexity param=0.4676398 #&gt; mean=42.98, MSE=650.7796 #&gt; left son=2 (31 obs) right son=3 (19 obs) #&gt; Primary splits: #&gt; speed &lt; 17.5 to the left, improve=0.4676398, (0 missing) #&gt; #&gt; Node number 2: 31 observations, complexity param=0.1104944 #&gt; mean=29.32258, MSE=267.9605 #&gt; left son=4 (15 obs) right son=5 (16 obs) #&gt; Primary splits: #&gt; speed &lt; 12.5 to the left, improve=0.4328244, (0 missing) #&gt; #&gt; Node number 3: 19 observations #&gt; mean=65.26316, MSE=474.5097 #&gt; #&gt; Node number 4: 15 observations #&gt; mean=18.2, MSE=78.42667 #&gt; #&gt; Node number 5: 16 observations #&gt; mean=39.75, MSE=220.9375 plot(cars_rp, uniform = T, margin = 0.05) text(cars_rp, all = T, use.n = T) 予測 # 学習用データに対する適合値 (内挿予測) cars_rp_pred &lt;- predict(cars_rp) cars_rp_fitted &lt;- data.frame(cars$speed, cars_rp_pred) plot(cars$speed, cars$dist) lines(cars_rp_fitted, type = &quot;s&quot;) # テストデータに対する予測値 (外挿予測) predict(cars_rp, newdata = testdat) #&gt; 1 2 3 #&gt; 18.20000 18.20000 65.26316 # 観測値, 線形回帰の適合値, 回帰木の適合値 data.frame(cars, cars_pred, cars_rp_fitted) #&gt; speed dist cars_pred cars.speed cars_rp_pred #&gt; 1 4 2 -1.849460 4 18.20000 #&gt; 2 4 10 -1.849460 4 18.20000 #&gt; 3 7 4 9.947766 7 18.20000 #&gt; 4 7 22 9.947766 7 18.20000 #&gt; 5 8 16 13.880175 8 18.20000 #&gt; 6 9 10 17.812584 9 18.20000 #&gt; 7 10 18 21.744993 10 18.20000 #&gt; 8 10 26 21.744993 10 18.20000 #&gt; 9 10 34 21.744993 10 18.20000 #&gt; 10 11 17 25.677401 11 18.20000 #&gt; 11 11 28 25.677401 11 18.20000 #&gt; 12 12 14 29.609810 12 18.20000 #&gt; 13 12 20 29.609810 12 18.20000 #&gt; 14 12 24 29.609810 12 18.20000 #&gt; 15 12 28 29.609810 12 18.20000 #&gt; 16 13 26 33.542219 13 39.75000 #&gt; 17 13 34 33.542219 13 39.75000 #&gt; 18 13 34 33.542219 13 39.75000 #&gt; 19 13 46 33.542219 13 39.75000 #&gt; 20 14 26 37.474628 14 39.75000 #&gt; 21 14 36 37.474628 14 39.75000 #&gt; 22 14 60 37.474628 14 39.75000 #&gt; 23 14 80 37.474628 14 39.75000 #&gt; 24 15 20 41.407036 15 39.75000 #&gt; 25 15 26 41.407036 15 39.75000 #&gt; 26 15 54 41.407036 15 39.75000 #&gt; 27 16 32 45.339445 16 39.75000 #&gt; 28 16 40 45.339445 16 39.75000 #&gt; 29 17 32 49.271854 17 39.75000 #&gt; 30 17 40 49.271854 17 39.75000 #&gt; 31 17 50 49.271854 17 39.75000 #&gt; 32 18 42 53.204263 18 65.26316 #&gt; 33 18 56 53.204263 18 65.26316 #&gt; 34 18 76 53.204263 18 65.26316 #&gt; 35 18 84 53.204263 18 65.26316 #&gt; 36 19 36 57.136672 19 65.26316 #&gt; 37 19 46 57.136672 19 65.26316 #&gt; 38 19 68 57.136672 19 65.26316 #&gt; 39 20 32 61.069080 20 65.26316 #&gt; 40 20 48 61.069080 20 65.26316 #&gt; 41 20 52 61.069080 20 65.26316 #&gt; 42 20 56 61.069080 20 65.26316 #&gt; 43 20 64 61.069080 20 65.26316 #&gt; 44 22 66 68.933898 22 65.26316 #&gt; 45 23 54 72.866307 23 65.26316 #&gt; 46 24 70 76.798715 24 65.26316 #&gt; 47 24 92 76.798715 24 65.26316 #&gt; 48 24 93 76.798715 24 65.26316 #&gt; 49 24 120 76.798715 24 65.26316 #&gt; 50 25 85 80.731124 25 65.26316 8.2 回帰木 データセット2: ワイン品質データ - winequality-white.csv - fixed acidity: 酢酸濃度 - volitle acidity: 揮発酸濃度 - citric acidity: クエン酸濃度 - chlorides: 塩化物 - sulfur dioxide: 二酸化硫黄 - sulphate: 硫酸塩 - fixed acidity: 酒石酸含有量（g/dm3) - volatile acidity: 酢酸含有量（g/dm3) - citric acid: クエン酸含有量（g/dm3) - residual sugar: 残留糖分含有量（g/dm3） - chlorides: 塩化ナトリウム含有量（g/dm3) - free sulfur dioxide: 遊離亜硫酸含有量（mg/dm3） - total sulfur dioxide: 総亜硫酸含有量（mg/dm3） - density: 密度（g/dm3) - pH: pH - sulphates: 硫酸カリウム含有量（g/dm3） - alcohol: アルコール度数（% vol.） - quality: ワインの品質 (0 (very bad) -- 10 (excellent)) 全データセットを学習用とテスト用にランダムに分割. wine &lt;- read.csv(&quot;winequality-white.csv&quot;, sep = &quot;;&quot;, skip = 1, header = T) set.seed(100) smpl_idx &lt;- sample(1:nrow(wine), 3000) # 元データ(行番号)から3000件を非復元抽出 wine_train &lt;- wine[smpl_idx, ] # 学習用データセット wine_test &lt;- wine[-smpl_idx, ] # テスト用データセット str(wine) #&gt; &#39;data.frame&#39;: 4898 obs. of 12 variables: #&gt; $ fixed.acidity : num 7 6.3 8.1 7.2 7.2 8.1 6.2 7 6.3 8.1 ... #&gt; $ volatile.acidity : num 0.27 0.3 0.28 0.23 0.23 0.28 0.32 0.27 0.3 0.22 ... #&gt; $ citric.acid : num 0.36 0.34 0.4 0.32 0.32 0.4 0.16 0.36 0.34 0.43 ... #&gt; $ residual.sugar : num 20.7 1.6 6.9 8.5 8.5 6.9 7 20.7 1.6 1.5 ... #&gt; $ chlorides : num 0.045 0.049 0.05 0.058 0.058 0.05 0.045 0.045 0.049 0.044 ... #&gt; $ free.sulfur.dioxide : num 45 14 30 47 47 30 30 45 14 28 ... #&gt; $ total.sulfur.dioxide: num 170 132 97 186 186 97 136 170 132 129 ... #&gt; $ density : num 1.001 0.994 0.995 0.996 0.996 ... #&gt; $ pH : num 3 3.3 3.26 3.19 3.19 3.26 3.18 3 3.3 3.22 ... #&gt; $ sulphates : num 0.45 0.49 0.44 0.4 0.4 0.44 0.47 0.45 0.49 0.45 ... #&gt; $ alcohol : num 8.8 9.5 10.1 9.9 9.9 10.1 9.6 8.8 9.5 11 ... #&gt; $ quality : int 6 6 6 6 6 6 6 6 6 6 ... hist(wine$quality) library(rpart) wine_rp &lt;- rpart(quality ~ ., data = wine_train) # quality以外の変数を説明変数に使用 wine_rp #&gt; n= 3000 #&gt; #&gt; node), split, n, deviance, yval #&gt; * denotes terminal node #&gt; #&gt; 1) root 3000 2320.0400 5.880333 #&gt; 2) alcohol&lt; 10.85 1883 1106.7690 5.612852 #&gt; 4) volatile.acidity&gt;=0.2525 985 464.4934 5.358376 * #&gt; 5) volatile.acidity&lt; 0.2525 898 508.5223 5.891982 #&gt; 10) volatile.acidity&gt;=0.2075 437 217.9405 5.720824 * #&gt; 11) volatile.acidity&lt; 0.2075 461 265.6443 6.054230 #&gt; 22) residual.sugar&lt; 12.575 369 183.0244 5.926829 * #&gt; 23) residual.sugar&gt;=12.575 92 52.6087 6.565217 * #&gt; 3) alcohol&gt;=10.85 1117 851.4396 6.331244 #&gt; 6) free.sulfur.dioxide&lt; 11.5 66 67.5303 5.378788 * #&gt; 7) free.sulfur.dioxide&gt;=11.5 1051 720.2759 6.391056 #&gt; 14) alcohol&lt; 12.45 795 512.6717 6.275472 * #&gt; 15) alcohol&gt;=12.45 256 164.0000 6.750000 * # 可視化 library(rpart.plot) rpart.plot(wine_rp, digit = 3) rpart.plot(wine_rp, digit = 4, fallen.leaves = T, type = 3, extra = 101) # 学習データによる予測の精度 wine_rp_train &lt;- predict(wine_rp) cor(wine_rp_train, wine_train$quality) #&gt; [1] 0.5324631 printcp(wine_rp) #&gt; #&gt; Regression tree: #&gt; rpart(formula = quality ~ ., data = wine_train) #&gt; #&gt; Variables actually used in tree construction: #&gt; [1] alcohol free.sulfur.dioxide residual.sugar #&gt; [4] volatile.acidity #&gt; #&gt; Root node error: 2320/3000 = 0.77335 #&gt; #&gt; n= 3000 #&gt; #&gt; CP nsplit rel error xerror xstd #&gt; 1 0.155959 0 1.00000 1.00109 0.027711 #&gt; 2 0.057651 1 0.84404 0.84827 0.026277 #&gt; 3 0.027428 2 0.78639 0.80140 0.025794 #&gt; 4 0.018795 3 0.75896 0.77812 0.024591 #&gt; 5 0.011842 4 0.74017 0.75908 0.024127 #&gt; 6 0.010000 6 0.71648 0.74854 0.023784 plotcp(wine_rp) # 手動による剪定例 (complex parameter(cp)の大きさに基づいて) wine_rp1 &lt;- prune(wine_rp, cp = 0.03) plot(wine_rp1, uniform = T, margin = 0.05) text(wine_rp1, all = T, use.n = T) 8.3 分類木 データセット3: 自動車評価データ - car.data.txt - Class (評価, 4水準), unacc, acc, good, vgood - buying (価格帯, 4): vhigh, high, med, low. - maint (維持費, 4): vhigh, high, med, low. - doors (ドア数, 4): 2, 3, 4, 5more. - persons (乗車人数, 3): 2, 4, more. - lug_boot (収納性, 3): small, med, big. - safety (安全性, 3): low, med, high. - source: https://archive.ics.uci.edu/ml/datasets/Car+Evaluation - 注) factorの並び (Levels) はデフォルトではアルファベット順 library(rpart) cardata0 &lt;- read.csv(&quot;car.data.txt&quot;, skip = 6) attach(cardata0) # factorの並び(Levels)はデフォルトではアルファベット順 → # 好ましくない～好ましいの順に, 並べ替え 評価 &lt;- factor(Class, levels = c(&quot;unacc&quot;, &quot;acc&quot;, &quot;good&quot;, &quot;vgood&quot;)) 価格帯 &lt;- factor(buying, levels = c(&quot;vhigh&quot;, &quot;high&quot;, &quot;med&quot;, &quot;low&quot;)) 維持費 &lt;- factor(maint, levels = c(&quot;vhigh&quot;, &quot;high&quot;, &quot;med&quot;, &quot;low&quot;)) ドア数 &lt;- factor(doors, levels = c(&quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5more&quot;)) 乗車人数 &lt;- factor(persons, levels = c(&quot;2&quot;, &quot;4&quot;, &quot;more&quot;)) 収納性 &lt;- factor(lug_boot, levels = c(&quot;small&quot;, &quot;med&quot;, &quot;big&quot;)) 安全性 &lt;- factor(safety, levels = c(&quot;low&quot;, &quot;med&quot;, &quot;high&quot;)) # cardata1 &lt;- data.frame(評価, 価格帯, 維持費, ドア数, 乗車人数, 収納性, 安全性) head(cardata1) #&gt; 評価 価格帯 維持費 ドア数 乗車人数 収納性 安全性 #&gt; 1 unacc vhigh vhigh 2 2 small low #&gt; 2 unacc vhigh vhigh 2 2 small med #&gt; 3 unacc vhigh vhigh 2 2 small high #&gt; 4 unacc vhigh vhigh 2 2 med low #&gt; 5 unacc vhigh vhigh 2 2 med med #&gt; 6 unacc vhigh vhigh 2 2 med high # 元データを, 学習用データとテスト用データに分割 # 学習用データでモデルに学習させる set.seed(100) smpl_idx &lt;- sample(1:nrow(cardata1), 1500) # 元データ(行番号)から1500件を非復元抽出 car_train &lt;- cardata1[smpl_idx, ] # 学習用データセット, 1500件 car_test &lt;- cardata1[-smpl_idx, ] # テスト用データセット, 残り 学習用データセットで訓練(学習). cardata &lt;- car_train table(cardata[, c(&quot;評価&quot;, &quot;価格帯&quot;)]) #&gt; 価格帯 #&gt; 評価 vhigh high med low #&gt; unacc 307 281 239 221 #&gt; acc 65 92 94 79 #&gt; good 0 0 20 40 #&gt; vgood 0 0 24 38 table(cardata[, c(&quot;評価&quot;, &quot;維持費&quot;)]) #&gt; 維持費 #&gt; 評価 vhigh high med low #&gt; unacc 310 270 235 233 #&gt; acc 60 89 102 79 #&gt; good 0 0 22 38 #&gt; vgood 0 12 25 25 table(cardata[, c(&quot;評価&quot;, &quot;ドア数&quot;)]) #&gt; ドア数 #&gt; 評価 2 3 4 5more #&gt; unacc 282 263 255 248 #&gt; acc 74 82 86 88 #&gt; good 14 15 16 15 #&gt; vgood 10 13 19 20 table(cardata[, c(&quot;評価&quot;, &quot;乗車人数&quot;)]) #&gt; 乗車人数 #&gt; 評価 2 4 more #&gt; unacc 494 283 271 #&gt; acc 0 169 161 #&gt; good 0 31 29 #&gt; vgood 0 29 33 table(cardata[, c(&quot;評価&quot;, &quot;収納性&quot;)]) # balanced #&gt; 収納性 #&gt; 評価 small med big #&gt; unacc 393 341 314 #&gt; acc 94 112 124 #&gt; good 18 24 18 #&gt; vgood 0 24 38 res_rp &lt;- rpart(評価 ~ ., data = cardata) summary(res_rp) #&gt; Call: #&gt; rpart(formula = 評価 ~ ., data = cardata) #&gt; n= 1500 #&gt; #&gt; CP nsplit rel error xerror xstd #&gt; 1 0.12389381 0 1.0000000 1.0000000 0.03931568 #&gt; 2 0.11283186 2 0.7522124 0.9181416 0.03833142 #&gt; 3 0.04535398 4 0.5265487 0.5265487 0.03130647 #&gt; 4 0.03982301 6 0.4358407 0.4889381 0.03037018 #&gt; 5 0.03097345 7 0.3960177 0.4137168 0.02830539 #&gt; 6 0.02876106 9 0.3340708 0.3805310 0.02730104 #&gt; 7 0.02101770 10 0.3053097 0.3429204 0.02608212 #&gt; 8 0.01880531 12 0.2632743 0.3163717 0.02516366 #&gt; 9 0.01327434 14 0.2256637 0.2389381 0.02214866 #&gt; 10 0.01000000 16 0.1991150 0.2212389 0.02137371 #&gt; #&gt; Variable importance #&gt; 安全性 乗車人数 維持費 収納性 価格帯 ドア数 #&gt; 30 28 20 10 10 1 #&gt; #&gt; Node number 1: 1500 observations, complexity param=0.1238938 #&gt; predicted class=unacc expected loss=0.3013333 P(node) =1 #&gt; class counts: 1048 330 60 62 #&gt; probabilities: 0.699 0.220 0.040 0.041 #&gt; left son=2 (508 obs) right son=3 (992 obs) #&gt; Primary splits: #&gt; 安全性 splits as LRR, improve=109.468500, (0 missing) #&gt; 乗車人数 splits as LRR, improve=104.970300, (0 missing) #&gt; 価格帯 splits as LLRR, improve= 17.172040, (0 missing) #&gt; 維持費 splits as LLRR, improve= 14.140400, (0 missing) #&gt; 収納性 splits as LRR, improve= 7.005849, (0 missing) #&gt; #&gt; Node number 2: 508 observations #&gt; predicted class=unacc expected loss=0 P(node) =0.3386667 #&gt; class counts: 508 0 0 0 #&gt; probabilities: 1.000 0.000 0.000 0.000 #&gt; #&gt; Node number 3: 992 observations, complexity param=0.1238938 #&gt; predicted class=unacc expected loss=0.4556452 P(node) =0.6613333 #&gt; class counts: 540 330 60 62 #&gt; probabilities: 0.544 0.333 0.060 0.062 #&gt; left son=6 (322 obs) right son=7 (670 obs) #&gt; Primary splits: #&gt; 乗車人数 splits as LRR, improve=155.34520, (0 missing) #&gt; 価格帯 splits as LLRR, improve= 26.05666, (0 missing) #&gt; 維持費 splits as LLRR, improve= 19.14227, (0 missing) #&gt; 安全性 splits as -LR, improve= 11.33566, (0 missing) #&gt; 収納性 splits as LRR, improve= 11.03210, (0 missing) #&gt; #&gt; Node number 6: 322 observations #&gt; predicted class=unacc expected loss=0 P(node) =0.2146667 #&gt; class counts: 322 0 0 0 #&gt; probabilities: 1.000 0.000 0.000 0.000 #&gt; #&gt; Node number 7: 670 observations, complexity param=0.1128319 #&gt; predicted class=acc expected loss=0.5074627 P(node) =0.4466667 #&gt; class counts: 218 330 60 62 #&gt; probabilities: 0.325 0.493 0.090 0.093 #&gt; left son=14 (333 obs) right son=15 (337 obs) #&gt; Primary splits: #&gt; 価格帯 splits as LLRR, improve=38.596470, (0 missing) #&gt; 維持費 splits as LLRR, improve=29.807250, (0 missing) #&gt; 収納性 splits as LRR, improve=17.341050, (0 missing) #&gt; 安全性 splits as -LR, improve=15.372940, (0 missing) #&gt; ドア数 splits as LRRR, improve= 4.288712, (0 missing) #&gt; Surrogate splits: #&gt; ドア数 splits as RLRR, agree=0.516, adj=0.027, (0 split) #&gt; 安全性 splits as -LR, agree=0.510, adj=0.015, (0 split) #&gt; 乗車人数 splits as -LR, agree=0.509, adj=0.012, (0 split) #&gt; 維持費 splits as RRRL, agree=0.507, adj=0.009, (0 split) #&gt; 収納性 splits as LRR, agree=0.504, adj=0.003, (0 split) #&gt; #&gt; Node number 14: 333 observations, complexity param=0.1128319 #&gt; predicted class=unacc expected loss=0.4714715 P(node) =0.222 #&gt; class counts: 176 157 0 0 #&gt; probabilities: 0.529 0.471 0.000 0.000 #&gt; left son=28 (162 obs) right son=29 (171 obs) #&gt; Primary splits: #&gt; 維持費 splits as LLRR, improve=51.712340, (0 missing) #&gt; 収納性 splits as LRR, improve=14.322380, (0 missing) #&gt; 安全性 splits as -LR, improve=12.358880, (0 missing) #&gt; 価格帯 splits as LR--, improve= 5.866990, (0 missing) #&gt; ドア数 splits as LRRR, improve= 2.000571, (0 missing) #&gt; Surrogate splits: #&gt; ドア数 splits as RRLR, agree=0.520, adj=0.012, (0 split) #&gt; 収納性 splits as LRR, agree=0.517, adj=0.006, (0 split) #&gt; #&gt; Node number 15: 337 observations, complexity param=0.04535398 #&gt; predicted class=acc expected loss=0.4866469 P(node) =0.2246667 #&gt; class counts: 42 173 60 62 #&gt; probabilities: 0.125 0.513 0.178 0.184 #&gt; left son=30 (166 obs) right son=31 (171 obs) #&gt; Primary splits: #&gt; 維持費 splits as LLRR, improve=30.432380, (0 missing) #&gt; 安全性 splits as -LR, improve=15.159790, (0 missing) #&gt; 収納性 splits as LRR, improve=10.848300, (0 missing) #&gt; 価格帯 splits as --LR, improve= 2.920132, (0 missing) #&gt; ドア数 splits as LRRR, improve= 2.729825, (0 missing) #&gt; Surrogate splits: #&gt; ドア数 splits as RRRL, agree=0.513, adj=0.012, (0 split) #&gt; 収納性 splits as LRR, agree=0.513, adj=0.012, (0 split) #&gt; 安全性 splits as -LR, agree=0.513, adj=0.012, (0 split) #&gt; #&gt; Node number 28: 162 observations, complexity param=0.0210177 #&gt; predicted class=unacc expected loss=0.1851852 P(node) =0.108 #&gt; class counts: 132 30 0 0 #&gt; probabilities: 0.815 0.185 0.000 0.000 #&gt; left son=56 (82 obs) right son=57 (80 obs) #&gt; Primary splits: #&gt; 価格帯 splits as LR--, improve=11.3888900, (0 missing) #&gt; 維持費 splits as LR--, improve=10.8401100, (0 missing) #&gt; 収納性 splits as LLR, improve= 1.9067460, (0 missing) #&gt; 安全性 splits as -LR, improve= 0.9437920, (0 missing) #&gt; ドア数 splits as LLRR, improve= 0.2358401, (0 missing) #&gt; Surrogate splits: #&gt; 収納性 splits as LLR, agree=0.531, adj=0.050, (0 split) #&gt; ドア数 splits as LRLR, agree=0.519, adj=0.025, (0 split) #&gt; #&gt; Node number 29: 171 observations, complexity param=0.03097345 #&gt; predicted class=acc expected loss=0.2573099 P(node) =0.114 #&gt; class counts: 44 127 0 0 #&gt; probabilities: 0.257 0.743 0.000 0.000 #&gt; left son=58 (57 obs) right son=59 (114 obs) #&gt; Primary splits: #&gt; 収納性 splits as LRR, improve=15.81287000, (0 missing) #&gt; 安全性 splits as -LR, improve=14.94250000, (0 missing) #&gt; ドア数 splits as LRRR, improve= 3.05237700, (0 missing) #&gt; 乗車人数 splits as -RL, improve= 0.05959792, (0 missing) #&gt; 価格帯 splits as LR--, improve= 0.05664293, (0 missing) #&gt; #&gt; Node number 30: 166 observations, complexity param=0.01880531 #&gt; predicted class=acc expected loss=0.2831325 P(node) =0.1106667 #&gt; class counts: 35 119 0 12 #&gt; probabilities: 0.211 0.717 0.000 0.072 #&gt; left son=60 (58 obs) right son=61 (108 obs) #&gt; Primary splits: #&gt; 収納性 splits as LRR, improve=7.922795, (0 missing) #&gt; 安全性 splits as -LR, improve=7.674699, (0 missing) #&gt; ドア数 splits as LRRR, improve=2.069439, (0 missing) #&gt; 維持費 splits as LR--, improve=1.602410, (0 missing) #&gt; 価格帯 splits as --LR, improve=1.284737, (0 missing) #&gt; #&gt; Node number 31: 171 observations, complexity param=0.04535398 #&gt; predicted class=good expected loss=0.6491228 P(node) =0.114 #&gt; class counts: 7 54 60 50 #&gt; probabilities: 0.041 0.316 0.351 0.292 #&gt; left son=62 (81 obs) right son=63 (90 obs) #&gt; Primary splits: #&gt; 安全性 splits as -LR, improve=22.499420, (0 missing) #&gt; 収納性 splits as LRR, improve=12.487990, (0 missing) #&gt; 維持費 splits as --LR, improve= 4.316887, (0 missing) #&gt; 価格帯 splits as --LR, improve= 3.767329, (0 missing) #&gt; ドア数 splits as LRRR, improve= 1.797828, (0 missing) #&gt; Surrogate splits: #&gt; ドア数 splits as RRLR, agree=0.538, adj=0.025, (0 split) #&gt; #&gt; Node number 56: 82 observations #&gt; predicted class=unacc expected loss=0 P(node) =0.05466667 #&gt; class counts: 82 0 0 0 #&gt; probabilities: 1.000 0.000 0.000 0.000 #&gt; #&gt; Node number 57: 80 observations, complexity param=0.0210177 #&gt; predicted class=unacc expected loss=0.375 P(node) =0.05333333 #&gt; class counts: 50 30 0 0 #&gt; probabilities: 0.625 0.375 0.000 0.000 #&gt; left son=114 (39 obs) right son=115 (41 obs) #&gt; Primary splits: #&gt; 維持費 splits as LR--, improve=21.40244000, (0 missing) #&gt; 収納性 splits as LLR, improve= 2.65723300, (0 missing) #&gt; 安全性 splits as -LR, improve= 1.91526000, (0 missing) #&gt; ドア数 splits as LLRR, improve= 0.56441530, (0 missing) #&gt; 乗車人数 splits as -LR, improve= 0.05639098, (0 missing) #&gt; Surrogate splits: #&gt; 収納性 splits as RLR, agree=0.562, adj=0.103, (0 split) #&gt; ドア数 splits as RLRR, agree=0.525, adj=0.026, (0 split) #&gt; #&gt; Node number 58: 57 observations, complexity param=0.03097345 #&gt; predicted class=unacc expected loss=0.4385965 P(node) =0.038 #&gt; class counts: 32 25 0 0 #&gt; probabilities: 0.561 0.439 0.000 0.000 #&gt; left son=116 (28 obs) right son=117 (29 obs) #&gt; Primary splits: #&gt; 安全性 splits as -LR, improve=21.17362000, (0 missing) #&gt; ドア数 splits as LRRR, improve= 1.20350900, (0 missing) #&gt; 乗車人数 splits as -RL, improve= 1.03815600, (0 missing) #&gt; 維持費 splits as --RL, improve= 0.09980507, (0 missing) #&gt; 価格帯 splits as LR--, improve= 0.05032432, (0 missing) #&gt; Surrogate splits: #&gt; 価格帯 splits as LR--, agree=0.526, adj=0.036, (0 split) #&gt; 維持費 splits as --RL, agree=0.526, adj=0.036, (0 split) #&gt; ドア数 splits as RRLR, agree=0.526, adj=0.036, (0 split) #&gt; 乗車人数 splits as -RL, agree=0.526, adj=0.036, (0 split) #&gt; #&gt; Node number 59: 114 observations #&gt; predicted class=acc expected loss=0.1052632 P(node) =0.076 #&gt; class counts: 12 102 0 0 #&gt; probabilities: 0.105 0.895 0.000 0.000 #&gt; #&gt; Node number 60: 58 observations, complexity param=0.01880531 #&gt; predicted class=acc expected loss=0.4482759 P(node) =0.03866667 #&gt; class counts: 26 32 0 0 #&gt; probabilities: 0.448 0.552 0.000 0.000 #&gt; left son=120 (29 obs) right son=121 (29 obs) #&gt; Primary splits: #&gt; 安全性 splits as -LR, improve=13.7931000, (0 missing) #&gt; 維持費 splits as LR--, improve= 2.2068970, (0 missing) #&gt; ドア数 splits as LRRR, improve= 1.9956380, (0 missing) #&gt; 価格帯 splits as --LR, improve= 0.8277504, (0 missing) #&gt; 乗車人数 splits as -RL, improve= 0.3325123, (0 missing) #&gt; Surrogate splits: #&gt; 維持費 splits as LR--, agree=0.552, adj=0.103, (0 split) #&gt; ドア数 splits as LLRR, agree=0.534, adj=0.069, (0 split) #&gt; #&gt; Node number 61: 108 observations #&gt; predicted class=acc expected loss=0.1944444 P(node) =0.072 #&gt; class counts: 9 87 0 12 #&gt; probabilities: 0.083 0.806 0.000 0.111 #&gt; #&gt; Node number 62: 81 observations, complexity param=0.02876106 #&gt; predicted class=acc expected loss=0.4444444 P(node) =0.054 #&gt; class counts: 3 45 33 0 #&gt; probabilities: 0.037 0.556 0.407 0.000 #&gt; left son=124 (28 obs) right son=125 (53 obs) #&gt; Primary splits: #&gt; 収納性 splits as LRR, improve=12.1816400, (0 missing) #&gt; 価格帯 splits as --LR, improve= 2.6916900, (0 missing) #&gt; 維持費 splits as --LR, improve= 2.2466560, (0 missing) #&gt; ドア数 splits as LLRR, improve= 1.5350200, (0 missing) #&gt; 乗車人数 splits as -RL, improve= 0.5724932, (0 missing) #&gt; #&gt; Node number 63: 90 observations, complexity param=0.03982301 #&gt; predicted class=vgood expected loss=0.4444444 P(node) =0.06 #&gt; class counts: 4 9 27 50 #&gt; probabilities: 0.044 0.100 0.300 0.556 #&gt; left son=126 (28 obs) right son=127 (62 obs) #&gt; Primary splits: #&gt; 収納性 splits as LRR, improve=18.2472100, (0 missing) #&gt; ドア数 splits as LLRR, improve= 3.1671560, (0 missing) #&gt; 維持費 splits as --LR, improve= 1.8000000, (0 missing) #&gt; 価格帯 splits as --LR, improve= 1.4888890, (0 missing) #&gt; 乗車人数 splits as -RL, improve= 0.6274484, (0 missing) #&gt; #&gt; Node number 114: 39 observations #&gt; predicted class=unacc expected loss=0 P(node) =0.026 #&gt; class counts: 39 0 0 0 #&gt; probabilities: 1.000 0.000 0.000 0.000 #&gt; #&gt; Node number 115: 41 observations #&gt; predicted class=acc expected loss=0.2682927 P(node) =0.02733333 #&gt; class counts: 11 30 0 0 #&gt; probabilities: 0.268 0.732 0.000 0.000 #&gt; #&gt; Node number 116: 28 observations #&gt; predicted class=unacc expected loss=0 P(node) =0.01866667 #&gt; class counts: 28 0 0 0 #&gt; probabilities: 1.000 0.000 0.000 0.000 #&gt; #&gt; Node number 117: 29 observations #&gt; predicted class=acc expected loss=0.137931 P(node) =0.01933333 #&gt; class counts: 4 25 0 0 #&gt; probabilities: 0.138 0.862 0.000 0.000 #&gt; #&gt; Node number 120: 29 observations #&gt; predicted class=unacc expected loss=0.2068966 P(node) =0.01933333 #&gt; class counts: 23 6 0 0 #&gt; probabilities: 0.793 0.207 0.000 0.000 #&gt; #&gt; Node number 121: 29 observations #&gt; predicted class=acc expected loss=0.1034483 P(node) =0.01933333 #&gt; class counts: 3 26 0 0 #&gt; probabilities: 0.103 0.897 0.000 0.000 #&gt; #&gt; Node number 124: 28 observations #&gt; predicted class=acc expected loss=0.1071429 P(node) =0.01866667 #&gt; class counts: 3 25 0 0 #&gt; probabilities: 0.107 0.893 0.000 0.000 #&gt; #&gt; Node number 125: 53 observations, complexity param=0.01327434 #&gt; predicted class=good expected loss=0.3773585 P(node) =0.03533333 #&gt; class counts: 0 20 33 0 #&gt; probabilities: 0.000 0.377 0.623 0.000 #&gt; left son=250 (25 obs) right son=251 (28 obs) #&gt; Primary splits: #&gt; 価格帯 splits as --LR, improve=4.69137500, (0 missing) #&gt; 維持費 splits as --LR, improve=2.97708900, (0 missing) #&gt; ドア数 splits as LRRR, improve=2.68221700, (0 missing) #&gt; 収納性 splits as -LR, improve=1.42290200, (0 missing) #&gt; 乗車人数 splits as -LR, improve=0.04851752, (0 missing) #&gt; Surrogate splits: #&gt; 維持費 splits as --RL, agree=0.547, adj=0.04, (0 split) #&gt; #&gt; Node number 126: 28 observations #&gt; predicted class=good expected loss=0.3571429 P(node) =0.01866667 #&gt; class counts: 4 6 18 0 #&gt; probabilities: 0.143 0.214 0.643 0.000 #&gt; #&gt; Node number 127: 62 observations #&gt; predicted class=vgood expected loss=0.1935484 P(node) =0.04133333 #&gt; class counts: 0 3 9 50 #&gt; probabilities: 0.000 0.048 0.145 0.806 #&gt; #&gt; Node number 250: 25 observations, complexity param=0.01327434 #&gt; predicted class=acc expected loss=0.4 P(node) =0.01666667 #&gt; class counts: 0 15 10 0 #&gt; probabilities: 0.000 0.600 0.400 0.000 #&gt; left son=500 (12 obs) right son=501 (13 obs) #&gt; Primary splits: #&gt; 維持費 splits as --LR, improve=7.38461500, (0 missing) #&gt; ドア数 splits as LLRR, improve=0.63636360, (0 missing) #&gt; 収納性 splits as -LR, improve=0.11688310, (0 missing) #&gt; 乗車人数 splits as -LR, improve=0.05194805, (0 missing) #&gt; Surrogate splits: #&gt; ドア数 splits as RRLR, agree=0.56, adj=0.083, (0 split) #&gt; 収納性 splits as -RL, agree=0.56, adj=0.083, (0 split) #&gt; #&gt; Node number 251: 28 observations #&gt; predicted class=good expected loss=0.1785714 P(node) =0.01866667 #&gt; class counts: 0 5 23 0 #&gt; probabilities: 0.000 0.179 0.821 0.000 #&gt; #&gt; Node number 500: 12 observations #&gt; predicted class=acc expected loss=0 P(node) =0.008 #&gt; class counts: 0 12 0 0 #&gt; probabilities: 0.000 1.000 0.000 0.000 #&gt; #&gt; Node number 501: 13 observations #&gt; predicted class=good expected loss=0.2307692 P(node) =0.008666667 #&gt; class counts: 0 3 10 0 #&gt; probabilities: 0.000 0.231 0.769 0.000 plot(res_rp, uniform = T, margin = 0.03) # → 美しくない text(res_rp, all = T, use.n = T) # → 美しくない plotcp(res_rp) # res.pred &lt;- predict(res.rpart) head(res.pred); tail(res.pred) res.pred.1 &lt;- # apply(res.pred, 1, which.max) library(rpart.plot) res.rp &lt;- rpart(評価 ~ ., data = cardata, control = rpart.control(cp = 0.1)) rpart.plot(res_rp, digit = 3) #, col = c(&#39;red&#39;, &#39;orange&#39;, &#39;yellow&#39;, &#39;blue&#39;)) rpart.plot(res_rp, digit = 4, fallen.leaves = T, type = 3, extra = 101) 学習済モデルを使い, テスト用データで外挿予測のパフォーマンスを評価する. car_rp_pred &lt;- predict(res_rp, car_test, type = &quot;class&quot;) # クラス分類の予測結果出力 # car_rp_pred &lt;- predict(res_rp, car_test, type = &#39;prob&#39;)\\t# 確率の出力 パッケージcaretの関数confusionMatrix()を使って, 混同行列より各種評価指標を計算する. 2種類の表示モードで結果を示す: 適合率 (precision) - 再現率 (recall) 表示 感度(sensitivity) - 特異度 (specificity) 表示 # 混同行列 tbl_rp &lt;- table(car_rp_pred, car_test$評価) library(caret) # 適合率 (precision) - 再現率 (recall) 表示 confusionMatrix(tbl_rp, mode = &quot;prec_recall&quot;) #&gt; Confusion Matrix and Statistics #&gt; #&gt; #&gt; car_rp_pred unacc acc good vgood #&gt; unacc 159 1 0 0 #&gt; acc 3 51 0 1 #&gt; good 0 2 9 0 #&gt; vgood 0 0 0 2 #&gt; #&gt; Overall Statistics #&gt; #&gt; Accuracy : 0.9693 #&gt; 95% CI : (0.9378, 0.9876) #&gt; No Information Rate : 0.7105 #&gt; P-Value [Acc &gt; NIR] : &lt; 2.2e-16 #&gt; #&gt; Kappa : 0.9306 #&gt; #&gt; Mcnemar&#39;s Test P-Value : NA #&gt; #&gt; Statistics by Class: #&gt; #&gt; Class: unacc Class: acc Class: good Class: vgood #&gt; Precision 0.9938 0.9273 0.81818 1.000000 #&gt; Recall 0.9815 0.9444 1.00000 0.666667 #&gt; F1 0.9876 0.9358 0.90000 0.800000 #&gt; Prevalence 0.7105 0.2368 0.03947 0.013158 #&gt; Detection Rate 0.6974 0.2237 0.03947 0.008772 #&gt; Detection Prevalence 0.7018 0.2412 0.04825 0.008772 #&gt; Balanced Accuracy 0.9832 0.9607 0.99543 0.833333 # 感度(sensitivity) - 特異度 (specificity) 表示 confusionMatrix(tbl_rp) # mode = &#39;sens_spec&#39; (デフォルト) #&gt; Confusion Matrix and Statistics #&gt; #&gt; #&gt; car_rp_pred unacc acc good vgood #&gt; unacc 159 1 0 0 #&gt; acc 3 51 0 1 #&gt; good 0 2 9 0 #&gt; vgood 0 0 0 2 #&gt; #&gt; Overall Statistics #&gt; #&gt; Accuracy : 0.9693 #&gt; 95% CI : (0.9378, 0.9876) #&gt; No Information Rate : 0.7105 #&gt; P-Value [Acc &gt; NIR] : &lt; 2.2e-16 #&gt; #&gt; Kappa : 0.9306 #&gt; #&gt; Mcnemar&#39;s Test P-Value : NA #&gt; #&gt; Statistics by Class: #&gt; #&gt; Class: unacc Class: acc Class: good Class: vgood #&gt; Sensitivity 0.9815 0.9444 1.00000 0.666667 #&gt; Specificity 0.9848 0.9770 0.99087 1.000000 #&gt; Pos Pred Value 0.9938 0.9273 0.81818 1.000000 #&gt; Neg Pred Value 0.9559 0.9827 1.00000 0.995575 #&gt; Prevalence 0.7105 0.2368 0.03947 0.013158 #&gt; Detection Rate 0.6974 0.2237 0.03947 0.008772 #&gt; Detection Prevalence 0.7018 0.2412 0.04825 0.008772 #&gt; Balanced Accuracy 0.9832 0.9607 0.99543 0.833333 パッケージcaretは, 多種多様な機械学習アルゴリズムを統一的な環境で 実行し比較できる環境を提供する. サポートベクターマシン (SVM), 勾配ブースティング, ランダムフォレスト, ニューラルネット, …, など多様な手法・アルゴリズムをサポートする. "],["主成分分析-pca.html", "9 主成分分析 (PCA) 9.1 基本操作 9.2 2種類のPCA: 相関行列ベース vs 分散共分散行列ベース", " 9 主成分分析 (PCA) 9.1 基本操作 データセット1: ソニー株価リターン vs パナソニック株価リターン &quot;sony_pana_close_Oct2015-Feb2016.csv&quot; - date: 10/01/15--02/19/16 - sony: ソニー株価 (日次終値) - panasonic: パナソニック株価 (同) - n = 94, p = 2 price_dat &lt;- read.csv(&quot;sony_pana_close_Oct2015-Feb2016.csv&quot;, header = T, row.name = 1) sony_ret &lt;- diff(log(price_dat[, &quot;sony&quot;])) pana_ret &lt;- diff(log(price_dat[, &quot;panasonic&quot;])) ret_df &lt;- data.frame(sony = sony_ret, pana = pana_ret, row.names = rownames(price_dat[-1, ])) head(ret_df) #&gt; sony pana #&gt; 10/02/15 0.032186686 -0.012492607 #&gt; 10/05/15 0.015394788 0.013693327 #&gt; 10/06/15 0.004445863 0.007174204 #&gt; 10/07/15 0.015406687 0.028964843 #&gt; 10/08/15 -0.016357710 0.016832838 #&gt; 10/09/15 0.018228035 0.036868819 # PCA (分散共分散行列に対して) ret_pca &lt;- princomp(ret_df) # PCA on covariance matrix summary(ret_pca) plot(ret_pca) #&gt; Importance of components: #&gt; Comp.1 Comp.2 #&gt; Standard deviation 0.03647457 0.01539678 #&gt; Proportion of Variance 0.84876066 0.15123934 #&gt; Cumulative Proportion 0.84876066 1.00000000 ret_pca$center ret_pca$sd # PC負荷量（固有ベクトル） ret_pca$loadings plot(ret_pca$loadings) # PC得点（符号反転前) plot(ret_pca$scores) head(ret_pca$scores) #&gt; sony pana #&gt; -0.001931057 -0.004103680 #&gt; Comp.1 Comp.2 #&gt; 0.03647457 0.01539678 #&gt; #&gt; Loadings: #&gt; Comp.1 Comp.2 #&gt; sony 0.721 0.693 #&gt; pana 0.693 -0.721 #&gt; #&gt; Comp.1 Comp.2 #&gt; SS loadings 1.0 1.0 #&gt; Proportion Var 0.5 0.5 #&gt; Cumulative Var 0.5 1.0 #&gt; Comp.1 Comp.2 #&gt; 10/02/15 0.018766080 0.029702336 #&gt; 10/05/15 0.024824650 -0.000809378 #&gt; 10/06/15 0.012415077 -0.003704272 #&gt; 10/07/15 0.035422744 -0.011804820 #&gt; 10/08/15 0.004122787 -0.025089215 #&gt; 10/09/15 0.042936376 -0.015543559 # PCA (相関行列に対して) ret_pca &lt;- princomp(ret_df, cor = T) # PCA on correlation matrix summary(ret_pca) biplot(ret_pca) #&gt; Importance of components: #&gt; Comp.1 Comp.2 #&gt; Standard deviation 1.3027885 0.5502202 #&gt; Proportion of Variance 0.8486289 0.1513711 #&gt; Cumulative Proportion 0.8486289 1.0000000 # 変数のスケーリング影響確認 ret_df2 &lt;- data.frame(sony = sony_ret * 10, pana = pana_ret, row.names = rownames(price_dat[-1, ])) # PCA (分散共分散行列に対して) ret_pca2 &lt;- princomp(ret_df2) # PCA on covariance matrix summary(ret_pca2) ret_pca2$loadings # PCA (分散共分散行列に対して) ret_pca3 &lt;- princomp(ret_df2, cor = T) # PCA on covariance matrix summary(ret_pca3) ret_pca3$loadings #&gt; Importance of components: #&gt; Comp.1 Comp.2 #&gt; Standard deviation 0.2843268 0.019751604 #&gt; Proportion of Variance 0.9951974 0.004802622 #&gt; Cumulative Proportion 0.9951974 1.000000000 #&gt; #&gt; Loadings: #&gt; Comp.1 Comp.2 #&gt; sony 0.998 #&gt; pana -0.998 #&gt; #&gt; Comp.1 Comp.2 #&gt; SS loadings 1.0 1.0 #&gt; Proportion Var 0.5 0.5 #&gt; Cumulative Var 0.5 1.0 #&gt; Importance of components: #&gt; Comp.1 Comp.2 #&gt; Standard deviation 1.3027885 0.5502202 #&gt; Proportion of Variance 0.8486289 0.1513711 #&gt; Cumulative Proportion 0.8486289 1.0000000 #&gt; #&gt; Loadings: #&gt; Comp.1 Comp.2 #&gt; sony 0.707 0.707 #&gt; pana 0.707 -0.707 #&gt; #&gt; Comp.1 Comp.2 #&gt; SS loadings 1.0 1.0 #&gt; Proportion Var 0.5 0.5 #&gt; Cumulative Var 0.5 1.0 データセット2: 従業員スキル評価データ(仮想) 文字化けする場合には, 英語版”testdat_eng.csv”を使用しても良い. &quot;testdat_30_jap.csv&quot; (日本語版/英語版) - 氏名/Name - 専門性/Expertise (0-100) - 分析力/Analytics (同) - リーダーシップ/Leadership (同) - プレゼン力/Presentation (同) - コミュ力/Communication (同) - n = 9, p = 5 tokuten &lt;- read.csv(&quot;testdat_jap.csv&quot;, header = T, row.names = 1, skip = 1) # colnames(tokuten) &lt;- c(&#39;name&#39;, &#39;E&#39;, &#39;A&#39;, &#39;L&#39;, &#39;P&#39;, &#39;C&#39;) tokuten # 散布図 pairs(tokuten) #&gt; 専門性 分析力 リーダーシップ プレゼン力 コミュ力 #&gt; 山田 80 87 70 65 67 #&gt; 鈴木 85 83 65 68 63 #&gt; 田中 75 70 80 79 75 #&gt; 中村 70 72 85 83 79 #&gt; 大野 88 85 85 90 91 #&gt; 松井 75 78 79 85 80 #&gt; 高木 80 80 75 75 80 #&gt; 三浦 83 72 85 80 83 #&gt; 佐藤 69 82 77 70 80 # 分散共分散行列に対するPCA（デフォルト) tokuten_pc &lt;- princomp(tokuten) summary(tokuten_pc) plot(tokuten_pc) # Scree plot = barplot of PC variances (eigenvalues) screeplot(tokuten_pc) # same as above #&gt; Importance of components: #&gt; Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 #&gt; Standard deviation 12.4807420 7.1424787 4.73037885 2.9724343 1.57269337 #&gt; Proportion of Variance 0.6477709 0.2121478 0.09305346 0.0367422 0.01028558 #&gt; Cumulative Proportion 0.6477709 0.8599188 0.95297222 0.9897144 1.00000000 # 主成分負荷量(係数) tokuten_pc$loadings # t(tokuten_pc$loadings) %*% tokuten_pc$loadings\\t\\t# 直交性の確認 注) # 下のprcompの結果と, 符号が逆転 # scatter plot for 1st &amp; 2nd PC plot(tokuten_pc$loadings[, 1:2]) plot(tokuten_pc$loadings[, 1:2], type = &quot;n&quot;, main = &quot;PC loadings: PC1 vs PC2&quot;) # plot only axis text(tokuten_pc$loadings, colnames(tokuten)) #&gt; #&gt; Loadings: #&gt; Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 #&gt; 専門性 0.775 0.536 0.332 #&gt; 分析力 0.227 0.554 -0.634 -0.325 0.366 #&gt; リーダーシップ -0.510 -0.137 0.323 0.786 #&gt; プレゼン力 -0.591 0.153 0.275 -0.741 #&gt; コミュ力 -0.582 0.223 -0.486 0.362 -0.496 #&gt; #&gt; Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 #&gt; SS loadings 1.0 1.0 1.0 1.0 1.0 #&gt; Proportion Var 0.2 0.2 0.2 0.2 0.2 #&gt; Cumulative Var 0.2 0.4 0.6 0.8 1.0 # 主成分得点(データ毎) tokuten_pc$scores plot(tokuten_pc$scores[, 1], tokuten_pc$scores[, 2], type = &quot;n&quot;, main = &quot;PC scores: PC1 vs PC2&quot;) # text(tokuten_pc$scores[, 1], tokuten_pc$scores[, 2], text(tokuten_pc$scores[, 1], tokuten_pc$scores[, 2], rownames(tokuten), family = &quot;HiraKakuProN-W3&quot;) # 日本語文字化け対応 (mac) # バイプロット biplot(tokuten_pc, family = &quot;HiraKakuProN-W3&quot;) # 日本語文字化け対応 (mac) # biplot(tokuten_pc)\\t\\t# plot PC &amp; PC scores ===&gt; display first 2 PCs only... # # 計算結果確認 (yama &lt;- tokuten[&#39;山田&#39;, ]-apply(tokuten, 2, mean))\\t# # demeaned score sum(tokuten_pc$loadings[, &#39;Comp.1&#39;] * yama)\\t\\t\\t# Yamada&#39;s # PC1 score sum(tokuten_pc$loadings[, &#39;Comp.2&#39;] * yama)\\t\\t\\t# Yamada&#39;s PC2 # score # compare this with tokuten_pc$scores[&#39;Yamada&#39;, ] #&gt; Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 #&gt; 山田 19.304343 2.7048291 -2.4411238 0.5754498 2.6750248 #&gt; 鈴木 21.655520 4.6155395 5.6073003 -1.7490457 -0.7368287 #&gt; 田中 -2.739766 -8.0346039 5.4765050 0.1887216 -0.4687677 #&gt; 中村 -9.682974 -9.9826824 0.6190434 -2.0247358 1.8769769 #&gt; 大野 -17.277425 14.9224986 -1.8787424 -1.1322419 0.8666327 #&gt; 松井 -6.867523 -1.4328205 -0.3577567 -5.3750278 -1.0885751 #&gt; 高木 1.694979 2.5684468 -1.6399325 1.7537323 -2.8802837 #&gt; 三浦 -9.824535 0.5291295 4.8161616 5.9565572 0.4056066 #&gt; 佐藤 3.737381 -5.8903368 -10.2014547 1.8065903 -0.6497859 # 第1, 2主成分 tokuten_pc$scores[, 1:2] tokuten_pc$loadings[, 1:2] # biplot(tokuten_pc$scores[, 1:2], tokuten_pc$loadings[, 1:2])\\t# loadings # accompany &#39;arrows&#39; &lt;-- Looks more useful!! biplot(tokuten_pc$scores[, 1:2], tokuten_pc$loadings[, 1:2], family = &quot;HiraKakuProN-W3&quot;) # 日本語文字化け対応 (mac) # biplot(tokuten_pc$loadings[, 1:2], tokuten_pc$scores[, 1:2])\\t# scores # accompany &#39;arrows&#39; biplot(tokuten_pc$loadings[, 1:2], tokuten_pc$scores[, 1:2], family = &quot;HiraKakuProN-W3&quot;) # 日本語文字化け対応 (mac) #&gt; Comp.1 Comp.2 #&gt; 山田 19.304343 2.7048291 #&gt; 鈴木 21.655520 4.6155395 #&gt; 田中 -2.739766 -8.0346039 #&gt; 中村 -9.682974 -9.9826824 #&gt; 大野 -17.277425 14.9224986 #&gt; 松井 -6.867523 -1.4328205 #&gt; 高木 1.694979 2.5684468 #&gt; 三浦 -9.824535 0.5291295 #&gt; 佐藤 3.737381 -5.8903368 #&gt; Comp.1 Comp.2 #&gt; 専門性 0.03162775 0.7753770 #&gt; 分析力 0.22710092 0.5541994 #&gt; リーダーシップ -0.50974398 -0.1365068 #&gt; プレゼン力 -0.59111858 0.1530044 #&gt; コミュ力 -0.58151936 0.2227311 # 評価項目ペア別, 各従業員のスコア散布図 plot(tokuten) # par(mfrow = c(2, 2)) plot(tokuten[, c(1, 2)], type = &quot;n&quot;, main = &quot;item 1 vs item 2&quot;) # plot only axis # text(tokuten[, c(1, 2)], rownames(tokuten)) text(tokuten[, c(1, 2)], rownames(tokuten), family = &quot;HiraKakuProN-W3&quot;) # 日本語文字化け対応 (mac) plot(tokuten[, c(2, 3)], type = &quot;n&quot;, main = &quot;item 2 vs item 3&quot;) # plot only axis # text(tokuten[, c(2, 3)], rownames(tokuten)) text(tokuten[, c(2, 3)], rownames(tokuten), family = &quot;HiraKakuProN-W3&quot;) # 日本語文字化け対応 (mac) # plot(tokuten[, c(3, 4)], type = &#39;n&#39;)\\t# plot only axis text(tokuten[, c(3, # 4)], rownames(tokuten)) plot(tokuten[, c(4, 5)], type = &#39;n&#39;)\\t# plot only # axis text(tokuten[, c(4, 5)], rownames(tokuten)) # 相関行列に対するPCA tokuten_pc2 &lt;- princomp(tokuten, cor = T) # summary(tokuten_pc2) tokuten_pc2$loadings tokuten_pc2$scores[, 1:2] # tokuten_pc2$loadings[, 1:2] # 代替的関数 prcomp()\\t# 不偏分散共分散行列の使用 (÷(n-1)) tokuten_prcomp &lt;- prcomp(tokuten) # tokuten_prcomp &lt;- prcomp(tokuten, scale = T) summary(tokuten_prcomp) tokuten_prcomp$rotation # PC負荷量 tokuten_prcomp$x # PC得点 # biplot(tokuten_prcomp) biplot(tokuten_prcomp, family = &quot;HiraKakuProN-W3&quot;) #&gt; Importance of components: #&gt; PC1 PC2 PC3 PC4 PC5 #&gt; Standard deviation 13.2378 7.5757 5.01732 3.15274 1.66809 #&gt; Proportion of Variance 0.6478 0.2122 0.09305 0.03674 0.01029 #&gt; Cumulative Proportion 0.6478 0.8599 0.95297 0.98971 1.00000 #&gt; PC1 PC2 PC3 PC4 PC5 #&gt; 専門性 -0.03162775 0.7753770 0.53571555 0.3316583 -0.02831625 #&gt; 分析力 -0.22710092 0.5541994 -0.63357070 -0.3251925 -0.36623249 #&gt; リーダーシップ 0.50974398 -0.1365068 -0.01367592 0.3227645 -0.78559724 #&gt; プレゼン力 0.59111858 0.1530044 0.27483158 -0.7411911 0.04766342 #&gt; コミュ力 0.58151936 0.2227311 -0.48567230 0.3615403 0.49561793 #&gt; PC1 PC2 PC3 PC4 PC5 #&gt; 山田 -19.304343 2.7048291 -2.4411238 0.5754498 -2.6750248 #&gt; 鈴木 -21.655520 4.6155395 5.6073003 -1.7490457 0.7368287 #&gt; 田中 2.739766 -8.0346039 5.4765050 0.1887216 0.4687677 #&gt; 中村 9.682974 -9.9826824 0.6190434 -2.0247358 -1.8769769 #&gt; 大野 17.277425 14.9224986 -1.8787424 -1.1322419 -0.8666327 #&gt; 松井 6.867523 -1.4328205 -0.3577567 -5.3750278 1.0885751 #&gt; 高木 -1.694979 2.5684468 -1.6399325 1.7537323 2.8802837 #&gt; 三浦 9.824535 0.5291295 4.8161616 5.9565572 -0.4056066 #&gt; 佐藤 -3.737381 -5.8903368 -10.2014547 1.8065903 0.6497859 9.2 2種類のPCA: 相関行列ベース vs 分散共分散行列ベース データセット3: USArrests data USArrests - 全米50州, 人口10万人当たり - 1973年の凶悪犯罪(assault, murder, rape)件数, 及び都市部の人口割合(%) - n = 50, p = 4 summary(USArrests) pairs(USArrests) #&gt; Murder Assault UrbanPop Rape #&gt; Min. : 0.800 Min. : 45.0 Min. :32.00 Min. : 7.30 #&gt; 1st Qu.: 4.075 1st Qu.:109.0 1st Qu.:54.50 1st Qu.:15.07 #&gt; Median : 7.250 Median :159.0 Median :66.00 Median :20.10 #&gt; Mean : 7.788 Mean :170.8 Mean :65.54 Mean :21.23 #&gt; 3rd Qu.:11.250 3rd Qu.:249.0 3rd Qu.:77.75 3rd Qu.:26.18 #&gt; Max. :17.400 Max. :337.0 Max. :91.00 Max. :46.00 # どちらが良いか? (pc_cr1 &lt;- princomp(USArrests)) (pc_cr2 &lt;- # princomp(USArrests, cor = TRUE)) (pc_cr3 &lt;- prcomp(USArrests)) #&gt; Standard deviations (1, .., p=4): #&gt; [1] 83.732400 14.212402 6.489426 2.482790 #&gt; #&gt; Rotation (n x k) = (4 x 4): #&gt; PC1 PC2 PC3 PC4 #&gt; Murder 0.04170432 -0.04482166 0.07989066 -0.99492173 #&gt; Assault 0.99522128 -0.05876003 -0.06756974 0.03893830 #&gt; UrbanPop 0.04633575 0.97685748 -0.20054629 -0.05816914 #&gt; Rape 0.07515550 0.20071807 0.97408059 0.07232502 (pc_cr4 &lt;- prcomp(USArrests, scale = TRUE)) #&gt; Standard deviations (1, .., p=4): #&gt; [1] 1.5748783 0.9948694 0.5971291 0.4164494 #&gt; #&gt; Rotation (n x k) = (4 x 4): #&gt; PC1 PC2 PC3 PC4 #&gt; Murder -0.5358995 0.4181809 -0.3412327 0.64922780 #&gt; Assault -0.5831836 0.1879856 -0.2681484 -0.74340748 #&gt; UrbanPop -0.2781909 -0.8728062 -0.3780158 0.13387773 #&gt; Rape -0.5434321 -0.1673186 0.8177779 0.08902432 plot(pc_cr3) summary(pc_cr3) #&gt; Importance of components: #&gt; PC1 PC2 PC3 PC4 #&gt; Standard deviation 83.7324 14.21240 6.4894 2.48279 #&gt; Proportion of Variance 0.9655 0.02782 0.0058 0.00085 #&gt; Cumulative Proportion 0.9655 0.99335 0.9991 1.00000 plot(pc_cr4) summary(pc_cr4) #&gt; Importance of components: #&gt; PC1 PC2 PC3 PC4 #&gt; Standard deviation 1.5749 0.9949 0.59713 0.41645 #&gt; Proportion of Variance 0.6201 0.2474 0.08914 0.04336 #&gt; Cumulative Proportion 0.6201 0.8675 0.95664 1.00000 # PCAにかける変数の絞り込み prcomp(~ Murder + Assault + Rape, data = USArrests, # scale = TRUE) "],["探索的因子分析.html", "10 探索的因子分析 10.1 基本操作 10.2 ライブラリpsychの利用 10.3 因子分析に有用なツール", " 10 探索的因子分析 10.1 基本操作 データセット1: 従業員評価データ (仮想) ここでは, 上のPCAで使用した”testdat3_jap.csv”の拡大版を使用する. (文字化けする場合には, 英語版”testdat_eng.csv”を使用しても良い.) &quot;testdat_30_jap.csv&quot; (日本語版/英語版) - 氏名/Name - 専門性/Expertise (0-100) - 分析力/Analytics (同) - リーダーシップ/Leadership (同) - プレゼン力/Presentation (同) - コミュ力/Communication (同) - n = 30, p = 5 Rで標準的に用意されている関数factanal()を使用する. # (1) factanal: 最尤法only tokuten &lt;- read.csv(&quot;testdat_30_jap.csv&quot;, header = T, row.names = 1, skip = 1) # tokuten &lt;- read.csv(&#39;testdat_30_eng.csv&#39;, header = T, row.names = 1, skip = # 1) tokuten &lt;- read.csv(&#39;testdat_jap.csv&#39;, header = T, row.names = 1, skip = # 1) tokuten_fac &lt;- factanal(tokuten, factors = 2) # default varimax回転 tokuten_fac tokuten_fac$loading # 確認(各因子のloadingsベクトルの2乗和--&gt; SS loadings)) sum(tokuten_fac$loading[, 1]^2) #&gt; #&gt; Call: #&gt; factanal(x = tokuten, factors = 2) #&gt; #&gt; Uniquenesses: #&gt; 専門性 分析力 リーダーシップ プレゼン力 コミュ力 #&gt; 0.849 0.238 0.005 0.208 0.100 #&gt; #&gt; Loadings: #&gt; Factor1 Factor2 #&gt; 専門性 0.172 0.349 #&gt; 分析力 -0.368 0.792 #&gt; リーダーシップ 0.980 -0.186 #&gt; プレゼン力 0.890 #&gt; コミュ力 0.936 0.153 #&gt; #&gt; Factor1 Factor2 #&gt; SS loadings 2.794 0.807 #&gt; Proportion Var 0.559 0.161 #&gt; Cumulative Var 0.559 0.720 #&gt; #&gt; Test of the hypothesis that 2 factors are sufficient. #&gt; The chi square statistic is 4.44 on 1 degree of freedom. #&gt; The p-value is 0.0351 #&gt; #&gt; Loadings: #&gt; Factor1 Factor2 #&gt; 専門性 0.172 0.349 #&gt; 分析力 -0.368 0.792 #&gt; リーダーシップ 0.980 -0.186 #&gt; プレゼン力 0.890 #&gt; コミュ力 0.936 0.153 #&gt; #&gt; Factor1 Factor2 #&gt; SS loadings 2.794 0.807 #&gt; Proportion Var 0.559 0.161 #&gt; Cumulative Var 0.559 0.720 #&gt; [1] 2.794063 # 因子負荷量barplot(factor loadings) par(mfrow = c(1, 2)) barplot(tokuten_fac$loading[, 1], col = &quot;red&quot;) barplot(tokuten_fac$loading[, 2], col = &quot;blue&quot;) Tips: プロット時, 日本語が文字化けする場合の対応 (特に, macユーザー): &gt; par(family = &quot;HiraKakuProN-W3&quot;) または, &gt; par(family = &quot;HG明朝E&quot;) # 因子（負荷量）行列の回転 scores = &#39;regression&#39; ==&gt; Thomson&#39;s score返す # （デフォルト, 戻り値無) tokuten_fac2 &lt;- factanal(tokuten, factors = 2, rotation = &quot;none&quot;, scores = &quot;regression&quot;) # 回転なし tokuten_fac3 &lt;- factanal(tokuten, factors = 2, rotation = &quot;promax&quot;, scores = &quot;regression&quot;) # promax回転（斜交回転) tokuten_fac4 &lt;- factanal(tokuten, factors = 2, scores = &quot;regression&quot;) # varimax回転（直交回転) # 因子得点(factor scores) tokuten_fac2$scores #&gt; Factor1 Factor2 #&gt; 山田 -1.26800407 0.38570292 #&gt; 鈴木 -1.95989876 0.01060196 #&gt; 田中 0.20645006 -1.63631682 #&gt; 中村 0.91696834 -1.38312209 #&gt; 大野 0.96895473 1.74205043 #&gt; 小林 0.09169331 0.01065859 #&gt; 伊藤 -0.48179366 0.42756367 #&gt; 高橋 0.93201465 -0.89712752 #&gt; 渡辺 -0.22275387 0.27605947 #&gt; 佐藤 -0.35296159 -0.25515430 #&gt; 山下 1.22524580 -0.78746751 #&gt; 木村 0.80403959 -0.04966399 #&gt; 山本 -0.22649622 0.19899511 #&gt; 宮崎 1.67523625 -0.20442665 #&gt; 山口 -1.12973504 0.24392701 #&gt; 阿部 1.54630529 -0.22549756 #&gt; 斎藤 -0.75968450 1.72490202 #&gt; 吉田 -1.68984987 -1.46995307 #&gt; 佐々木 -0.80152021 -0.47761662 #&gt; 石川 0.66965259 0.60177580 #&gt; 山崎 -0.66802546 -0.35549465 #&gt; 中山 0.78533445 0.67322981 #&gt; 藤田 -0.77928205 0.88172247 #&gt; 加藤 -0.83886445 -0.82003132 #&gt; 清水 0.53238028 1.91115337 #&gt; 池田 0.49224657 -0.22365300 #&gt; 井上 -0.63623615 -0.04435814 #&gt; 林 -0.94437205 0.48040513 #&gt; 中島 0.21941624 -1.01764392 #&gt; 森 1.69353978 0.27877943 tokuten_fac3$scores #&gt; Factor1 Factor2 #&gt; 山田 -1.03462508 0.66053651 #&gt; 鈴木 -1.83144475 0.43930549 #&gt; 田中 -0.45570109 -1.67056983 #&gt; 中村 0.31020607 -1.57450638 #&gt; 大野 1.59853100 1.51845067 #&gt; 小林 0.09010822 -0.00947258 #&gt; 伊藤 -0.28165229 0.53011595 #&gt; 高橋 0.51707362 -1.09504471 #&gt; 渡辺 -0.09913025 0.32295107 #&gt; 佐藤 -0.43179522 -0.17623394 #&gt; 山下 0.83521312 -1.05026730 #&gt; 木村 0.73336738 -0.22523546 #&gt; 山本 -0.13320378 0.24721940 #&gt; 宮崎 1.48794583 -0.56956087 #&gt; 山口 -0.96135865 0.48945650 #&gt; 阿部 1.35883050 -0.56228456 #&gt; 斎藤 -0.02732251 1.87959737 #&gt; 吉田 -2.16579425 -1.09045510 #&gt; 佐々木 -0.94015940 -0.29907982 #&gt; 石川 0.86590070 0.45126013 #&gt; 山崎 -0.76668658 -0.20697743 #&gt; 中山 1.00259189 0.49692945 #&gt; 藤田 -0.38013416 1.04632852 #&gt; 加藤 -1.11095876 -0.63104097 #&gt; 清水 1.25670992 1.78193683 #&gt; 池田 0.37232594 -0.32985202 #&gt; 井上 -0.61349683 0.09512940 #&gt; 林 -0.69394517 0.68380497 #&gt; 中島 -0.19815346 -1.05885961 #&gt; 森 1.69675803 -0.09358169 tokuten_fac4$scores #&gt; Factor1 Factor2 #&gt; 山田 -1.18038461 0.602738127 #&gt; 鈴木 -1.92747306 0.355194538 #&gt; 田中 -0.08460701 -1.647117489 #&gt; 中村 0.65937060 -1.522855235 #&gt; 大野 1.26028286 1.544441669 #&gt; 小林 0.09213844 -0.005637015 #&gt; 伊藤 -0.39907004 0.505647037 #&gt; 高橋 0.75967163 -1.047085528 #&gt; 渡辺 -0.17071995 0.310938604 #&gt; 佐藤 -0.39234102 -0.189087623 #&gt; 山下 1.06762028 -0.990716564 #&gt; 木村 0.78276599 -0.190324936 #&gt; 山本 -0.18796003 0.235734215 #&gt; 宮崎 1.61315438 -0.495923070 #&gt; 山口 -1.06921083 0.438850587 #&gt; 阿部 1.48252735 -0.493985703 #&gt; 斎藤 -0.44441816 1.831638615 #&gt; 吉田 -1.92207351 -1.149777378 #&gt; 佐々木 -0.87303770 -0.329176947 #&gt; 石川 0.76506664 0.474596410 #&gt; 山崎 -0.72014255 -0.232441752 #&gt; 中山 0.89151385 0.524587102 #&gt; 藤田 -0.61203044 1.005054108 #&gt; 加藤 -0.97003241 -0.659683285 #&gt; 清水 0.86026215 1.787703833 #&gt; 池田 0.44522905 -0.306754687 #&gt; 井上 -0.63411816 0.068251272 #&gt; 林 -0.84514031 0.639034836 #&gt; 中島 0.03698524 -1.040372398 #&gt; 森 1.71617133 -0.023471343 # 因子負荷量(factor loadings) tokuten_fac2$loading #&gt; #&gt; Loadings: #&gt; Factor1 Factor2 #&gt; 専門性 0.108 0.374 #&gt; 分析力 -0.502 0.715 #&gt; リーダーシップ 0.997 #&gt; プレゼン力 0.873 0.171 #&gt; コミュ力 0.895 0.316 #&gt; #&gt; Factor1 Factor2 #&gt; SS loadings 2.821 0.780 #&gt; Proportion Var 0.564 0.156 #&gt; Cumulative Var 0.564 0.720 tokuten_fac3$loading #&gt; #&gt; Loadings: #&gt; Factor1 Factor2 #&gt; 専門性 0.249 0.348 #&gt; 分析力 -0.186 0.820 #&gt; リーダーシップ 0.930 -0.229 #&gt; プレゼン力 0.886 #&gt; コミュ力 0.963 0.118 #&gt; #&gt; Factor1 Factor2 #&gt; SS loadings 2.675 0.859 #&gt; Proportion Var 0.535 0.172 #&gt; Cumulative Var 0.535 0.707 tokuten_fac4$loading #&gt; #&gt; Loadings: #&gt; Factor1 Factor2 #&gt; 専門性 0.172 0.349 #&gt; 分析力 -0.368 0.792 #&gt; リーダーシップ 0.980 -0.186 #&gt; プレゼン力 0.890 #&gt; コミュ力 0.936 0.153 #&gt; #&gt; Factor1 Factor2 #&gt; SS loadings 2.794 0.807 #&gt; Proportion Var 0.559 0.161 #&gt; Cumulative Var 0.559 0.720 # tokuten_fac$loading # 因子得点と因子負荷量のbiplot biplot(tokuten_fac2$scores, # tokuten_fac2$loading) biplot(tokuten_fac3$scores, tokuten_fac3$loading) # biplot(tokuten_fac4$scores, tokuten_fac4$loading) biplot(tokuten_fac2$scores, tokuten_fac2$loading, family = &quot;HiraKakuProN-W3&quot;, main = &quot;無回転&quot;) # 日本語文字化け対応 (mac) biplot(tokuten_fac3$scores, tokuten_fac3$loading, family = &quot;HiraKakuProN-W3&quot;, main = &quot;promax回転&quot;) # 日本語文字化け対応 (mac) biplot(tokuten_fac4$scores, tokuten_fac4$loading, family = &quot;HiraKakuProN-W3&quot;, main = &quot;varimax回転&quot;) # 日本語文字化け対応 (mac) # 独自因子（共通因子で説明出来なかった変動の割合) tokuten_fac$uniquenesses # specific variance #&gt; 専門性 分析力 リーダーシップ プレゼン力 コミュ力 #&gt; 0.84871769 0.23762850 0.00500000 0.20796645 0.09970145 10.2 ライブラリpsychの利用 関数psych()では, モデル推定方法や因子負荷行列の回転方法等に選択肢がある. - モデル推定方法 (引数fm=): 最尤法 (&quot;ml&quot;), 一般化最小2乗法 (&quot;gls&quot;), 重み付き最小2乗法 (&quot;gls&quot;), 最小残差法 (&quot;mires&quot;). デフォルトは&quot;mires&quot; - 回転方法 (引数rotate=): オブリミン (&quot;oblimin&quot;), バリマックス(&quot;varimax&quot;)など. library(psych) # 主因子法 tokuten_fa &lt;- fa(r = tokuten, nfactors = 2, rotate = &quot;none&quot;, fm = &quot;pa&quot;, scores = T) # 回転なし tokuten_fa2 &lt;- fa(r = tokuten, nfactors = 2, rotate = &quot;oblimin&quot;, fm = &quot;pa&quot;, scores = T) # # tokuten_fa &lt;- fa(r = tokuten, nfactors = 2 , rotate = &#39;none&#39;, fm = &#39;ml&#39;, # scores = T) デフォルト: rotate = &#39;oblimin&#39;, fm = &#39;minres&#39; 最尤法, fm = &#39;ml&#39;, # 一般化最小2乗法, &#39;gls&#39;, 重み付き最小2乗法&#39;gls&#39;, 最小残差法&#39;mires&#39; # PCAとの比較 tokuten_fa\\t\\t# standardized loadings (pattern matrix)表示 summary(tokuten_fa) tokuten_fa2$loadings tokuten_fa$scores par(mfrow = c(1, 2)) # plot(tokuten_fa) plot(tokuten_fa2) biplot(tokuten_fa, main = &quot;無回転&quot;) biplot(tokuten_fa2, main = &quot;oblimin回転&quot;) par(mfrow = c(1, 1)) #&gt; #&gt; Factor analysis with Call: fa(r = tokuten, nfactors = 2, rotate = &quot;none&quot;, scores = T, fm = &quot;pa&quot;) #&gt; #&gt; Test of the hypothesis that 2 factors are sufficient. #&gt; The degrees of freedom for the model is 1 and the objective function was 0.35 #&gt; The number of observations was 30 with Chi Square = 8.79 with prob &lt; 0.003 #&gt; #&gt; The root mean square of the residuals (RMSA) is 0.04 #&gt; The df corrected root mean square of the residuals is 0.12 #&gt; #&gt; Tucker Lewis Index of factoring reliability = 0.163 #&gt; RMSEA index = 0.508 and the 10 % confidence intervals are 0.245 0.856 #&gt; BIC = 5.39 #&gt; Loadings: #&gt; PA1 PA2 #&gt; 専門性 0.355 0.470 #&gt; 分析力 -0.184 0.660 #&gt; リーダーシップ 0.911 -0.255 #&gt; プレゼン力 0.958 #&gt; コミュ力 0.914 #&gt; #&gt; PA1 PA2 #&gt; SS loadings 2.744 0.738 #&gt; Proportion Var 0.549 0.148 #&gt; Cumulative Var 0.549 0.696 #&gt; PA1 PA2 #&gt; 山田 -1.02034162 0.019100487 #&gt; 鈴木 -1.81609325 0.890601637 #&gt; 田中 0.11710274 -1.131035071 #&gt; 中村 0.97786615 -1.497888977 #&gt; 大野 1.18686813 1.448453889 #&gt; 小林 0.14568067 0.518419196 #&gt; 伊藤 -0.66256206 0.755510959 #&gt; 高橋 0.82501884 -1.016144138 #&gt; 渡辺 -0.37950698 -0.296849891 #&gt; 佐藤 -0.35540858 0.222170566 #&gt; 山下 1.38305140 -0.665023036 #&gt; 木村 0.62270049 -0.476316678 #&gt; 山本 -0.44361495 -0.426060496 #&gt; 宮崎 1.64994564 -0.484538347 #&gt; 山口 -0.76190378 -0.118039940 #&gt; 阿部 1.56078515 0.162024351 #&gt; 斎藤 -0.74336529 2.028239692 #&gt; 吉田 -1.93942159 -0.937774682 #&gt; 佐々木 -0.94328316 -0.363190871 #&gt; 石川 0.73867734 0.571203066 #&gt; 山崎 -0.93450472 -0.725912234 #&gt; 中山 1.01147500 0.113093275 #&gt; 藤田 -0.67086121 1.318200214 #&gt; 加藤 -0.80985800 -1.173736127 #&gt; 清水 0.62804134 1.271351417 #&gt; 池田 0.76781783 -0.165605242 #&gt; 井上 -0.64479954 0.598812374 #&gt; 林 -0.96156230 0.407994217 #&gt; 中島 -0.06969367 -0.843983587 #&gt; 森 1.54175000 -0.003076023 10.3 因子分析に有用なツール ライブラリpsychの関数fa.parallel(), vss()は, 因子数の決定に有用である. # library(psych) 平行分析(parallel analysis) デフォルト: fm = &#39;minres&#39; (res_parallel &lt;- fa.parallel(tokuten)) # minres法(デフォルト), PCA &amp; 因子分析 # fa.parallel(tokuten, fm = &#39;wls&#39;) fa.parallel(tokuten, fm = &#39;ml&#39;, fa = # &#39;fa&#39;)\\t# 最尤法+因子分析(のみ)実行 サンプルデータから作られるscreeプロットと, # シミュレーションデータの行列(サンプルと同じサイズ)のscreeプロットとを比較 → # 因子数をsuggest fa = &#39;both&#39; (デフォルト): \\t\\t# PCA, # 主因子法の固有値を同時に表示 # VSS (Very Simple Structure) 規準 Very Simple Structure criterion ( VSS) for # estimating the optimal number of factors 最大となる因子数を探す (tokuten_vss # &lt;- vss(tokuten, n = 5, rotate = &#39;oblimin&#39;, fm = &#39;wls&#39;)\\t)\\t# n: Number of # factors to extract (&gt; (初期仮説の)因子数) デフォルト: # 因子数 n = 8, 回転 # rotate = &#39;varimax&#39; → VSS, MAP, その他の因子数決定基準の数値を表示 tmp_vss &lt;- vss(tokuten, n = 4, fm = &quot;ml&quot;) # VSS.plot(tmp_vss) print(tmp_vss) #&gt; Parallel analysis suggests that the number of factors = 1 and the number of components = 1 #&gt; Call: fa.parallel(x = tokuten) #&gt; Parallel analysis suggests that the number of factors = 1 and the number of components = 1 #&gt; #&gt; Eigen Values of #&gt; Original factors Resampled data Simulated data Original components #&gt; 1 2.78 1.02 1.08 2.96 #&gt; Resampled components Simulated components #&gt; 1 1.53 1.6 #&gt; #&gt; Very Simple Structure #&gt; Call: vss(x = tokuten, n = 4, fm = &quot;ml&quot;) #&gt; VSS complexity 1 achieves a maximimum of 0.81 with 1 factors #&gt; VSS complexity 2 achieves a maximimum of 0.92 with 3 factors #&gt; #&gt; The Velicer MAP achieves a minimum of 0.15 with 1 factors #&gt; BIC achieves a minimum of 1.04 with 2 factors #&gt; Sample Size adjusted BIC achieves a minimum of 4.15 with 2 factors #&gt; #&gt; Statistics by number of factors #&gt; vss1 vss2 map dof chisq prob sqresid fit RMSEA BIC SABIC complex #&gt; 1 0.81 0.00 0.15 5 2.1e+01 0.00083 2.01 0.81 0.32 3.9 19.5 1.0 #&gt; 2 0.80 0.92 0.27 1 4.4e+00 0.03506 0.88 0.92 0.34 1.0 4.2 1.2 #&gt; 3 0.79 0.92 0.45 -2 0.0e+00 NA 0.52 0.95 NA NA NA 1.5 #&gt; 4 0.79 0.92 1.00 -4 2.1e-14 NA 0.54 0.95 NA NA NA 1.5 #&gt; eChisq SRMR eCRMS eBIC #&gt; 1 1.1e+01 1.4e-01 0.19 -5.7 #&gt; 2 1.6e+00 5.2e-02 0.16 -1.8 #&gt; 3 7.8e-17 3.6e-10 NA NA #&gt; 4 1.7e-14 5.3e-09 NA NA ライブラリpsych内の関数fa.plot()やfa.diagram()は, 因子間の関係性 (相関構造や階層構造) を調べるのに有用である. fa.plot(tokuten_fa, cut = 0.5) # fa.plot(tokuten_fa2, cut = 0.5) fa.diagram(tokuten_fa) # fa.diagram(tokuten_fa2) また, ライブラリqgraphの関数qgraph()も同様な目的で有用である. # 相関ネットワーク(参考) library(qgraph) qgraph(cor(tokuten), edge.labels = T, minimum = 0.2, edge.color = &quot;black&quot;) # 確証的因子分析 library(lavaan) cfa()関数 "],["クラスター分析.html", "11 クラスター分析 11.1 基本操作 11.2 データ分析例 11.3 発展的なクラスター分析", " 11 クラスター分析 11.1 基本操作 クラスター分析の基本操作を学ぶため, 主成分分析の章でも使用した, 従業員スキル評価データ \\((n=9,p=5)\\) をここでも使用する. tokuten &lt;- read.csv(&quot;testdat_eng.csv&quot;, skip = 1, header = T, row.names = 1) # tokuten &lt;- read.csv(&#39;testdat_jap.csv&#39;, skip = 1, header = T, row.names = 1) # tokuten &lt;- read.csv(&#39;testdat_30_eng.csv&#39;, skip = 1, header = T, row.names = # 1) tokuten &lt;- read.csv(&#39;testdat_30_jap.csv&#39;, skip = 1, header = T, row.names # = 1) 階層型クラスタリング 階層型クラスタリングは, Rの標準パッケージの一つstatsに含まれる 関数hclust()を用いて実行することができる. hclust()への入力として, クラスタリング対象間の距離行列を与える必要がある. もし入力データセット (tokuten) の個体間 (行) のクラスタリングを行うのであれば そのまま関数dist()を適用すれば良い. 一方, 変数間 (列) のクラスタリングであれば, 一旦データセットを転置 (行と列の入れ替え) する必要がある. tokuten_dist &lt;- dist(tokuten) # method = &#39;binary&#39;, &#39;canberra&#39;, &#39;maximum&#39;, &#39;manhattan&#39; # 距離行列 round(tokuten_dist, 2) #&gt; yamada suzuki tanaka nakamura ohno Matsui takagi miura #&gt; suzuki 9.54 #&gt; tanaka 25.96 27.55 #&gt; nakamura 31.91 35.03 9.27 #&gt; ohno 38.65 41.00 28.21 26.19 #&gt; Matsui 27.50 29.98 11.22 10.10 20.00 #&gt; takagi 18.52 21.73 13.82 18.14 23.13 12.04 #&gt; miura 30.66 32.70 12.57 13.93 18.92 13.04 14.39 #&gt; sato 19.72 26.34 17.18 18.30 30.90 16.76 12.41 21.66 # 距離の近い人を集めて、クラスターを形成 1:C1 = {tanaka, nakamura} 2:C2 = # {yamada, suzuki}, or C2 = {C1, matsui} or else?? --&gt; どちらが優先される? # 最遠隣法(furthest negibhor method), 完全連結法(complete linkage method) (tokuten_hc_1 &lt;- hclust(tokuten_dist)) # method = &#39;complete&#39; #&gt; #&gt; Call: #&gt; hclust(d = tokuten_dist) #&gt; #&gt; Cluster method : complete #&gt; Distance : euclidean #&gt; Number of objects: 9 # summary(tokuten_hc_1) # クラスター形成履歴 マイナス付き =&gt; 個体番号. マイナス無し =&gt; クラスター番号 tokuten_hc_1$merge #&gt; [,1] [,2] #&gt; [1,] -3 -4 #&gt; [2,] -1 -2 #&gt; [3,] -6 1 #&gt; [4,] -7 -9 #&gt; [5,] -8 3 #&gt; [6,] 4 5 #&gt; [7,] -5 6 #&gt; [8,] 2 7 # デンドログラム (樹形図) の枝の高さ tokuten_hc_1$height #&gt; [1] 9.273618 9.539392 11.224972 12.409674 13.928388 21.656408 30.903074 #&gt; [8] 41.000000 # デンドログラム par(mfrow = c(1, 2)) plot(tokuten_hc_1) plot(tokuten_hc_1, hang = -1) # 葉の位置(高さ)を揃える # 各個体の属するクラスター番号 cutree(tokuten_hc_1, k = 3) # k: クラスター数 #&gt; yamada suzuki tanaka nakamura ohno Matsui takagi miura #&gt; 1 1 2 2 3 2 2 2 #&gt; sato #&gt; 2 # 代替的手法 (tokuten_hc_2 &lt;- hclust(tokuten_dist, method = &quot;single&quot;)) # 最近隣法 #&gt; #&gt; Call: #&gt; hclust(d = tokuten_dist, method = &quot;single&quot;) #&gt; #&gt; Cluster method : single #&gt; Distance : euclidean #&gt; Number of objects: 9 (tokuten_hc_3 &lt;- hclust(tokuten_dist, method = &quot;ward.D2&quot;)) # ウォード法 #&gt; #&gt; Call: #&gt; hclust(d = tokuten_dist, method = &quot;ward.D2&quot;) #&gt; #&gt; Cluster method : ward.D2 #&gt; Distance : euclidean #&gt; Number of objects: 9 (tokuten_hc_4 &lt;- hclust(tokuten_dist, method = &quot;average&quot;)) # 群平均法 #&gt; #&gt; Call: #&gt; hclust(d = tokuten_dist, method = &quot;average&quot;) #&gt; #&gt; Cluster method : average #&gt; Distance : euclidean #&gt; Number of objects: 9 (tokuten_hc_5 &lt;- hclust(tokuten_dist, method = &quot;centroid&quot;)) # 中心点法/重心法 #&gt; #&gt; Call: #&gt; hclust(d = tokuten_dist, method = &quot;centroid&quot;) #&gt; #&gt; Cluster method : centroid #&gt; Distance : euclidean #&gt; Number of objects: 9 (tokuten_hc_6 &lt;- hclust(tokuten_dist, method = &quot;median&quot;)) # メジアン法 #&gt; #&gt; Call: #&gt; hclust(d = tokuten_dist, method = &quot;median&quot;) #&gt; #&gt; Cluster method : median #&gt; Distance : euclidean #&gt; Number of objects: 9 par(mfrow = c(2, 2)) plot(tokuten_hc_1) plot(tokuten_hc_2) plot(tokuten_hc_3) plot(tokuten_hc_4) par(mfrow = c(1, 2)) plot(tokuten_hc_5) plot(tokuten_hc_6) 距離尺度として中心点法 (重心法) やメジアン法を選択した場合には, 「inversion (逆転) 現象」 の発生が確認される. 「inversion現象」とは、階層的クラスタリングのプロセスにおいて, 本来ならばステップが進むにつれてより”遠く”にあるクラスターと結合していくべきところ, 実際にはより”近く”のクラスターが後に統合されるという状況を指す. すなわち, 結合の順序が距離に関する単調性を失い, 距離の近い順に結合するという直感的な (本来あるべき) 順序に反する状態であり, クラスタリングがうまくいっていないことを示す. この現象が生じた場合には, 異なる距離尺度や統合方法を検討する必要がある. k-means法 非階層型クラスタリングの主要な方法であるk-means法は, Rの標準パッケージの一つstatsに含まれる 関数kmeans()を用いて実行することができる. # k-means法 (tokuten_hc_k &lt;- kmeans(tokuten, 3)) #&gt; K-means clustering with 3 clusters of sizes 6, 1, 2 #&gt; #&gt; Cluster means: #&gt; Expertise Analytics Leadership Presentation Communication #&gt; 1 75.33333 75.66667 80.16667 78.66667 79.5 #&gt; 2 88.00000 85.00000 85.00000 90.00000 91.0 #&gt; 3 82.50000 85.00000 67.50000 66.50000 65.0 #&gt; #&gt; Clustering vector: #&gt; yamada suzuki tanaka nakamura ohno Matsui takagi miura #&gt; 3 3 1 1 2 1 1 1 #&gt; sato #&gt; 1 #&gt; #&gt; Within cluster sum of squares by cluster: #&gt; [1] 540.3333 0.0000 45.5000 #&gt; (between_SS / total_SS = 72.9 %) #&gt; #&gt; Available components: #&gt; #&gt; [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; #&gt; [6] &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; tokuten_hc_k$cluster #&gt; yamada suzuki tanaka nakamura ohno Matsui takagi miura #&gt; 3 3 1 1 2 1 1 1 #&gt; sato #&gt; 1 (tokuten_hc_k &lt;- kmeans(tokuten, 2)) #&gt; K-means clustering with 2 clusters of sizes 7, 2 #&gt; #&gt; Cluster means: #&gt; Expertise Analytics Leadership Presentation Communication #&gt; 1 77.14286 77 80.85714 80.28571 81.14286 #&gt; 2 82.50000 85 67.50000 66.50000 65.00000 #&gt; #&gt; Clustering vector: #&gt; yamada suzuki tanaka nakamura ohno Matsui takagi miura #&gt; 2 2 1 1 1 1 1 1 #&gt; sato #&gt; 1 #&gt; #&gt; Within cluster sum of squares by cluster: #&gt; [1] 996.0 45.5 #&gt; (between_SS / total_SS = 51.9 %) #&gt; #&gt; Available components: #&gt; #&gt; [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; #&gt; [6] &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; tokuten_hc_k$cluster #&gt; yamada suzuki tanaka nakamura ohno Matsui takagi miura #&gt; 2 2 1 1 1 1 1 1 #&gt; sato #&gt; 1 変数に対するクラスタリング tokuten_dist3 &lt;- dist(t(tokuten)) tokuten_hc3 &lt;- hclust(tokuten_dist3, method = &quot;ward.D2&quot;) tokuten_hc3$merge # クラスター形成履歴 #&gt; [,1] [,2] #&gt; [1,] -3 -5 #&gt; [2,] -4 1 #&gt; [3,] -1 -2 #&gt; [4,] 2 3 tokuten_hc3$height # デンドログラム(樹形図)の枝の高さ #&gt; [1] 12.20656 14.61734 19.74842 44.74967 par(mfrow = c(1, 2)) plot(tokuten_hc3) plot(tokuten_hc3, hang = -1) # デンドログラム cutree(tokuten_hc3, k = 3) # 各個体の属するクラスター番号 (k: クラスター数) #&gt; Expertise Analytics Leadership Presentation Communication #&gt; 1 2 3 3 3 par(mfrow = c(1, 1)) 11.2 データ分析例 データセット (1): ワイン品質データ (再掲) 回帰木で使用したワイン品質データを使用する. ここでは, 回帰問題において目的変数であったワイン品質quality は除き, ワインの化学的特性を表す11の変数のみを用いて, 白ワインのデータセットをクラスター分類することを試みる. - winequality-white.csv - fixed acidity: 酢酸濃度 - volitle acidity: 揮発酸濃度 - citric acidity: クエン酸濃度 - chlorides: 塩化物 - sulfur dioxide: 二酸化硫黄 - sulphate: 硫酸塩 - fixed acidity: 酒石酸含有量（g/dm3) - volatile acidity: 酢酸含有量（g/dm3) - citric acid: クエン酸含有量（g/dm3) - residual sugar: 残留糖分含有量（g/dm3） - chlorides: 塩化ナトリウム含有量（g/dm3) - free sulfur dioxide: 遊離亜硫酸含有量（mg/dm3） - total sulfur dioxide: 総亜硫酸含有量（mg/dm3） - density: 密度（g/dm3) - pH: pH - sulphates: 硫酸カリウム含有量（g/dm3） - alcohol: アルコール度数（% vol.） - quality: ワインの品質 (0 (very bad) -- 10 (excellent)) winedat &lt;- read.csv(&quot;winequality-white.csv&quot;, sep = &quot;;&quot;, skip = 1, header = T) wine &lt;- winedat[, -12] # qualityを除く wine_s &lt;- scale(wine) # 標準化 レコードのクラスタリング 個別のワイン (行) を, 11個の化学的特性 (列) に関する類似性によりクラスタリングする. # wine_dist &lt;- dist(wine) wine_dist &lt;- dist(wine_s) # method = &#39;binary&#39;, &#39;canberra&#39;, &#39;maximum&#39;, &#39;manhattan&#39; (wine_hc &lt;- hclust(wine_dist, method = &quot;ward.D2&quot;)) # ウォード法 #&gt; #&gt; Call: #&gt; hclust(d = wine_dist, method = &quot;ward.D2&quot;) #&gt; #&gt; Cluster method : ward.D2 #&gt; Distance : euclidean #&gt; Number of objects: 4898 plot(wine_hc, labels = F, main = &quot;wineデータ (標準化後): レコードのクラスタリング\\nEuclidean + Ward&quot;, family = &quot;HiraKakuProN-W3&quot;) # plot(wine_hc, labels=F, main=&#39;wineデータ (標準化前): # レコードのクラスタリング\\nEuclidean + Ward&#39;) # 指定したクラスター数kで, デンドログラムを切断 rect.hclust(wine_hc, k = 5, border = &quot;red&quot;) rect.hclust(wine_hc, k = 9, border = &quot;blue&quot;) 変数のクラスタリング 11個の化学的特性 (列) を, ワインに関する類似性によりクラスタリングする. # wine_dist &lt;- dist(t(wine)) wine_dist &lt;- dist(t(wine_s)) (wine_hc &lt;- hclust(wine_dist, method = &quot;ward.D2&quot;)) # ウォード法 #&gt; #&gt; Call: #&gt; hclust(d = wine_dist, method = &quot;ward.D2&quot;) #&gt; #&gt; Cluster method : ward.D2 #&gt; Distance : euclidean #&gt; Number of objects: 11 plot(wine_hc, main = &quot;wineデータ (標準化後): 変数のクラスタリング\\nEuclidean + Ward&quot;, family = &quot;HiraKakuProN-W3&quot;) # plot(wine_hc, main=&#39;wineデータ (標準化前): 変数のクラスタリング\\nEuclidean + # Ward&#39;) クラスター間距離による結果の相違 (wine_hc &lt;- hclust(wine_dist, method = &quot;complete&quot;)) #&gt; #&gt; Call: #&gt; hclust(d = wine_dist, method = &quot;complete&quot;) #&gt; #&gt; Cluster method : complete #&gt; Distance : euclidean #&gt; Number of objects: 11 plot(wine_hc, main = &quot;wineデータ (標準化後): レコードのクラスタリング\\nEuclidean + complete&quot;, family = &quot;HiraKakuProN-W3&quot;) (wine_hc &lt;- hclust(wine_dist, method = &quot;single&quot;)) #&gt; #&gt; Call: #&gt; hclust(d = wine_dist, method = &quot;single&quot;) #&gt; #&gt; Cluster method : single #&gt; Distance : euclidean #&gt; Number of objects: 11 plot(wine_hc, main = &quot;wineデータ (標準化後): レコードのクラスタリング\\nEuclidean + single&quot;, family = &quot;HiraKakuProN-W3&quot;) (wine_hc &lt;- hclust(wine_dist, method = &quot;average&quot;)) #&gt; #&gt; Call: #&gt; hclust(d = wine_dist, method = &quot;average&quot;) #&gt; #&gt; Cluster method : average #&gt; Distance : euclidean #&gt; Number of objects: 11 plot(wine_hc, main = &quot;wineデータ (標準化後): レコードのクラスタリング\\nEuclidean + average&quot;, family = &quot;HiraKakuProN-W3&quot;) (wine_hc &lt;- hclust(wine_dist, method = &quot;centroid&quot;)) #&gt; #&gt; Call: #&gt; hclust(d = wine_dist, method = &quot;centroid&quot;) #&gt; #&gt; Cluster method : centroid #&gt; Distance : euclidean #&gt; Number of objects: 11 plot(wine_hc, main = &quot;wineデータ (標準化後): レコードのクラスタリング\\nEuclidean + centroid&quot;, family = &quot;HiraKakuProN-W3&quot;) 重心法 (中心点法)において, inversion現象の発生が確認される. データセット (2): (ID無し) POSデータ 地方のサービスエリアの売店のレシートデータ500件 (実際のデータに加工を加えた上でランダムにサブセット化. オリジナルの地域・店舗が特定されないように工夫). &quot;pos_sample500_dist.csv&quot; - n = 500, p = 16 - 行: レシート (顧客の一回当り購入バスケット) - 列: &quot;洋菓子土産&quot;〜&quot;デザート類&quot;までの16分類の商品群 - 値: 対応するレシートに含まれる対応する商品の売上個数 (0,1,2,...) データ読み込み &amp; 距離行列の計算 # posデータの読み込み posdat &lt;- read.csv(file(&quot;pos_sample500_dist.csv&quot;, encoding = &quot;Shift_JIS&quot;)) # 500件 階層クラスタリング 16個の商品分類 (列) を, それらの買われ方 (レシートへの反映のされ方) の類似性によって, クラスタリングする. # 商品分類間の距離行列 pos_dist &lt;- dist(t(posdat)) # カテゴリー間の距離 # pos_dist &lt;- dist(posdat)\\t\\t\\t# 顧客間の距離 pos_hc &lt;- hclust(pos_dist, method = &quot;ward.D2&quot;) pos_hc$merge # クラスター形成履歴 #&gt; [,1] [,2] #&gt; [1,] -9 -10 #&gt; [2,] -6 -13 #&gt; [3,] -7 1 #&gt; [4,] 2 3 #&gt; [5,] -8 4 #&gt; [6,] -3 5 #&gt; [7,] -16 6 #&gt; [8,] -11 7 #&gt; [9,] -5 8 #&gt; [10,] -12 9 #&gt; [11,] -14 10 #&gt; [12,] -4 11 #&gt; [13,] -15 12 #&gt; [14,] -1 13 #&gt; [15,] -2 14 pos_hc$height # デンドログラム(樹形図)の高さ #&gt; [1] 7.280110 8.062258 8.103497 8.644844 9.014803 9.549370 9.702724 #&gt; [8] 10.785793 12.548572 13.754999 15.099669 18.753461 30.374223 41.487118 #&gt; [15] 65.724108 plot(pos_hc, hang = -1, family = &quot;HiraKakuProN-W3&quot;) # 問題点? デンドログラム内に「chaining現象」が発生していることが分かる. 「chaining現象」は, クラスターが連鎖的に長く伸びる形で形成される現象を指す. この現象は, 階層クラスタリングの過程で, 一つのクラスターが次々に近くのデータ点を吸収して成長していくことで生する. chaining現象は, 特に, 単連結法で生じやすいと考えられる. 本来異なるクラスターに属すべきデータポイントが, 同一のクラスターに統合されてしまうことで, データの真の構造を見落としてしまう可能性がある. 対処法として, クラスター間の結合方法を変える (単連結法から他の方法) ことでchaining現象の影響を軽減できる場合がある. また, 距離尺度を変更することで軽減できる場合がある. このposデータの取る値は, 各レシートごとの商品群の購買個数を表している. 同時購買されやすい・されにくいが商品群間の距離に反映されてしまっている. ここでは, データに加工を施し, 値が購買個数 (頻度) → 購買有無 (0/1)に変換してみる. posdat2 &lt;- posdat posdat2[posdat2 &gt;= 2] &lt;- 1 pos_dist2 &lt;- dist(t(posdat2)) # pos_hc2 &lt;- hclust(pos_dist2, method = &quot;ward.D2&quot;) pos_hc2$merge # クラスター形成履歴 #&gt; [,1] [,2] #&gt; [1,] -7 -9 #&gt; [2,] -10 1 #&gt; [3,] -16 2 #&gt; [4,] -3 -13 #&gt; [5,] 3 4 #&gt; [6,] -6 5 #&gt; [7,] -5 6 #&gt; [8,] -11 -14 #&gt; [9,] -8 7 #&gt; [10,] 8 9 #&gt; [11,] -12 10 #&gt; [12,] -4 11 #&gt; [13,] -1 -2 #&gt; [14,] -15 12 #&gt; [15,] 13 14 pos_hc2$height # デンドログラム(樹形図)の高さ #&gt; [1] 4.472136 4.760952 4.983305 5.099020 5.369668 5.550633 6.989788 #&gt; [8] 7.348469 7.348469 8.011356 9.468448 9.536032 15.874508 16.016132 #&gt; [15] 20.502178 plot(pos_hc2, hang = -1, family = &quot;HiraKakuProN-W3&quot;) 更新されたデンドログラムより, chainingが緩和されたのが確認される. 次に, 距離尺度およびクラスター結合方法の変更を試みる. ここでは, 距離尺度としてコサイン距離を, クラスター結合方法としてウォード法を採用する. # マンハッタン距離 pos_dist &lt;- dist(t(posdat), method = &#39;manhattan&#39;)\\t\\t # cosine距離の使用 library(proxy) pos_dist &lt;- proxy::dist(t(posdat), method = &quot;cosine&quot;) # cosine距離 pos_hc &lt;- hclust(pos_dist, method = &quot;ward.D2&quot;) plot(pos_hc, hang = -1, family = &quot;HiraKakuProN-W3&quot;) pos_dist2 &lt;- proxy::dist(t(posdat2), method = &quot;cosine&quot;) # cosine距離 pos_hc2 &lt;- hclust(pos_dist2, method = &quot;ward.D2&quot;) plot(pos_hc2, hang = -1, family = &quot;HiraKakuProN-W3&quot;) 以上, データ間の距離やクラスター結合方法の選択が結果に大きく影響することが確認される. Hartiganルールによるクラスター数Kの決定 Hartiganルールでは, (k+1)番目のクラスターを加えるか否かの判定を逐次行い, 最適なクラスター数\\(K\\)を決定する. library(useful) ## pos_km &lt;- FitKMeans(posdat, max.cluster = 20, seed = 1)\\t# 客 pos_km_item &lt;- FitKMeans(t(posdat), seed = 1) # 商品 pos_km_item #&gt; Clusters Hartigan AddCluster #&gt; 1 2 14.359657 TRUE #&gt; 2 3 8.985054 FALSE #&gt; 3 4 7.062050 FALSE #&gt; 4 5 3.181427 FALSE #&gt; 5 6 2.307692 FALSE #&gt; 6 7 2.131698 FALSE #&gt; 7 8 1.964241 FALSE #&gt; 8 9 1.112276 FALSE #&gt; 9 10 1.706499 FALSE #&gt; 10 11 1.190476 FALSE #&gt; 11 12 1.142857 FALSE # (クラスター数, Hartigan数, クラスターを追加するべきか否か) PlotHartigan(pos_km_item) # 閾値=10(デフォルト) 上記出力より, クラスター数\\(K=2\\)が選択される. 次元縮約 → 顧客 (レシート) のクラスタリング つぎに, 顧客 (レシート) のクラスタリングを行う. 変数の次元が大きい場合には, クラスタリングの実行に先立って, 類似性を測る変数の次元を落とすのが効果的なことが多い. 本データセットの商品分類数 (16) は, 大きいとは言えないが, デモンストレーションのため, 例えばPCAによる次元縮約によって 事前に5つに絞り込んでおく. # 変数(商品)に関して次元縮約pca実行 posdat_pc &lt;- prcomp(posdat) posdat_pc &lt;- prcomp(posdat, scale. = T) # 標準化 summary(posdat_pc) #&gt; Importance of components: #&gt; PC1 PC2 PC3 PC4 PC5 PC6 PC7 #&gt; Standard deviation 1.26384 1.22037 1.10340 1.07873 1.03901 1.0245 0.99523 #&gt; Proportion of Variance 0.09983 0.09308 0.07609 0.07273 0.06747 0.0656 0.06191 #&gt; Cumulative Proportion 0.09983 0.19291 0.26901 0.34174 0.40921 0.4748 0.53672 #&gt; PC8 PC9 PC10 PC11 PC12 PC13 PC14 #&gt; Standard deviation 0.9911 0.9847 0.96511 0.93339 0.91726 0.89735 0.8772 #&gt; Proportion of Variance 0.0614 0.0606 0.05821 0.05445 0.05259 0.05033 0.0481 #&gt; Cumulative Proportion 0.5981 0.6587 0.71693 0.77138 0.82397 0.87430 0.9224 #&gt; PC15 PC16 #&gt; Standard deviation 0.8070 0.76844 #&gt; Proportion of Variance 0.0407 0.03691 #&gt; Cumulative Proportion 0.9631 1.00000 plot(posdat_pc, family = &quot;HiraKakuProN-W3&quot;) posdat_score5 &lt;- posdat_pc$x[, 1:5] posdat_pc$rotation[, 1:5] #&gt; PC1 PC2 PC3 PC4 PC5 #&gt; 洋菓子土産 0.19801045 -0.41680964 0.07830286 -0.204893941 0.15189427 #&gt; 和菓子土産 0.31926545 -0.17429488 0.29451622 -0.024368667 -0.06823826 #&gt; 地域限定菓子 0.11093983 -0.21464500 0.08853735 -0.122379290 0.51464240 #&gt; 水産加工品 0.38402139 0.47345139 0.12406842 0.018704524 -0.05301032 #&gt; 地元名産品 0.28312609 0.25920695 0.07566671 0.432182990 0.21885500 #&gt; 畜産加工品 0.31510689 0.36152933 0.04970947 -0.404289871 0.10659114 #&gt; 農産加工品 0.14914158 -0.03604771 0.16126513 0.142280575 -0.30221851 #&gt; ご当地グロッサリー 0.20348286 0.20049364 0.03892426 -0.150258935 -0.38320847 #&gt; 玩具土産 -0.18783969 0.06869904 0.48245433 -0.205983425 0.07184299 #&gt; 雑貨土産 -0.08771897 0.09893981 0.46577309 -0.185847154 -0.18455564 #&gt; 雑貨類 -0.13980819 0.13804035 -0.38617293 -0.166604346 -0.17361526 #&gt; 菓子類 -0.33688987 0.10484451 0.32900498 -0.001601135 0.21715666 #&gt; 麺類 0.21239584 0.14992291 -0.10107137 -0.106430151 0.49276377 #&gt; パン.弁当類 -0.14576690 0.29113765 -0.32482427 -0.311169546 0.14077896 #&gt; 飲料 -0.46157964 0.32927414 0.15896997 0.041046373 0.11399644 #&gt; デザート等 -0.02334359 0.16142448 0.01003826 0.578340358 0.12251255 posdat_dist5 &lt;- dist(posdat_score5) # レコード間 # posdat_dist5 &lt;- dist(t(posdat_score5)) # 合成変数間 posdat_hc5 &lt;- hclust(posdat_dist5, method = &quot;ward.D2&quot;) plot(posdat_hc5, family = &quot;HiraKakuProN-W3&quot;) rect.hclust(posdat_hc5, k = 8, border = &quot;blue&quot;) # pos_km5 &lt;- FitKMeans(posdat_score5, max.cluster = 20, seed = 1) # PlotHartigan(pos_km5) 11.3 発展的なクラスター分析 先に使用した従業員評価データ (拡張版) をここでも使用する. 距離としては, 相関係数 (類似度) をベースにしたcosine距離 (=1-相関係数) を使用する. tokuten &lt;- read.csv(&quot;testdat_30_jap.csv&quot;, skip = 1, header = T, row.names = 1) 関数agnes パッケージcluster内にある関数agnes (AGglomerative NESting) を使うことで, 階層型クラスタリングをより高度に制御しながら実行することが可能. library(cluster) dist_matrix &lt;- as.dist(1 - cor(tokuten)) res_agnes &lt;- agnes(dist_matrix, method = &quot;average&quot;) plot(res_agnes) dist_matrix &lt;- as.dist(1 - cor(t(tokuten))) res_agnes &lt;- agnes(dist_matrix, method = &quot;average&quot;) plot(res_agnes) 関数pheatmap パッケージpheatmapは, 主にヒートマップの作成に使用されるが, 相関に基づく階層型クラスタリングを行い, ヒートマップとして可視化する機能もあり. 遺伝子発現データなどのbioinformatics分野で活用. library(pheatmap) pheatmap(tokuten, clustering_distance_rows = as.dist(1 - cor(t(tokuten))), clustering_distance_cols = as.dist(1 - cor(tokuten))) 関数pam メドイド (medoid) は, クラスタ内の全ての点に対する距離の合計が最小となるような, クラスタ内に存在するデータ点であり, K-medoids法は, メドイドを中心としてクラスタリングを行う手法である. PAM (Partitioning Around Medoids) は, K-medoids法の考え方を実装した具体的アルゴリズムの一種である. Rでは, パッケージcluster内の関数pamを使用することができる. 一方, K-medoids法に類似したクラスタリング手法として, K-median法がある. K-median法は, 各クラスタ内のデータポイントの中央値に基づいて「中央点」決定し, クラスタ内の全データポイントと「中央点」との距離の合計を最小化するようにクラスタリングを行う. これらの手法を使うことで, 外れ値に強い, ロバストなクラスタリング結果が得られる. # pam関数によるK-medoids法の実行 k &lt;- 3 # クラスタの数 res_pam &lt;- pam(tokuten, k) print(res_pam) #&gt; Medoids: #&gt; ID 専門性 分析力 リーダーシップ プレゼン力 コミュ力 #&gt; 山口 15 77 87 71 68 66 #&gt; 伊藤 7 80 80 75 75 80 #&gt; 石川 20 80 80 83 86 86 #&gt; Clustering vector: #&gt; 山田 鈴木 田中 中村 大野 小林 伊藤 高橋 渡辺 佐藤 山下 #&gt; 1 1 2 3 3 3 2 3 2 2 3 #&gt; 木村 山本 宮崎 山口 阿部 斎藤 吉田 佐々木 石川 山崎 中山 #&gt; 3 2 3 1 3 2 1 1 3 1 3 #&gt; 藤田 加藤 清水 池田 井上 林 中島 森 #&gt; 2 1 3 3 2 1 2 3 #&gt; Objective function: #&gt; build swap #&gt; 10.01806 10.01806 #&gt; #&gt; Available components: #&gt; [1] &quot;medoids&quot; &quot;id.med&quot; &quot;clustering&quot; &quot;objective&quot; &quot;isolation&quot; #&gt; [6] &quot;clusinfo&quot; &quot;silinfo&quot; &quot;diss&quot; &quot;call&quot; &quot;data&quot; plot(res_pam) "],["アソシエーション分析.html", "12 アソシエーション分析 12.1 基本操作 12.2 データ分析例", " 12 アソシエーション分析 12.1 基本操作 arulesパッケージの機能 簡単な例 (仮想データ). library(arules) # as, apriori, inspect, itemFrequencyPlot # データ作成 data1 &lt;- list(c(&quot;パン&quot;, &quot;缶コーヒー&quot;, &quot;チーズ&quot;), c(&quot;パン&quot;, &quot;おにぎり&quot;, &quot;ビール&quot;, &quot;オムツ&quot;), c(&quot;おでん&quot;, &quot;ビール&quot;, &quot;チーズ&quot;), c(&quot;ビール&quot;, &quot;オムツ&quot;, &quot;タバコ&quot;), c(&quot;おでん&quot;, &quot;おにぎり&quot;, &quot;お茶&quot;), c(&quot;パン&quot;, &quot;ビール&quot;, &quot;ハム&quot;), c(&quot;ビール&quot;, &quot;タバコ&quot;)) data1 #&gt; [[1]] #&gt; [1] &quot;パン&quot; &quot;缶コーヒー&quot; &quot;チーズ&quot; #&gt; #&gt; [[2]] #&gt; [1] &quot;パン&quot; &quot;おにぎり&quot; &quot;ビール&quot; &quot;オムツ&quot; #&gt; #&gt; [[3]] #&gt; [1] &quot;おでん&quot; &quot;ビール&quot; &quot;チーズ&quot; #&gt; #&gt; [[4]] #&gt; [1] &quot;ビール&quot; &quot;オムツ&quot; &quot;タバコ&quot; #&gt; #&gt; [[5]] #&gt; [1] &quot;おでん&quot; &quot;おにぎり&quot; &quot;お茶&quot; #&gt; #&gt; [[6]] #&gt; [1] &quot;パン&quot; &quot;ビール&quot; &quot;ハム&quot; #&gt; #&gt; [[7]] #&gt; [1] &quot;ビール&quot; &quot;タバコ&quot; 入力データ形式に注意が必要. data_tran &lt;- as(data1, &quot;transactions&quot;) as(data_tran, &quot;matrix&quot;) # matrix形式 #&gt; おでん おにぎり お茶 オムツ タバコ チーズ ハム パン ビール 缶コーヒー #&gt; [1,] FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE FALSE TRUE #&gt; [2,] FALSE TRUE FALSE TRUE FALSE FALSE FALSE TRUE TRUE FALSE #&gt; [3,] TRUE FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE FALSE #&gt; [4,] FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE TRUE FALSE #&gt; [5,] TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #&gt; [6,] FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE FALSE #&gt; [7,] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE as(data_tran, &quot;data.frame&quot;) # data_frame形式 #&gt; items #&gt; 1 {チーズ,パン,缶コーヒー} #&gt; 2 {おにぎり,オムツ,パン,ビール} #&gt; 3 {おでん,チーズ,ビール} #&gt; 4 {オムツ,タバコ,ビール} #&gt; 5 {おでん,おにぎり,お茶} #&gt; 6 {ハム,パン,ビール} #&gt; 7 {タバコ,ビール} アイテム頻度プロット itemFrequencyPlot(data_tran, type = &quot;absolute&quot;) # support &gt;= 2 (支持度が2以上) itemFrequencyPlot(data_tran, type = &quot;absolute&quot;, support = 2) 次に, アソシエーション・ルール (相関ルール) の生成・抽出を行う. アソシエーション・ルールの生成・抽出 # 相関ルール抽出関数 data_ap &lt;- apriori(data_tran, parameter = NULL, appearance = NULL, control = NULL) #&gt; Apriori #&gt; #&gt; Parameter specification: #&gt; confidence minval smax arem aval originalSupport maxtime support minlen #&gt; 0.8 0.1 1 none FALSE TRUE 5 0.1 1 #&gt; maxlen target ext #&gt; 10 rules TRUE #&gt; #&gt; Algorithmic control: #&gt; filter tree heap memopt load sort verbose #&gt; 0.1 TRUE TRUE FALSE TRUE 2 TRUE #&gt; #&gt; Absolute minimum support count: 0 #&gt; #&gt; set item appearances ...[0 item(s)] done [0.00s]. #&gt; set transactions ...[10 item(s), 7 transaction(s)] done [0.00s]. #&gt; sorting and recoding items ... [10 item(s)] done [0.00s]. #&gt; creating transaction tree ... done [0.00s]. #&gt; checking subsets of size 1 2 3 4 done [0.00s]. #&gt; writing ... [32 rule(s)] done [0.00s]. #&gt; creating S4 object ... done [0.00s]. → 抽出されたアイテムがゼロ件. うまくいかない. 何故!? 実際, ルールの抽出条件を絞り込む必要有り. # support上位6位のルールを抽出 inspect(head(sort(data_ap, by = &quot;support&quot;), n = 6)) #&gt; lhs rhs support confidence coverage lift count #&gt; [1] {タバコ} =&gt; {ビール} 0.2857143 1 0.2857143 1.400000 2 #&gt; [2] {オムツ} =&gt; {ビール} 0.2857143 1 0.2857143 1.400000 2 #&gt; [3] {缶コーヒー} =&gt; {チーズ} 0.1428571 1 0.1428571 3.500000 1 #&gt; [4] {缶コーヒー} =&gt; {パン} 0.1428571 1 0.1428571 2.333333 1 #&gt; [5] {お茶} =&gt; {おでん} 0.1428571 1 0.1428571 3.500000 1 #&gt; [6] {お茶} =&gt; {おにぎり} 0.1428571 1 0.1428571 3.500000 1 inspect(head(sort(data_ap, by = &quot;lift&quot;), n = 6)) #&gt; lhs rhs support confidence coverage lift #&gt; [1] {チーズ, パン} =&gt; {缶コーヒー} 0.1428571 1 0.1428571 7.0 #&gt; [2] {おでん, おにぎり} =&gt; {お茶} 0.1428571 1 0.1428571 7.0 #&gt; [3] {缶コーヒー} =&gt; {チーズ} 0.1428571 1 0.1428571 3.5 #&gt; [4] {お茶} =&gt; {おでん} 0.1428571 1 0.1428571 3.5 #&gt; [5] {お茶} =&gt; {おにぎり} 0.1428571 1 0.1428571 3.5 #&gt; [6] {パン, 缶コーヒー} =&gt; {チーズ} 0.1428571 1 0.1428571 3.5 #&gt; count #&gt; [1] 1 #&gt; [2] 1 #&gt; [3] 1 #&gt; [4] 1 #&gt; [5] 1 #&gt; [6] 1 ルール数の調整の例. data_ap1 &lt;- apriori(data_tran, parameter = list(supp = 0.2, maxlen = 3)) #&gt; Apriori #&gt; #&gt; Parameter specification: #&gt; confidence minval smax arem aval originalSupport maxtime support minlen #&gt; 0.8 0.1 1 none FALSE TRUE 5 0.2 1 #&gt; maxlen target ext #&gt; 3 rules TRUE #&gt; #&gt; Algorithmic control: #&gt; filter tree heap memopt load sort verbose #&gt; 0.1 TRUE TRUE FALSE TRUE 2 TRUE #&gt; #&gt; Absolute minimum support count: 1 #&gt; #&gt; set item appearances ...[0 item(s)] done [0.00s]. #&gt; set transactions ...[10 item(s), 7 transaction(s)] done [0.00s]. #&gt; sorting and recoding items ... [7 item(s)] done [0.00s]. #&gt; creating transaction tree ... done [0.00s]. #&gt; checking subsets of size 1 2 done [0.00s]. #&gt; writing ... [2 rule(s)] done [0.00s]. #&gt; creating S4 object ... done [0.00s]. inspect(head(sort(data_ap1, by = &quot;support&quot;), n = 20)) # inspect in arules #&gt; lhs rhs support confidence coverage lift count #&gt; [1] {タバコ} =&gt; {ビール} 0.2857143 1 0.2857143 1.4 2 #&gt; [2] {オムツ} =&gt; {ビール} 0.2857143 1 0.2857143 1.4 2 → 結果の読み方の例: (1行目) タバコを購入した人 (lhs) がビールを購買する (rhs) ケースの支持度が0.2857, 確信度が1, リフト値が1.4 12.2 データ分析例 データセット: (ID無し) POSデータ (再掲) 地方のサービスエリアの売店のレシートデータ500件 (実際のデータに加工を加えた上でランダムにサブセット化. オリジナルの地域・店舗が特定されないように工夫). &quot;pos_sample500_dist.csv&quot; - n = 500, p = 16 - 行: レシート (顧客の一回当り購入バスケット) - 列: &quot;洋菓子土産&quot;〜&quot;デザート類&quot;までの16分類の商品群 - 値: 対応するレシートに含まれる対応する商品の売上個数 (0,1,2,...) posデータの読み込み # library(arules) posdat &lt;- as.matrix(read.csv(file(&quot;pos_sample500_dist.csv&quot;, encoding = &quot;Shift_JIS&quot;), header = T)) head(posdat) #&gt; 洋菓子土産 和菓子土産 地域限定菓子 水産加工品 地元名産品 畜産加工品 #&gt; [1,] 0 2 0 0 0 0 #&gt; [2,] 2 3 0 0 0 0 #&gt; [3,] 0 0 0 2 1 0 #&gt; [4,] 2 0 0 0 0 0 #&gt; [5,] 0 5 0 0 0 0 #&gt; [6,] 2 2 0 0 0 0 #&gt; 農産加工品 ご当地グロッサリー 玩具土産 雑貨土産 雑貨類 菓子類 麺類 #&gt; [1,] 0 0 0 0 0 1 0 #&gt; [2,] 0 0 0 0 0 0 0 #&gt; [3,] 0 0 0 0 0 0 0 #&gt; [4,] 0 1 0 0 0 0 0 #&gt; [5,] 0 0 0 0 0 0 0 #&gt; [6,] 0 0 0 0 0 0 0 #&gt; パン.弁当類 飲料 デザート等 #&gt; [1,] 0 2 0 #&gt; [2,] 0 0 0 #&gt; [3,] 0 0 0 #&gt; [4,] 0 0 0 #&gt; [5,] 0 0 0 #&gt; [6,] 0 0 0 data.tran &lt;- as(posdat, &quot;itemMatrix&quot;) # itemMatrix in sparse format アイテム頻度プロット # itemFrequencyPlot(data_tran, type = &#39;absolute&#39;) itemFrequencyPlot(data_tran) # 相対頻度 アソシエーション・ルール生成&amp;抽出 data.ap &lt;- apriori(data_tran, parameter = list(supp = 0.001, conf = 0.001, minlen = 2, maxlen = 3)) #&gt; Apriori #&gt; #&gt; Parameter specification: #&gt; confidence minval smax arem aval originalSupport maxtime support minlen #&gt; 0.001 0.1 1 none FALSE TRUE 5 0.001 2 #&gt; maxlen target ext #&gt; 3 rules TRUE #&gt; #&gt; Algorithmic control: #&gt; filter tree heap memopt load sort verbose #&gt; 0.1 TRUE TRUE FALSE TRUE 2 TRUE #&gt; #&gt; Absolute minimum support count: 0 #&gt; #&gt; set item appearances ...[0 item(s)] done [0.00s]. #&gt; set transactions ...[10 item(s), 7 transaction(s)] done [0.00s]. #&gt; sorting and recoding items ... [10 item(s)] done [0.00s]. #&gt; creating transaction tree ... done [0.00s]. #&gt; checking subsets of size 1 2 3 done [0.00s]. #&gt; writing ... [65 rule(s)] done [0.00s]. #&gt; creating S4 object ... done [0.00s]. 支持度でソート (上位20件) inspect(head(sort(data_ap, by = &quot;support&quot;), n = 20)) #&gt; lhs rhs support confidence coverage #&gt; [1] {タバコ} =&gt; {ビール} 0.2857143 1 0.2857143 #&gt; [2] {オムツ} =&gt; {ビール} 0.2857143 1 0.2857143 #&gt; [3] {缶コーヒー} =&gt; {チーズ} 0.1428571 1 0.1428571 #&gt; [4] {缶コーヒー} =&gt; {パン} 0.1428571 1 0.1428571 #&gt; [5] {お茶} =&gt; {おでん} 0.1428571 1 0.1428571 #&gt; [6] {お茶} =&gt; {おにぎり} 0.1428571 1 0.1428571 #&gt; [7] {ハム} =&gt; {パン} 0.1428571 1 0.1428571 #&gt; [8] {ハム} =&gt; {ビール} 0.1428571 1 0.1428571 #&gt; [9] {チーズ, 缶コーヒー} =&gt; {パン} 0.1428571 1 0.1428571 #&gt; [10] {パン, 缶コーヒー} =&gt; {チーズ} 0.1428571 1 0.1428571 #&gt; [11] {チーズ, パン} =&gt; {缶コーヒー} 0.1428571 1 0.1428571 #&gt; [12] {おでん, お茶} =&gt; {おにぎり} 0.1428571 1 0.1428571 #&gt; [13] {おにぎり, お茶} =&gt; {おでん} 0.1428571 1 0.1428571 #&gt; [14] {おでん, おにぎり} =&gt; {お茶} 0.1428571 1 0.1428571 #&gt; [15] {ハム, パン} =&gt; {ビール} 0.1428571 1 0.1428571 #&gt; [16] {ハム, ビール} =&gt; {パン} 0.1428571 1 0.1428571 #&gt; [17] {オムツ, タバコ} =&gt; {ビール} 0.1428571 1 0.1428571 #&gt; [18] {おでん, チーズ} =&gt; {ビール} 0.1428571 1 0.1428571 #&gt; [19] {チーズ, ビール} =&gt; {おでん} 0.1428571 1 0.1428571 #&gt; [20] {おでん, ビール} =&gt; {チーズ} 0.1428571 1 0.1428571 #&gt; lift count #&gt; [1] 1.400000 2 #&gt; [2] 1.400000 2 #&gt; [3] 3.500000 1 #&gt; [4] 2.333333 1 #&gt; [5] 3.500000 1 #&gt; [6] 3.500000 1 #&gt; [7] 2.333333 1 #&gt; [8] 1.400000 1 #&gt; [9] 2.333333 1 #&gt; [10] 3.500000 1 #&gt; [11] 7.000000 1 #&gt; [12] 3.500000 1 #&gt; [13] 3.500000 1 #&gt; [14] 7.000000 1 #&gt; [15] 1.400000 1 #&gt; [16] 2.333333 1 #&gt; [17] 1.400000 1 #&gt; [18] 1.400000 1 #&gt; [19] 3.500000 1 #&gt; [20] 3.500000 1 apriori関数のパラメータ指定において, minlen, maxlenによって, ルールに含まれるアイテム数の合計値 (lhs+rhs) の最小値, 最大値をコントロールする. 読み方の例: (20行目) 洋菓子土産と和菓子土産を同時に買った人 (lhs) が飲料 (rhs) を買うケースの支持度が0.026, 確信度が0.1806, リフト値が0.6019 # 確信度でソート(上位10件) inspect(head(sort(data_ap, by = &quot;confidence&quot;), n = 10)) #&gt; lhs rhs support confidence coverage lift #&gt; [1] {缶コーヒー} =&gt; {チーズ} 0.1428571 1 0.1428571 3.500000 #&gt; [2] {缶コーヒー} =&gt; {パン} 0.1428571 1 0.1428571 2.333333 #&gt; [3] {お茶} =&gt; {おでん} 0.1428571 1 0.1428571 3.500000 #&gt; [4] {お茶} =&gt; {おにぎり} 0.1428571 1 0.1428571 3.500000 #&gt; [5] {ハム} =&gt; {パン} 0.1428571 1 0.1428571 2.333333 #&gt; [6] {ハム} =&gt; {ビール} 0.1428571 1 0.1428571 1.400000 #&gt; [7] {タバコ} =&gt; {ビール} 0.2857143 1 0.2857143 1.400000 #&gt; [8] {オムツ} =&gt; {ビール} 0.2857143 1 0.2857143 1.400000 #&gt; [9] {チーズ, 缶コーヒー} =&gt; {パン} 0.1428571 1 0.1428571 2.333333 #&gt; [10] {パン, 缶コーヒー} =&gt; {チーズ} 0.1428571 1 0.1428571 3.500000 #&gt; count #&gt; [1] 1 #&gt; [2] 1 #&gt; [3] 1 #&gt; [4] 1 #&gt; [5] 1 #&gt; [6] 1 #&gt; [7] 2 #&gt; [8] 2 #&gt; [9] 1 #&gt; [10] 1 # リフト値でソート(下位10件) inspect(head(rev(sort(data_ap, by = &quot;lift&quot;)), n = 10)) #&gt; lhs rhs support confidence coverage lift #&gt; [1] {おにぎり, オムツ, パン} =&gt; {ビール} 0.1428571 1 0.1428571 1.4 #&gt; [2] {オムツ, パン} =&gt; {ビール} 0.1428571 1 0.1428571 1.4 #&gt; [3] {おにぎり, パン} =&gt; {ビール} 0.1428571 1 0.1428571 1.4 #&gt; [4] {おにぎり, オムツ} =&gt; {ビール} 0.1428571 1 0.1428571 1.4 #&gt; [5] {おでん, チーズ} =&gt; {ビール} 0.1428571 1 0.1428571 1.4 #&gt; [6] {オムツ, タバコ} =&gt; {ビール} 0.1428571 1 0.1428571 1.4 #&gt; [7] {ハム, パン} =&gt; {ビール} 0.1428571 1 0.1428571 1.4 #&gt; [8] {オムツ} =&gt; {ビール} 0.2857143 1 0.2857143 1.4 #&gt; [9] {タバコ} =&gt; {ビール} 0.2857143 1 0.2857143 1.4 #&gt; [10] {ハム} =&gt; {ビール} 0.1428571 1 0.1428571 1.4 #&gt; count #&gt; [1] 1 #&gt; [2] 1 #&gt; [3] 1 #&gt; [4] 1 #&gt; [5] 1 #&gt; [6] 1 #&gt; [7] 1 #&gt; [8] 2 #&gt; [9] 2 #&gt; [10] 1 リフト値 (or 確信度) 下位10件を調べる意味は? "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
