[["index.html", "ビジネス統計 1 はじめに 1.1 コースの目的 1.2 統計学/統計的手法の学習について 1.3 ビジネス応用における統計学の最近の趨勢 1.4 本書内の記載の注意点", " ビジネス統計 林 高樹 2025-09-28 (内容は随時更新されます) 1 はじめに 1.1 コースの目的 近年の”データサイエンス”分野の発展の中で, その支柱の分野の一つとしての 統計学の重要性が高まっている. いまや, 簡単なデータ分析であれば, ChatGPTなどの生成AIが, 人間に代わって分析を行ったり, 分析のためのプログラムを書いたりしてくれる時代となった. しかしながら, 少なくとも現状の技術水準では生成AIの出力の 正確性は保証されておらず, 生成AIの出力の正しさを確認できるのは人間である. 当面は, 人間の手による分析, 人間の頭による分析方法論の正しい理解や結果の解釈が求められるだろう. AIブームより前に始まっていた”ビッグデータ”の時代において, 多種多様かつ大量の, 組織内・外のデータを分析する技術は 会社経営において決定的に重要となっている. 一方, 統計学やデータサイエンスの一人の初学者としては, いきなり 高度なITスキルを前提とする最新の機械学習系手法を駆使したビッグデータ“解析を行おうとするのでなく, まずは, 古典的な統計的方法論を正しく“スモール・データセット”に応用できるようになることが重要である. 本コースは, 多変量解析を中心にさまざまな統計的データ分析の手法を学び, これらの手法を学術研究や実務に応用できるようになるための基盤作り目指す. 統計ソフトウェアRを利用しながら学んでいく. 1.2 統計学/統計的手法の学習について 学習の目標 統計学/統計的手法は, サイエンスとアートの二つの側面があることから, 的確な応用を行うためには, 両面を同時に, バランスよく学ぶ必要がある. 統計理論を正確に理解する 数学的に正しい概念や手続きの理解 統計手法の実践 (運用法) を学ぶ 業務経験や知識, 統計的手法の経験則, 費用対効果等の判断 ※ 統計学は数学(の一分野)ではない (数学を道具として学問体系が作られいる) 一方の理解が不十分であると, 妥当な分析が行えず, 不正確あるいは間違った分析結果や解釈につながるリスクがある. 統計学/統計的手法の学習方法 3つの要素・ルート, それに対応した教科書・参考書がある. すなわち, 入門 (文章&amp;図表主体): 手法の概念, 用途, 特徴の大雑把な理解を図る 理論 (数式主体): 手法の理論的・技術的側面, 詳細の正確な理解を図る 実習 (コード主体): 手を動かすことで手法を経験, 実践力をつける 統計学を学習するにあたっては, 理解の段階に応じて, これらの要素を, 少しずつ万遍なく学びながら, “スパイラル”状に次の段階に進んでいくのが最も効果的であると筆者は考える. すなわち, 理論の学習を全くやらず, 入門と実習のみを学習するようなアプローチは, 理論的理解のないまま統計的分析を実践することになるため危険である. 筆者の経験上, プログラミングの得意な”エンジニア系”のデータサイエンティストにはそのような傾向を持つ人が少なからずいると感じている. 書籍ごとに目的や想定する読者層は異なり, それに対応するようにこれらの要素の割合が異なる. したがって, 学習者は自身の学習目的に照らして適切な本を選択する必要がある. 本書では, Rを用いながら代表的な統計手法を学んでいく. 統計学の教科書例 Rコードによる分析例を示しながら, 各手法や理論の解説を行っている書籍は 多数存在するが, バランス良くこれらを配置していると筆者が感じる教科書のタイトルを幾つか紹介する. 【統計学/R】 山田剛史, 杉澤武俊, 村井潤一郎 (2008), Rによるやさしい統計学, オーム社 【データ分析/R】 Kosuke Imai (2017), Quantitative Social Science: An Introduction, Princeton University Press (今井耕介(著), 粕谷祐子, 原田勝孝, 久保浩樹 (訳) (2018), 社会科学のためのデータ分析入門(上)(下), 岩波書店) 【機械学習/R】 R. James, G., Witten, D., Hastie, T., Tibshirani (2013), An Introduction to Statistical Learning: with Applications in R, Wiley. (James他(著), 落海浩, 首藤信通 (訳) (2018), Rによる統計的学習入門, 朝倉書店) 参考として, 次の書籍は, コードを載せずまた数式を使った説明も殆どなしに, 文章主体で (計量経済学の) 手法の概念や分析結果の解釈の仕方を平易に説明している良書である. 山本 勲 (2015), 実証分析のための計量経済学, 中央経済社 統計学/統計的手法の学習ステップ 入門・初級ステップ - レベル①: ソフトウェアを正しく動かせる - 目的に応じた適切な手法の選択 - 適切なデータの加工, ソフトウェアの操作 - 出力結果 (帳票, 図表) の正しい見方 - レベル②: 手法の背後にある理論を理解する - (②A) 概念や定義の正しい理解 (言葉やイメージ) - (②B) 数式による厳密な理解 ※ ①の達成度を高めるためには, ②の理解を高める必要 ① ⇒ ② ⇔ ① 中級ステップ - 特定の分野における (計量経済学, 心理学, 疫学, …) 統計的手法の理解と実践が出来るようになる 1.3 ビジネス応用における統計学の最近の趨勢 統計学, さらには中核分野として内包するデータサイエンス分野において扱う 対象データの特徴として以下のような傾向がみられる 大規模化 (“ビッグデータ”) レコード数 n → 大, 変数の数 p → 大 データ数より説明変数が多い場合も （“n&lt;p問題”) 従来の統計学: 「n 小・中規模」, かつ, 「n&gt;p」 高頻度・高速化 (従来) 四半期・月次… → 1日内, 秒, ミリ秒, …, リアルタイム 非構造化 画像, 音声, テキスト等 自動化 衛星画像, アクセスログ, IoTデータ, ウェアラブル・データ, … “マルチモーダル”化 テキスト・画像・音声・動画など複数の種類のデータを一括して処理 (AIによる)自動生成 一方, 経営(学)分野への応用の観点では次のような傾向がある. 文章や発言内容の自然言語処理・テキスト解析技術の重要性の高まり BERT, GPT-4, … 生成系AI技術の活用 文章 (ChatGPT, BARD), 画像 (DALLE, Stable Solution), 音楽 (Stable Audio, Suno), 等 複数のデータソースの有機的な組合せ活用の重要性 財務諸表等の“ハードデータ” × SNS等から得られた“ソフトデータ” 外部ソース・データ ×社内業務データ … 1.4 本書内の記載の注意点 読者への注) パス名は、各自のPC環境に応じて適宜変更すること "],["r言語の基本.html", "2 R言語の基本 2.1 Rの基本プログラミング 2.2 データの型や構造 2.3 データの操作・演算 2.4 R関数 2.5 データの可視化 2.6 ファイル入出力 2.7 パッケージtidyverse", " 2 R言語の基本 2.1 Rの基本プログラミング 主な参考文献： 金 (2017),『Rによるデータサイエンス』, 森北出版. 山田他 (2008),『Rによるやさしい統計学』, オーム社. Venables, Smith, and R Development Core Team (03), R入門. http://minato.sip21c.org/swtips/R-jp-docs/R-intro-170.jp.pdf R Core Team (2024). R Language Definition, R Foundation for Statistical Computing. https://cran.r-project.org/doc/manuals/r-release/R-lang.pdf 本コースは, 基本的なRプログラミングにもっぱら限定 よりモダンなプログラミング (本コース終了後) → tidyverse 例. 松村他, 『RユーザーのためのRStudio[実践]入門]』, 技術評論社 Rコーディングスタイルの例 Google, “Google’s R Style Guide”, https://google.github.io/styleguide/Rguide.html Hadley Wickham, “Tidyverse Style Guide”, https://style.tidyverse.org/ 2.1.1 基本操作 数値 (ベクトル), 演算の直接評価 2 + 3 ## [1] 5 c(1, 2, 3, 4) ## [1] 1 2 3 4 1:4 ## [1] 1 2 3 4 変数xに値を格納. 変数xに対する演算 - 基本形: 変数名 &lt;- 代入する値 x &lt;- c(1, 2, 3, 4, 5) x = c(1, 2, 3, 4, 5) x ## [1] 1 2 3 4 5 (x &lt;- c(1, 2, 3, 4, 5)) # 代入と表示を同時に実行 ## [1] 1 2 3 4 5 x^2 ## [1] 1 4 9 16 25 x**2 ## [1] 1 4 9 16 25 xに関数を適用 mean(x) ## [1] 3 var(x) ## [1] 2.5 sd(x) ## [1] 1.581139 summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1 2 3 3 4 5 # sqrt, summary, ... その他, R言語の基本 - 空白は無視される - Pythonと異なり, インデントは意味を持たない - 一行に二つのコマンドを入力する場合は, 間をセミコロン (;) で区切る - 行頭がシャープ (#) で始まる行は丸々無視される (コメント行) - 中括弧 {} は通常、コードのブロックを作成するために使用. 主に, 条件文、ループ、関数などのブロック構造を定義する際に使用. 2.1.2 基本構文 Rでは, データの加工や分析を行う際などに, 分析者自らの手で処理の手順をプログラミングをすることができる. forループ # 繰り返し処理 (forループ) for (変数名 in 変数のリスト){ 1回分の処理内容 } ``` #### if文 {-} ``` # 条件分岐 (if文) # if (条件式) 処理1 else 処理2 x &lt;- 0 for(i in 1:10){x &lt;- x + i} x # aaa &lt;- c(1, 3, 5) for (a in aaa) print (a) ## [1] 55 ## [1] 1 ## [1] 3 ## [1] 5 2.1.3 自作関数 同様な処理を”パラメータ”を変えながら何度も実行する場合は, 関数を作っておくと便利である. ※ 関数に付与する名前として, Rですでに使われている関数名や, Rで特別な意味を持つ値 (T, Fなど)は避けること # 自作関数の作成 関数名 &lt;- function(引数1, 引数2, ...){ 処理内容 } myfunc &lt;- function(y){ x &lt;- 0 for (i in 1:y) x &lt;- x + i return(x) } # 実行例 myfunc(10) myfunc2 &lt;- function(y){ if(y &gt; 10) print(&quot;yes&quot;) else print(&quot;no&quot;) } # 実行例 myfunc2(5) ## [1] 55 ## [1] &quot;no&quot; 2.1.4 パッケージのインストール &amp; 読み込み #lda # lda関数 → このままだエラー発生 library() # インストール済パッケージ一覧 search() # 読み込み済みパッケージ一覧 library(MASS) # MASSパッケージの読み込み(ロード) search() # アタッチされたパッケージのリスト表示 lda install.packages(&quot;DAAG&quot;) # http://cran.r-project.org # http://cran.r-project.org/web/packages/googleVis/index.html # パッケージインストローラー 2.1.5 ヘルプ 関数のヘルプ R関数helpを使うか, RStudioのプルダウンメニューやHelpペインを使用 help(&quot;fivenum&quot;) # 関数fivenumのヘルプ ?fivenum 2.2 データの型や構造 ここで, R言語の基礎を理解するのに重要な二つの概念について, 初心者を念頭に正確性を犠牲にしながら概要について述べる. 実際はここでの記載よりもはるかに複雑で, 技術的にも難易度が高い. 包括的かつ技術的に正確な内容については, 例えば, https://adv-r.hadley.nz を参照されたい. 2.2.1 データの値の種類 (“データ型”) Rでは, データの取る値の主要な種類 (type) として, 実数型 (double), 整数型 (integer), 文字列型 (character), 論理型 (logical) がある. また, 実数型, 整数型はまとめて数値型 (numeric) とも呼ばれる. 初心者は, 実数型と整数型の違いは気にしなくても良い. # 実数型 3.14 2.718 # 整数型 1L 5L # 文字列型 &quot;KBS&quot; &quot;日吉&quot; # 論理型 TRUE # または, T FALSE # または, F 次に, データの値の種類として, 上記以外に応用上知っておきたいものとして, 因子型 (factor),日付型 (Date)がある. 因子型は, カテゴリーデータに対して, 日付型は日付や時刻を表すデータに対して使うことができる. Rでは, カテゴリーデータ (ベクトル) を因子型としてオブジェクトに格納しておけば, その後の統計分析においてわざわざダミー変数を作る操作は (おおむね) 不要となる. また, 日付型として格納したデータは日付や時間に関する処理において効果を発揮する. 与えられたデータに対して, R組み込み関数である factor(), as.Date() を適用することでこれらの型に変換することができる. 少しだけ発展的な内容になるが, 因子型は整数型を値に持つベクトル, 日付型は実数型を値に持つベクトルとしてR内部で扱われる (ベクトルやオブジェクトについては次に述べる). # 因子型 factor(c(&quot;L&quot;, &quot;M&quot;, &quot;H&quot;, &quot;M&quot;, &quot;L&quot;, &quot;M&quot;)) # L/M/Hの3水準の因子型ベクトル (長さ5) # 日付型 as.Date(&quot;2023-10-02&quot;) 2.2.2 データの配列の仕方 (“データ構造”) Rでは, データの配置の仕方の種類の主要なものとして, ベクトル (vector), リスト (list), 行列 (matrix), 配列(array), データフレーム (data frame) などがある. 分析に応じて, 適切なデータの構造にして処理を行う必要がある. # ベクトル c(3.14, 2.718) c(&quot;KBS&quot;, &quot;日吉&quot;) # リスト list(&quot;KBS&quot;, 1962L, 1:10) # 行列 matrix(1:8, nrow = 2, byrow = T) # 配列 arra(1:12, c(2, 3, 2)) # データフレーム data.frame(name = c(&quot;Steve&quot;, &quot;Top&quot;), income = c(40000, 50000)) ちなみに, これらの”データ構造”には階層関係があり, 行列や配列はベクトルの特別な場合, リストはベクトルの特別な場合, データフレームはリストの特別な場合である. データフレームは, リスト (異なる種類のデータを同時に要素として持つ) でありながら, リストの各要素 (ベクトル) の長さが等しく, 2次元の行列の形式にデータが並べられたものである. R言語では, ベクトルが最も基本的な”データ構造”である. Rを用いた統計分析では, データフレームを用いるケースが非常に多いため, データフレームを使えるようになることが必須である. Rで分析を行う場合には, データや関数 (データ処理するための手続きを書いたコード) をオブジェクト (object) と呼ばれる”箱”に名前を付けて一旦格納し, その名前を呼び出す形で処理を実行するのが便利である. 量的変数や質的変数を同時に持つデータセットの分析には, データフレームが便利である. オブジェクトにはクラス (class) というオブジェクトの持つデータ構造の種類の属性が付与される. なお, Rには, type, class, modeの3つの”型”が存在し混乱しやすい. 初心者は違いを気にする必要はなく, 大雑把に, 上の”データ型”は関数 typeof(), “データ構造”は関数 class() により調べることができると知っていれば十分である. 興味のある読者は以下を参照: https://cran.r-project.org/doc/manuals/r-release/R-lang.html#Attributes 特別な値 Rにおける分析において注意や対処が必要な, データの取り得る特別な値として, - NA (欠損値) - NULL (非存在) - NaN (非数値) - Inf (無限大) これらの値をテストする関数が用意されている. is.na() is.null() is.nan() is.infinite() is.finite() # NAの含まれている例 x &lt;- c(1, NA, 3, 4, 5) x == NA ## [1] NA NA NA NA NA is.na(x) ## [1] FALSE TRUE FALSE FALSE FALSE mean(x) ## [1] NA mean(x, na.rm = T) ## [1] 3.25 # NULLの含まれている例 x &lt;- 1:5 names(x) &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;) x ## a b c d e ## 1 2 3 4 5 names(x) &lt;- NULL x ## [1] 1 2 3 4 5 # NaN, Infの発生例 0 / 0 ## [1] NaN 1 / 0 ## [1] Inf 上の”データ型”や”データ構造”を調べる関数も用意されている. # 整数値を持つ行列の例 abc &lt;- matrix(1:8, nrow = 2, byrow = T) is.numeric(abc) ## [1] TRUE is.integer(abc) ## [1] TRUE is.matrix(abc) ## [1] TRUE typeof(abc) ## [1] &quot;integer&quot; class(abc) ## [1] &quot;matrix&quot; &quot;array&quot; mode(abc) ## [1] &quot;numeric&quot; str(abc) ## int [1:2, 1:4] 1 5 2 6 3 7 4 8 2.2.3 ベクトル # 以下は, 互いに等価 aaa &lt;- c(2, 4, 6, 8) # 変数aaaに数値ベクトル(2,4,6,8)を割り当てる aaa = c(2, 4, 6, 8) aaa = seq(2, 8, 2) c(2, 4, 6, 8) -&gt; aaa aaa &lt;- 1:4 * 2 # assign(&quot;aaa&quot;, c(2, 4, 6, 8)) # 値を割り当てる際に, &quot;環境&quot;を指定することができる # ベクトルの長さ length(aaa) # 文字列ベクトル bbb &lt;- c(&quot;東京&quot;, &quot;埼玉&quot;, &quot;千葉&quot;, &quot;神奈川&quot;) # ベクトルの各要素に名前 (ラベル) を付与 names(aaa) &lt;- bbb # ベクトル要素の取り出し bbb[1] bbb[c(2, 4)] bbb[c(T, F, T, F)] # インデックスの値がT (TRUE) の要素の取り出し bbb[c(T, F, F)] # 注意 ## [1] 4 ## [1] &quot;東京&quot; ## [1] &quot;埼玉&quot; &quot;神奈川&quot; ## [1] &quot;東京&quot; &quot;千葉&quot; ## [1] &quot;東京&quot; &quot;神奈川&quot; 2.2.4 行列 matrix(0, 3, 4) # 全要素0の3x4-行列 matrix(0:4, 3, 4) # 行列の値に使うベクトル (0:4) の長さと 行数 (3)・列数 (4) が不一致 ## Warning in matrix(0:4, 3, 4): data length [5] is not a sub-multiple or multiple ## of the number of rows [3] ## [,1] [,2] [,3] [,4] ## [1,] 0 0 0 0 ## [2,] 0 0 0 0 ## [3,] 0 0 0 0 ## [,1] [,2] [,3] [,4] ## [1,] 0 3 1 4 ## [2,] 1 4 2 0 ## [3,] 2 0 3 1 ccc &lt;- matrix(c(3, 2, 1, 6, 5, 4), 2, 3) # 2x3-行列 ccc[1, 1] # (1, 1)成分 ## [1] 3 ccc[1, ] # 第1行(行ベクトル) ## [1] 3 1 5 ccc[, 1] # 第1列(列ベクトル) ## [1] 3 2 ccc[-2, ] # 2行目を除く → 2x4-行列 ## [1] 3 1 5 ccc[, -2] # 2列目を除く → 3x3-行列 ## [,1] [,2] ## [1,] 3 5 ## [2,] 2 4 dim(ccc); nrow(ccc); ncol(ccc) # セミコロン(;)により, 複数のコマンドを1行に収め, 順次実行 ## [1] 2 3 ## [1] 2 ## [1] 3 ccc[2, 3] &lt;- 10 # (2, 3)成分に値10を代入 # 行列にラベルを付与 colnames(ccc) &lt;- c(&quot;大阪&quot;, &quot;京都&quot;, &quot;名古屋&quot;) # 列ラベル rownames(ccc) &lt;- c(&quot;2012&quot;, &quot;2013&quot;) # 行ラベル ccc ## 大阪 京都 名古屋 ## 2012 3 1 5 ## 2013 2 6 10 t(ccc) # 転置 ## 2012 2013 ## 大阪 3 2 ## 京都 1 6 ## 名古屋 5 10 2.2.5 リスト ベクトル, 行列, 配列, リスト等の異なる型(&amp;異なる長さ)のオブジェクトを一つにまとめたオブジェクト L1 &lt;- list(rep(&quot;A&quot;, 3), 1:0, matrix(1:8, 2, 4)) L1[[1]] # 1番目の要素(変数)の取り出し k &lt;- list (name = &quot;Taro&quot;, salary = 50000, male = T) k2 &lt;- list (&quot;Taro&quot;, 50000, T) # 要素名 (タグ)なしの場合 k$sal # 要素名は省略形可 # リストはベクトルの一種 (recursive vector) # 一方, 通常のベクトルはatomic vector (それ以上分解できない) # vector()からリスト生成する場合 z &lt;- vector (mode = &quot;list&quot;) z[[&quot;abd&quot;]] &lt;- 5 k[1:2] # 元のリストの部分リスト k2 &lt;- k[2] class(k2); str(k2) k2a &lt;- k[[2]] # 2番目の要素(変数)の取り出し (要素の型を持つ結果を返す) # k[[1:2]] # --&gt; エラー class(k2a); str(k2a) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; ## [1] 50000 ## $name ## [1] &quot;Taro&quot; ## ## $salary ## [1] 50000 ## ## [1] &quot;list&quot; ## List of 1 ## $ salary: num 50000 ## [1] &quot;numeric&quot; ## num 50000 リストの要素追加・削除 z &lt;- list(a = &quot;abcd&quot;, b = 10) z$c &lt;- &quot;piano&quot; z[[4]] &lt;- 15 z[5:6] &lt;- c(TRUE, FALSE) z$b &lt;- NULL # xxx &lt;- 1:10 yyy &lt;- 0.5 * xxx + rnorm(10) lm_res &lt;- lm(yyy ~ xxx) is.list(lm_res) lm_res[[1]] lm_res$coef lm_res[&quot;coefficients&quot;] ## [1] TRUE ## (Intercept) xxx ## 0.7173661 0.4806091 ## (Intercept) xxx ## 0.7173661 0.4806091 ## $coefficients ## (Intercept) xxx ## 0.7173661 0.4806091 2.2.6 データフレーム リストの特別な場合 長さが等しい複数のベクトルを要素に持つリスト 数値と文字列などの異なるデータが混在するデータを行列のように扱える Rにおける様々な統計分析において多用される kids &lt;- c(&quot;taro&quot;, &quot;hanako&quot;) ages &lt;- c(10, 8) d &lt;- data.frame(kids, ages, stringsAsFactors = FALSE) # 注: stringsAsFactors = T: 文字ベクトルをfactorとして扱う d str(d) # 以下の3つは等価な操作 d[[1]] # データフレームの第1列 (リストの一番目の要素) を取り出す(→ 文字列ベクトル) d$kids # 変数(kids)のように取り出す d[, 1] # 行列のように操作 (--&gt; 便利) # ただし, d[1] # 第1列をデータフレーム (リスト) として取り出す df1 &lt;- data.frame(letters[1:3], 3:1) rownames(df1) &lt;- c(&quot;大阪&quot;, &quot;京都&quot;, &quot;名古屋&quot;) colnames(df1) &lt;- c(&quot;方言種類&quot;, &quot;順位&quot;) class(df1) is.vector(df1) ## kids ages ## 1 taro 10 ## 2 hanako 8 ## &#39;data.frame&#39;: 2 obs. of 2 variables: ## $ kids: chr &quot;taro&quot; &quot;hanako&quot; ## $ ages: num 10 8 ## [1] &quot;taro&quot; &quot;hanako&quot; ## [1] &quot;taro&quot; &quot;hanako&quot; ## [1] &quot;taro&quot; &quot;hanako&quot; ## kids ## 1 taro ## 2 hanako ## [1] &quot;data.frame&quot; ## [1] FALSE 2.3 データの操作・演算 2.3.1 ベクトルの結合, ソート vec1 = 1:4 vec2 = 2:5 rbind(vec1, vec2) # ベクトルの行方向への結合 cbind(vec1, vec2) # べクトルの列方向への結合 vec3 &lt;- c(2, 5, 1, 3) sort(vec3) # 昇順 rev(vec3) # 順番を逆転させる ccc[, order(ccc[&quot;2012&quot;, ])] ccc[, sort.list(ccc[&quot;2012&quot;, ])] ## [,1] [,2] [,3] [,4] ## vec1 1 2 3 4 ## vec2 2 3 4 5 ## vec1 vec2 ## [1,] 1 2 ## [2,] 2 3 ## [3,] 3 4 ## [4,] 4 5 ## [1] 1 2 3 5 ## [1] 3 1 5 2 ## 京都 大阪 名古屋 ## 2012 1 3 5 ## 2013 6 2 10 ## 京都 大阪 名古屋 ## 2012 1 3 5 ## 2013 6 2 10 2.3.2 二項演算 x &lt;- c(1, 3, 5, 2); y &lt;- c(-3, 1, -1, -2) x + y x * y x / y x ^ 2 x &lt; y ## [1] -2 4 4 0 ## [1] -3 3 -5 -4 ## [1] -0.3333333 3.0000000 -5.0000000 -1.0000000 ## [1] 1 9 25 4 ## [1] FALSE FALSE FALSE FALSE 2.3.3 論理演算 lx &lt;- c(T, T, F); ly &lt;- c(F, F, F) lx &amp; ly lx &amp;&amp; ly # 最初の要素間の論理演算が成り立つと, 以降の演算は行わない lx | ly lx || ly # 最初の要素間の論理演算が成り立つと, 以降の演算は行わない 2.3.4 条件式 # ==, &gt;, &lt;, &gt;=, &lt;= # &amp;&amp;, || 2.3.5 行列演算 A &lt;- matrix(c(1, 2, 3, 4, 5, 6), 3, 2) B &lt;- matrix(c(2, 1, -1, -2), 2, 2) A %*% B # 行列の積 # diag # 対角行列 # solve # 逆行列 ## [,1] [,2] ## [1,] 6 -9 ## [2,] 9 -12 ## [3,] 12 -15 2.4 R関数 2.4.1 数学基本関数 # sum; sqrt; abs # exp; log; log10; log2; sin; cos # round; ceiling; floor 2.4.2 基本統計量の計算 # mean, max; min; range; median; quantile # var; sd # summary # table # cov; cor 統計 &lt;- c(rep(&quot;好き&quot;, 8), rep(&quot;嫌い&quot;, 7)) 数学 &lt;- c(rep(&quot;好き&quot;, 6), rep(&quot;嫌い&quot;, 9)) table(統計, 数学) # クロス集計表, ベクトルは同一長 ## 数学 ## 統計 好き 嫌い ## 好き 6 2 ## 嫌い 0 7 2.4.3 確率分布 # dxxx(q) # 確率密度, q:確率点 # pxxx(q) # 累積確率, q:確率点 # qxxx(p) # 確率点, p:確率 # rxxx(n) # 乱数, n:個数 # ------------------------------------------------------------------ # xxx部分: # unif(x, min, max) # 一様分布 # norm(x, mean, sd) # 正規分布 # exp(x, rate) # 指数分布 # binom(x, size, prob) # ２項分布 # pois(x, lambda) # ポアソン分布 # t(x, df) # t分布 # chisq(x, df) # カイ2乗分布 # f(x, df1, df2) # F分布 curve(xを含んだ式, from = xの左端点, to = xの右端点) # 関数のグラフ描画 curve(dnorm(x, mean = 0, sd = 1), from = -4, to = 4) curve(dnorm(x, mean = 1, sd = 2), from = -4, to = 4, add = T) # 問：t分布(自由度4)の形状は? 2.4.4 その他便利な関数 # sweep # scale # ifelse ifelse(統計 == &quot;好き&quot;, 1, 0) # apply (X, MARGIN, FUN, ...) apply(ccc, 1, sum) apply(ccc, 2, sum) colMeans(ccc) rowMeans(ccc) ## [1] 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 ## 2012 2013 ## 9 18 ## 大阪 京都 名古屋 ## 5 7 15 ## 大阪 京都 名古屋 ## 2.5 3.5 7.5 ## 2012 2013 ## 3 6 2.5 データの可視化 # 棒グラフ barplot(ccc) barplot(ccc, beside = T) barplot(ccc, beside = T, col = c(&quot;lightblue&quot;, &quot;lavender&quot;), main = &quot;test&quot;) # apply(ccc, 1, pie) # pie # hist # 折れ線グラフ (行列の各列(変数)の同時プロット) matplot(ccc, type = &quot;l&quot;) matplot(t(ccc), type = &quot;l&quot;) # 箱ひげ図 boxplot(ccc) boxplot(t(ccc)) # 散布図 # plot pairs(ccc) #install.packages(&quot;car&quot;) #library(car) #scatterplot(ccc) # install.packages(&quot;scatterplot3d&quot;); library(scatterplot3d) # scatterplot3d # その他のグラフ # coplot; mosaic plot; stars; faces; persp; image; contour # その他 # windows() #新しいグラフィック・ウィンドウを開く # par(mfrow = c(2, 2)) # より洗練されたグラフ. やや難易度が高いがモダンなアプローチ # install.packages(&quot;ggplot2&quot;, dependencies = T) library(ggplot2) ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Petal.Length)) + geom_point(aes(colour = Species)) + geom_smooth(method = &quot;lm&quot;, colour = &quot;lightblue&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; 2.6 ファイル入出力 2.6.1 テキストファイル読み込み ファイル読み込み用のR関数には, ファイルの格納場所 (パス) とファイル名を知らせる必要がある パスを省略すると, 現在のディレクトリ (getwd関数で確認可能) 下でファイルを探す. もし, 存在しなければ, エラーとなる ここでは, あらかじめ, 各自のPCのデスクトップ上に, “BStat_2025”という名前のフォルダ (ディレクトリ) を作成していると想定 ファイルは, カンマ(, )で区切られたcsv形式や, タブで区切られたtsv形式の テキストファイルであるとする. data1 &lt;- read.table(&quot;/[パス名]/ファイル名&quot;, header = T, row.names = 1) # オプション # header = T: 1行目が列ラベル # row.names = 1: 1列目が行ラベル または, file.path関数を使ってファイルの格納されているパス(経路)を指定しても良い. # ユーザー(yamada)が, デスクトップ(Desktop)フォルダの下に授業用フォルダ(BStat_2025)を作成した場合のパスの指定 fpath &lt;- file.path(&quot;~&quot;, &quot;Desktop&quot;, &quot;BStat_2025&quot;) # または fpath &lt;- file.path(&quot;Users&quot;, &quot;yamada&quot;, &quot;Desktop&quot;, &quot;BStat_2025&quot;) # ifile &lt;- file.path(fpath, &quot;) # 例えば, # data1 &lt;- read.table(&quot;/Users/[アカウント名]/Desktop/BStat_2025/data.txt&quot;, sep = &quot;, &quot;) # または # data1 &lt;- read.csv(&quot;/Users/[アカウント名]/Desktop/BStat_2025/data.txt&quot;, header = T, row.names = 1) # 代替的に # data2 &lt;- scan(&quot;/Users/[アカウント名]/Desktop/BStat_2025/data.txt&quot;, sep = &quot;, &quot;) #matrix(scan(&quot;/Users/[アカウント名]/Desktop/BStat_2025/data.txt&quot;, sep = &quot;, &quot;), 3, 4, byrow = T) # matrix(scan(&quot;/Users/[アカウント名]/Desktop/BStat_2025/data.txt&quot;, sep = &quot;, &quot;), 3, 4) # scan()において, 数値, 文字が混在している場合, 列ごとにデータ属性を指定する必要 # data2.txt # a 1 2 # b 2 3 # c 3 4 # data3 &lt;- scan(&quot;/Users/[アカウント名]/Desktop/BStat_2025/data.txt&quot;, sep = &quot;, &quot;, list(x = &quot;&quot;, y = 0, z = 0)) # data.frame(data2) # データフレーム化 # パスを指定せずに, テキストファイルの置かれているフォルダに移動してから # ファイル名のみを使って読み込んでも良い # setwd(&quot;/Users/[アカウント名]/Desktop/BStat_2025&quot;) data1 &lt;- read.table(&quot;data.txt&quot;, sep = &quot;, &quot;) # パッケージ&quot;foreign&quot;により, SAS, SPSS等のファイル形式のデータの読み込みが可能 2.6.2 テキストファイル書き出し write(data3, &quot;/[パス名]/ファイル名&quot;) # 例えば, # write.table(data3, &quot;/Users/[アカウント名]/Desktop/BStat_2025/data3_out1.txt&quot;) # write.table(data3, &quot;/Users/[アカウント名]/Desktop/BStat_2025/data3_out1.txt&quot;, append = T) # write.csv() # sink(&quot;/Users/[アカウント名]/Desktop/BStat_2025/data3_out1.txt&quot;) data1; data2 sink() 2.7 パッケージtidyverse tidyverseは, Hadley Wickhamによって開発が進められているRパッケージ (群) である. データのインポート, 整理, 加工, 可視化, 分析を簡単かつ効率的に行うための一連のツールを提供する. tidyverseの中核をなすパッケージには以下のものがある: ggplot2: データの可視化を行うためのパッケージ. レイヤーの概念を用い, データポイント, 統計的変換, スケール, 軸, 凡例など, グラフの各要素を個別に定義し, 組み合わせることができる. これにより, 高度にカスタマイズされたグラフを容易に作成可能. dplyr: データの操作と変形を行うためのパッケージ. フィルタリング, 並べ替え, 集約など, データフレームに対する一般的な操作を簡単かつ直感的に行うための関数を提供. tidyr: データの整理と整形を簡単にするためのパッケージ. データセットのレイアウトを整形する等のクリーニングのタスクに対応しながら,「tidy形式」としてデータを再構築するツールを提供. 例えば, データを多数の列に広げる「wide形式」と データをより少ない列にまとめるが行を増やす「long形式」間の変換, 欠損値への適切な対処, 一列を複数列に分割あるいは複数列を一列に結合する等の処理. readr: さまざまな形式のテキストデータ (例えば, csv, tsv形式) を読み込み, Rのデータフレームとして効率的にインポートするためのパッケージ. 標準のR関数よりも高速に動作し, ファイルの読み込み時によくある問題 (データ型の自動認識, 欠損値の扱い等) をより柔軟に処理. さらに, 便利な機能を持つパッケージとして, purrr: リストと関数型プログラミングを扱うためのパッケージ. リストの操作, 要素の繰り返し処理, 条件に基づく要素の抽出など, 複雑なデータ構造の操作を簡単にする関数を提供. tibble: データフレームをより現代的かつ柔軟に扱うためのパッケージで, 印刷時の見やすさ, 列名の非標準的な文字の扱い, サブセット操作の改善など, データフレームを強化し使いやすさを改善. stringr: 文字列データの操作を行うためのパッケージ. Rの標準文字列操作機能よりも一貫性と可読性に優れたインターフェースを提供し、文字列の検索, 置換, 分割, 結合などのタスクを簡単に行うことが可能. forcats: 因子 (カテゴリカルデータ) を扱うためのパッケージ. 因子水準の順序変更, 要約, 結合, 分離など, 因子型のデータを操作するための便利な関数を提供. lubridate: 日付と時刻のデータを扱うためのパッケージ. 日付や時刻の加算・減算, 部分的な抽出, 時間差の計算など, 操作を直感的かつ効率的にするための関数を提供. tidyverseは, データを「tidy」（整然とした）形式で扱うことに焦点を当てている. tidyデータの原則では, 各変数が列に, 各観測値が行に, 各種類の観測単位がテーブルに配置される. この原則に従うことで, データ分析がより直感的で効率的になる. tidyverseパッケージは, Rでのデータ分析作業を容易にし, コードをより読みやすく, 書きやすくすることを指向している. それぞれのパッケージは単独で使用することも出来るが, 一緒に使用することでより使い勝手が向上し便利である. データサイエンスにおける日常的なタスクを簡潔に, かつ効率的に行うための強力なツールセットと言える. tidyverseのホームページ https://tidyverse.tidyverse.org/ 2.7.1 Q: Rの初心者はtidyverseから勉強することは可能か? 今日では, R言語の基本を学ばずにいきなりtidyverseから勉強することは, tidyverseを入口としてR言語の学習を始めるユーザーも多いと思われる. 特にデータ分析やデータサイエンスに焦点を当てている初心者にとっては代替的な選択肢である. tidyverseは, データの取り扱いを直感的かつ効率的にすることを目的として設計されており, その構文は初心者にとって学びやすいように工夫されている. tidyverseの利点: 直感的な構文: tidyverseの関数は覚えやすく, 理解しやすい構文を持っているため, R言語の初心者でも扱いやすい. データ分析のワークフローを強化: tidyverseはデータのインポート, 整理, 加工, 可視化, 分析という一連のデータ分析プロセスに対応するツールを提供する. これにより, データ分析の基本的な流れを簡単に学ぶことができる. 広範なコミュニティとサポート: tidyverseはRユーザー内に多くの熱狂的なファンがいて, コミュニティを形成している. オンラインでのサポートや学習リソースが豊富にある. 注意点: R言語の基本概念の理解の必要性: R言語の基本的な概念 (変数の割り当て, 関数の使用方法, データ型など) は、tidyverseを効率的に使用するためにも理解しておく必要あり. 限定的な機能: tidyverseだけではカバーできないR言語の機能も多くある. すなわち, tidyverseから学習を始めても, いずれはR言語のより広範な機能やパッケージにも目を向けることが必要. tidyverseに含まれるパッケージを利用すると, 確かに多くの複雑な処理が簡潔かつエレガントに書けたりすることがあり, その機能性を実感することも多い. しかし, Rプログラミングを行っている中で, R言語の基本を知らないと困るようなことの方が多い. よって, 筆者の考えでは, Rの初学者はいきなりtidyverse系を軸に学習を開始するよりは, tidyverseに含まれるパッケージはあくまでRの多数あるパッケージの一部であると位置付けて, 標準的なRを学びながらこれらのパッケージの用法を学ぶというスタンスで良い. 本コースは, tidyverse系の扱いはこのような方針に従って進めるものとする. 2.7.2 tidyverseの基本 サンプルコードの出所: ChatGPT (GPT-4) 以下, tidyverseの基本を理解するために, データのインポート, 加工, そして可視化のステップを含むシンプルなチュートリアルを紹介する. ここでは, tidyverseの中のreadr, dplyr, ggplot2の三つのパッケージを使用する. これらはtidyverseの中で最もよく使用されるパッケージである. ステップ 1: tidyverseをインストールして読み込む まず, tidyverseパッケージをインストールし, ライブラリに読み込む. # tidyverseパッケージのインストール # install.packages(&quot;tidyverse&quot;) # ライブラリに読み込む library(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.4 ✔ readr 2.1.5 ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ ggplot2 3.5.1 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.3 ✔ tidyr 1.3.1 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors ステップ 2: データをインポートする tidyverseには様々なサンプルデータが含まれている. ここでは, mtcarsデータセットを使用する. mtcarsは, 1974年のMotor Trend US誌に掲載された32台の自動車に関するデータである. # mtcarsデータセットを使用する data &lt;- mtcars ステップ 3: データを加工する dplyrを使用してデータを加工する. ここでは, mpg（ガロンあたりのマイル数）が20を超える車両のみを選択し、cyl（シリンダー数）ごとの平均mpgを計算する. # dplyrを使ってデータをフィルタリングし、集約する filtered_data &lt;- data %&gt;% filter(mpg &gt; 20) %&gt;% group_by(cyl) %&gt;% summarise(mean_mpg = mean(mpg)) # 結果を表示 print(filtered_data) ## # A tibble: 2 × 2 ## cyl mean_mpg ## &lt;dbl&gt; &lt;dbl&gt; ## 1 4 26.7 ## 2 6 21.1 ステップ 4: データを可視化する 最後に, ggplot2を使ってデータの可視化を行う. ここでは, cylごとのmean_mpgを棒グラフで表示する. # ggplot2を使ってデータを可視化 ggplot(filtered_data, aes(x = factor(cyl), y = mean_mpg)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;blue&quot;) + theme_minimal() + labs(title = &quot;Cylinder-wise Mean MPG&quot;, x = &quot;Number of Cylinders&quot;, y = &quot;Mean MPG&quot;) 関数ggplot()の別の使用例として, マルチパネル化した散布図を示す. # データを可視化する # mpgとhpの関係を示す散布図を作成し、cylごとに異なるパネルに表示する ggplot(data, aes(x = mpg, y = hp)) + geom_point() + facet_wrap(~cyl) + theme_minimal() + theme(panel.background = element_rect(fill = &quot;gray&quot;)) + labs(title = &quot;Scatterplot of MPG vs HP by Cylinder&quot;, x = &quot;Miles Per Gallon (MPG)&quot;, y = &quot;Horsepower (HP)&quot;) "],["仮説検定.html", "3 仮説検定 3.1 平均値の差の検定 3.2 カイ二乗検定 3.3 分析例: 統計テストデータ", " 3 仮説検定 本章では, 仮説検定の中でも, 実務において良く用いられる, 平均値の差の検定 独立性の検定 について説明する. 独立性の検定と共通点のある 適合度検定 についても説明する. 3.1 平均値の差の検定 t検定は, Rで標準的に用意されている関数t.test()を用いて行う. # t.test(x, y = NULL, # alternative = c(&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;), # mu = 0, paired = FALSE, var.equal = FALSE, # conf.level = 0.95, ...) # # # One Sample t-test # Performs one and two sample t-tests on vectors of data. 平均値の差の検定 (ペア検定) ペア検定を実施する場合には, t.test()の引数pairedを真 (TRUE, またはT) に設定する. デフォルトは,paired = FALSE (F) である (ペア検定ではない). ここでは, Rにデフォルトで収録されているデータセットsleepを用いる. データセット: sleep sleep: 睡眠薬の効果を調べる実験データ - 患者10名, 2種類の睡眠薬の比較 (コントロールに対する睡眠時間の増加分) - (extra, group, ID) 20件, 患者(ID) 10名 # help(sleep) # Data which show the effect of two soporific drugs (increase in hours of sleep compared to control) on 10 patients. head(sleep); tail(sleep) #&gt; extra group ID #&gt; 1 0.7 1 1 #&gt; 2 -1.6 1 2 #&gt; 3 -0.2 1 3 #&gt; 4 -1.2 1 4 #&gt; 5 -0.1 1 5 #&gt; 6 3.4 1 6 #&gt; extra group ID #&gt; 15 -0.1 2 5 #&gt; 16 4.4 2 6 #&gt; 17 5.5 2 7 #&gt; 18 1.6 2 8 #&gt; 19 4.6 2 9 #&gt; 20 3.4 2 10 最初に, Rへの指示を簡潔にするために, attach(sleep) を実行する. これにより, sleep内に含まれる変数extra,group,IDについては, それらがsleepの変数であることをその都度教えてなくても, Rは認識できるようになる (Rの“サーチパス”に加わる). attach(sleep) par(mfrow = c(1, 2)) plot(extra); hist(extra) 両側検定, 片側検定は, 引数alternativeの値 (“two.sided”, “less”, “greater”) で指定する. デフォルトはalteranative=\"two.sided\", すなわち, 両側検定である. # t.test(extra) # 注) デフォルトはpaired = F (ペア検定ではない) # 両側検定 t.test(extra[group == 1], extra[group == 2], paired = T) #&gt; #&gt; Paired t-test #&gt; #&gt; data: extra[group == 1] and extra[group == 2] #&gt; t = -4.0621, df = 9, p-value = 0.002833 #&gt; alternative hypothesis: true mean difference is not equal to 0 #&gt; 95 percent confidence interval: #&gt; -2.4598858 -0.7001142 #&gt; sample estimates: #&gt; mean difference #&gt; -1.58 片側検定は, 右側はalternative = \"greater\", 左側はalternative = \"greater\"で指定する. # 片側検定 t.test(extra[group == 1], extra[group == 2], paired = T, alternative = &quot;greater&quot;) # 片側 (右側) 検定) #&gt; #&gt; Paired t-test #&gt; #&gt; data: extra[group == 1] and extra[group == 2] #&gt; t = -4.0621, df = 9, p-value = 0.9986 #&gt; alternative hypothesis: true mean difference is greater than 0 #&gt; 95 percent confidence interval: #&gt; -2.293005 Inf #&gt; sample estimates: #&gt; mean difference #&gt; -1.58 t.test(extra[group == 1], extra[group == 2], paired = T, alternative = &quot;less&quot;) # 片側 (左側) 検定) #&gt; #&gt; Paired t-test #&gt; #&gt; data: extra[group == 1] and extra[group == 2] #&gt; t = -4.0621, df = 9, p-value = 0.001416 #&gt; alternative hypothesis: true mean difference is less than 0 #&gt; 95 percent confidence interval: #&gt; -Inf -0.8669947 #&gt; sample estimates: #&gt; mean difference #&gt; -1.58 boxplot(sleep) # bad example #boxplot(extra) boxplot(extra ~ group) par(mfrow = c(1, 1)) 平均値の差に関するt検定は, 二つの標本 \\((x_1,x_2,...,x_{n_1})\\), \\((y_1,y_2,...,y_{n_2})\\) の差\\(\\mu_1-\\mu_2\\)がゼロか否かを評価する統計的な手続きである (ここで, 未知の真の平均はそれぞれ, \\(\\mu_1\\), \\(\\mu_2\\)と表記する). ペア検定は, さらに, この2標本のサイズが等しく (\\(n_1=n_2\\equiv n\\)), しかも, 各々のデータ点がペア \\((x_i,y_i)\\) として扱える (“対応がある”) ような特別な場合である. 対応のあるケースの例としては, 同一の企業からデータを2種類, あるいは2時点について採取した場合である. このようなペアのケースにおける平均値の差の検定は, 2標本のまま扱うのではなく, 各データ点ペアの差 \\((x_i-y_i)\\) を取ることで1標本に集約した上で, 1標本に対する平均値ゼロのt検定として行う. すなわち, 帰無仮説\\(H_0: \\mu_1-\\mu_2=0\\)に対して, 対立仮説は, 両側検定: \\(H_1: \\mu_1-\\mu_2 \\neq 0\\) 片側 (右側) 検定: \\(H_1: \\mu_1-\\mu_2&gt;0\\) 片側 (左側) 検定: \\(H_1: \\mu_1-\\mu_2&lt;0\\) である. 上のsleepデータセットのケースにおいては, 標本平均の差 (\\(\\bar{x}_1-\\bar{x}_2\\))の大きさが\\(-1.58\\)であり, 対応する分散の大きさ (上の結果では表示されていない) に比べて十分に小さい (帰無仮説であるゼロ からマイナス方向に遠く離れている). その結果, 両側検定では, p値は0.002833となり, 1%有意水準 (\\(\\alpha=0.01\\)) でも帰無仮説は棄却される こととなった. 一方, 対立仮説として, 右側 \\(H_1: \\mu_1-\\mu_2&gt;0\\) を採用した場合には, 標本から計算される平均値の差は正の値となることが期待され (\\(\\bar{x}_1-\\bar{x}_2&gt;0\\)), これは, データセットsleepから計算された値 (\\(-1.58\\)) とは明らかに整合的ではない. このことは, 対応するp値が\\(0.9986\\)と 1に近く, 帰無仮説を棄却できない大きさとなっていることに表れている. 平均値の差の検定 ペアを構成しない一般の2標本の平均値の差に関するt検定においては, 2標本の持つ未知の分散の大きさが等しいかが問題になる. t.test()のデフォルトでは等分散性が成立しない (var.equal = FALSE) 設定となっている. この時は, Welch検定が実行される. もちろん, 等分散性が成立する場合には, var.equalの値はTまたはTRUE と指定せねばならない. この時は, 未知の分散はプール化された (“pooled”) 推定値を持ちいたt検定が行われる. A/Bテストデータ (仮想データ) デザイン A と B のどちらが平均的な購入意向を高めるか - id, group (A:旧デザイン, B:新デザイン), purchase_intent (1--7) - 標本サイズ: nA=50, nB=60 abtest_df &lt;- read.csv(&quot;purchase_1-7.csv&quot;, header = T) boxplot(purchase_intent ~ group, data = abtest_df) 等分散性の検定は, var.test()を用いることができる # 等分散性の検定 (F検定) var.test(purchase_intent ~ group, data = abtest_df) #&gt; #&gt; F test to compare two variances #&gt; #&gt; data: purchase_intent by group #&gt; F = 1.1253, num df = 49, denom df = 59, p-value = 0.6606 #&gt; alternative hypothesis: true ratio of variances is not equal to 1 #&gt; 95 percent confidence interval: #&gt; 0.6592874 1.9472951 #&gt; sample estimates: #&gt; ratio of variances #&gt; 1.125316 # ※ 省略されることも多い その上で, Welch検定か, プール化された分散推定値を用いるt検定を実行する. 上の等分散性検定の結果より, 帰無仮説 (2標本の分散は等しい, \\(\\sigma_1^2=\\sigma_2^2\\)) は棄却されないことから (p値\\(=0.6606\\)), 等分散の場合について実行すれば良い. # 等分散の場合 (Pooled variance使用するt検定) t.test(purchase_intent ~ group, var.equal = T, data = abtest_df) #&gt; #&gt; Two Sample t-test #&gt; #&gt; data: purchase_intent by group #&gt; t = -4.3526, df = 108, p-value = 3.067e-05 #&gt; alternative hypothesis: true difference in means between group A and group B is not equal to 0 #&gt; 95 percent confidence interval: #&gt; -1.1449141 -0.4284192 #&gt; sample estimates: #&gt; mean in group A mean in group B #&gt; 4.480000 5.266667 しかし, 繰り返し仮説検定を行うことによる弊害 (多重検定問題) を回避するため, 等分散性の検定を行わず, いきなりWelch検定を実行することも多い. # 等分散でない場合 (Welch t検定) t.test(purchase_intent ~ group, data = abtest_df) #&gt; #&gt; Welch Two Sample t-test #&gt; #&gt; data: purchase_intent by group #&gt; t = -4.3291, df = 101.99, p-value = 3.505e-05 #&gt; alternative hypothesis: true difference in means between group A and group B is not equal to 0 #&gt; 95 percent confidence interval: #&gt; -1.1470981 -0.4262352 #&gt; sample estimates: #&gt; mean in group A mean in group B #&gt; 4.480000 5.266667 自主課題: Q. 以上の検定結果を解釈しなさい. Q. 片側検定にするには? 対立仮説の方向はどちらか? Q. 購入意向 (1-7) は順序尺度では? Q. 正規分布を前提にしたt検定を使用して良いか? 正規性の確認方法 t検定は, 標本を抽出するもととなる母集団の確率分布が 正規分布であるというのが分布に関する基本的な仮定である. したがって, t検定の実行に先立ち, 標本が正規分布に従うか否かを調べ, t検定の前提条件が満たされているかどうかを確認することが求められる. これは, 標本数が小さい時には特に注意する必要がある. # ヒストグラム作成 A &lt;- abtest_df[abtest_df$group == &quot;A&quot;, &quot;purchase_intent&quot;] B &lt;- abtest_df[abtest_df$group == &quot;B&quot;, &quot;purchase_intent&quot;] hist(A, col = rgb(1, 0.5, 0, 0.5)); hist(B, col = rgb(0, 0.5, 1, 0.5), add = T) # 正規性の検定 ks.test(A, &quot;pnorm&quot;); ks.test(B, &quot;pnorm&quot;) # コルモゴロフ・スミルノフ (Kolmogorov-Smirnov) 検定 #&gt; #&gt; Asymptotic one-sample Kolmogorov-Smirnov test #&gt; #&gt; data: A #&gt; D = 0.99865, p-value &lt; 2.2e-16 #&gt; alternative hypothesis: two-sided #&gt; #&gt; Asymptotic one-sample Kolmogorov-Smirnov test #&gt; #&gt; data: B #&gt; D = 0.99865, p-value &lt; 2.2e-16 #&gt; alternative hypothesis: two-sided shapiro.test(A); shapiro.test(B) # シャピロ・ウィルク (Shapiro-Wilk) 検定 #&gt; #&gt; Shapiro-Wilk normality test #&gt; #&gt; data: A #&gt; W = 0.89953, p-value = 0.0004638 #&gt; #&gt; Shapiro-Wilk normality test #&gt; #&gt; data: B #&gt; W = 0.89262, p-value = 7.16e-05 qqnorm(A); qqnorm(B) # q-qプロット なお, 関数ggplot()を使うことで, よりエレガントなプロットを描くことが出来る. # install.packages(&quot;ggplot2&quot;) # 必要に応じてインストール library(ggplot2) ggplot(abtest_df, aes(x = purchase_intent, fill = group)) + geom_histogram(position = &quot;identity&quot;, alpha = 0.5, bins = 7, # ビンの数を適宜指定 color = &quot;grey&quot;) + # 枠線の色 (任意) scale_fill_manual(values = c(&quot;A&quot; = &quot;orange&quot;, &quot;B&quot; = &quot;lightblue&quot;)) + labs(title = &quot;Histogram of Two Groups (Overlapped)&quot;, x = &quot;Purchase Intent (1-7)&quot;, y = &quot;Count&quot;) + theme_minimal() 平均値の差の検定 (ノンパラメトリック検定) 上記のように, t検定は標本を抽出するもととなる母集団の確率分布が 正規分布であるというのが分布に関する基本的な仮定である. しかしその一方で, 標本数が十分に大きい時は, 母集団の確率分布が 正規分布である必要はない. それは, 中心極限定理の働きによって, 検定統計量が帰無仮説の下では 近似的にStudent t分布に従うことが理論的に示されるためである. 一方, 標本数が大きくない場合には, 母集団の確率分布が正規分布でない場合には, t検定の前提条件から乖離する状況となるため, 別の検定法を用いることが適切である. ノンパラメトリック検定法は, 分布形状に関する仮定によらない, ロバスト (頑強) な検定方法である. Wilcoxonの順位和検定 (Mann–WhitneyのU検定) ここでは, 2群の平均値の差に関する t検定の代替的手法として, ウィルコクソン (Wilcoxon) の順位和検定を紹介する. 2群の中央値の差を調べる 外れ値に対して頑強 wilcox.test(purchase_intent ~ group, data = abtest_df) #&gt; #&gt; Wilcoxon rank sum test with continuity correction #&gt; #&gt; data: purchase_intent by group #&gt; W = 861.5, p-value = 5.992e-05 #&gt; alternative hypothesis: true location shift is not equal to 0 # A,Bを定義しておいた場合, 以下の実行も可能 # wilcox.test(A, B) 自主課題: Q. 以上の検定結果を解釈しなさい. 3.2 カイ二乗検定 # chisq.test(x, y = NULL, correct = TRUE, # p = rep(1 / length(x), length(x)), rescale.p = FALSE, simulate.p.value = FALSE, B = 2000) # chisq.test performs chi-squared contingency table tests and goodness-of-fit tests. 3.2.1 独立性検定 # 演習用データの作成 (実務では, ファイルを読み込む) d1 &lt;- matrix(c(rep(c(&quot;a1&quot;, &quot;b1&quot;), 76), rep(c(&quot;a1&quot;, &quot;b2&quot;), 15), rep(c(&quot;a1&quot;, &quot;b3&quot;), 41)), byrow = T, ncol = 2) d2 &lt;- matrix(c(rep(c(&quot;a2&quot;, &quot;b1&quot;), 95), rep(c(&quot;a2&quot;, &quot;b2&quot;), 30), rep(c(&quot;a2&quot;, &quot;b3&quot;), 85)), byrow = T, ncol = 2) d3 &lt;- matrix(c(rep(c(&quot;a3&quot;, &quot;b1&quot;), 135), rep(c(&quot;a3&quot;, &quot;b2&quot;), 70), rep(c(&quot;a3&quot;, &quot;b3&quot;), 95)), byrow = T, ncol = 2) d4 &lt;- matrix(c(rep(c(&quot;a4&quot;, &quot;b1&quot;), 69), rep(c(&quot;a4&quot;, &quot;b2&quot;), 10), rep(c(&quot;a4&quot;, &quot;b3&quot;), 29)), byrow = T, ncol = 2) data2 &lt;- rbind(d1, d2, d3, d4) colnames(data2) &lt;- c(&quot;A&quot;, &quot;B&quot;) # 商品種類(A), 販売チャネル(B) head(data2) #&gt; A B #&gt; [1,] &quot;a1&quot; &quot;b1&quot; #&gt; [2,] &quot;a1&quot; &quot;b1&quot; #&gt; [3,] &quot;a1&quot; &quot;b1&quot; #&gt; [4,] &quot;a1&quot; &quot;b1&quot; #&gt; [5,] &quot;a1&quot; &quot;b1&quot; #&gt; [6,] &quot;a1&quot; &quot;b1&quot; table(data2) # marginal totals #&gt; data2 #&gt; a1 a2 a3 a4 b1 b2 b3 #&gt; 132 210 300 108 375 125 250 (tbl &lt;- table(data2[, 1], data2[, 2])) # contingency table #&gt; #&gt; b1 b2 b3 #&gt; a1 76 15 41 #&gt; a2 95 30 85 #&gt; a3 135 70 95 #&gt; a4 69 10 29 chisq.test(data2[, &quot;A&quot;], data2[, &quot;B&quot;]) #&gt; #&gt; Pearson&#39;s Chi-squared test #&gt; #&gt; data: data2[, &quot;A&quot;] and data2[, &quot;B&quot;] #&gt; X-squared = 27.661, df = 6, p-value = 0.0001088 # chisq.test(data2[, 1], data2[, 2]) # または chisq.test(tbl) #&gt; #&gt; Pearson&#39;s Chi-squared test #&gt; #&gt; data: tbl #&gt; X-squared = 27.661, df = 6, p-value = 0.0001088 # A/Bテスト # サイト導線A/Bとで, コンバージョンへの効果を比較 # 有 無 # サイト導線A 50 131 # サイト導線B 23 35 ABdat &lt;- matrix(c(50, 131, 23, 35), ncol = 2, byrow = T) chisq.test(ABdat) #&gt; #&gt; Pearson&#39;s Chi-squared test with Yates&#39; continuity correction #&gt; #&gt; data: ABdat #&gt; X-squared = 2.4566, df = 1, p-value = 0.117 # 導線Aでコンバージョンしない人が10人増えた # 有 無 # サイト導線A 50 141 # サイト導線B 23 35 ABdat2 &lt;- matrix(c(50, 141, 23, 35), ncol = 2, byrow = T) chisq.test(ABdat2) #&gt; #&gt; Pearson&#39;s Chi-squared test with Yates&#39; continuity correction #&gt; #&gt; data: ABdat2 #&gt; X-squared = 3.2764, df = 1, p-value = 0.07028 # Fisherの正確確率検定 fisher.test(ABdat2) #&gt; #&gt; Fisher&#39;s Exact Test for Count Data #&gt; #&gt; data: ABdat2 #&gt; p-value = 0.06915 #&gt; alternative hypothesis: true odds ratio is not equal to 1 #&gt; 95 percent confidence interval: #&gt; 0.2796541 1.0568106 #&gt; sample estimates: #&gt; odds ratio #&gt; 0.5410443 3.2.2 適合度検定 # メンデルのデータ（エンドウの交雑実験） # 種子の特徴(形質), 黄色・丸い, 黄色・しわ, 緑色・丸い, 緑色・しわ obs &lt;- c(315, 101, 108, 32 ) # 観測度数 prob &lt;- c(9, 3, 3, 1) / 16 # 理論確率分布 chisq.test(obs, p = prob) # obs と prob を用いたカイ二乗検定 #&gt; #&gt; Chi-squared test for given probabilities #&gt; #&gt; data: obs #&gt; X-squared = 0.47002, df = 3, p-value = 0.9254 # 確認用 ex &lt;- prob * sum(obs) chisq &lt;- sum((obs - ex) ^ 2 / ex) pval &lt;- 1 - pchisq(chisq, 3) 3.3 分析例: 統計テストデータ testdat &lt;- read.csv(&quot;BS_stattest.csv&quot;, header = F) # year(学年), MF(性別:男性1女性2), AS(文理:文系1その他2理系3), # math(数学履修年数), work(勤務年数), stat(統計学経験0-2), MS(経営科学好き嫌い0-3), # s4(4級相当得点), s3(3級相当得点), s2(2級相当得点) colnames(testdat) &lt;- c(&quot;year&quot;, &quot;MF&quot;, &quot;AS&quot;, &quot;math&quot;, &quot;work&quot;, &quot;stat&quot;, &quot;MS&quot;, &quot;s4&quot;, &quot;s3&quot;, &quot;s2&quot;) score &lt;- apply(testdat[, c(&quot;s4&quot;, &quot;s3&quot;, &quot;s2&quot;)], 1, sum) testdat2 &lt;- cbind(testdat, score) # モダンな方法 # library(tidyverse) # testdat2 &lt;- testdat %&gt;% mutate(score = s4 + s3 + s2) # データの要約 # attach()を使わない場合: # table(testdat2$MF) # table(testdat2[, c(&quot;MF&quot;, &quot;AS&quot;)]) # table(testdat2$s3) attach(testdat2) table(MF) #&gt; MF #&gt; 1 2 #&gt; 29 8 table(MF, AS) #&gt; AS #&gt; MF 1 2 3 #&gt; 1 17 3 9 #&gt; 2 4 1 3 table(s3) #&gt; s3 #&gt; 3 4 5 6 7 8 9 #&gt; 3 2 5 10 5 10 2 summary(score) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 10.00 16.00 18.00 17.97 20.00 24.00 fivenum(score) #&gt; [1] 10 16 18 20 24 hist(score) # 相関係数 # cor(testdat[, c(&quot;math&quot;, &quot;s2&quot;)]) # cor(testdat[, c(&quot;math&quot;, &quot;s3&quot;)]) # cor(testdat[, c(&quot;math&quot;, &quot;s4&quot;)]) cor(math, s2) #&gt; [1] 0.04241976 cor(math, s3) #&gt; [1] 0.2481946 cor(math, s4) #&gt; [1] 0.3205779 pairs(testdat[, 8:10]) # cor(testdat[, c(&quot;MS&quot;, &quot;s4&quot;, &quot;s3&quot;, &quot;s2&quot;)]) cor(cbind(MS, s4, s3, s2)) #&gt; MS s4 s3 s2 #&gt; MS 1.0000000 0.2592955 0.4922877 0.2600562 #&gt; s4 0.2592955 1.0000000 0.1315443 0.1245631 #&gt; s3 0.4922877 0.1315443 1.0000000 0.6435060 #&gt; s2 0.2600562 0.1245631 0.6435060 1.0000000 cor(MS, math) #&gt; [1] 0.2864297 # 箱ひげ図 boxplot(score ~ factor(MF)) # 2-level factor boxplot(score ~ factor(MS)) # 4-level factor # 平均値の差の検定 score_MF &lt;- split(score, factor(MF)) t.test(score_MF$&#39;1&#39;, score_MF$&#39;2&#39;) #&gt; #&gt; Welch Two Sample t-test #&gt; #&gt; data: score_MF$&quot;1&quot; and score_MF$&quot;2&quot; #&gt; t = 2.8923, df = 13.276, p-value = 0.01237 #&gt; alternative hypothesis: true difference in means is not equal to 0 #&gt; 95 percent confidence interval: #&gt; 0.884667 6.063609 #&gt; sample estimates: #&gt; mean of x mean of y #&gt; 18.72414 15.25000 # score2 &lt;- testdat[, &quot;s2&quot;] # score3 &lt;- testdat[, &quot;s3&quot;] # score4 &lt;- testdat[, &quot;s4&quot;] # t.test(score4, score3, paired = T, alternative = &quot;greater&quot;) # t.test(score3, score2, paired = T, alternative = &quot;greater&quot;) # ペア検定 t.test(s4, s3, paired = T, alternative = &quot;greater&quot;) #&gt; #&gt; Paired t-test #&gt; #&gt; data: s4 and s3 #&gt; t = 0.87426, df = 36, p-value = 0.1939 #&gt; alternative hypothesis: true mean difference is greater than 0 #&gt; 95 percent confidence interval: #&gt; -0.2516528 Inf #&gt; sample estimates: #&gt; mean difference #&gt; 0.2702703 t.test(s3, s2, paired = T, alternative = &quot;greater&quot;) #&gt; #&gt; Paired t-test #&gt; #&gt; data: s3 and s2 #&gt; t = 5.305, df = 36, p-value = 2.95e-06 #&gt; alternative hypothesis: true mean difference is greater than 0 #&gt; 95 percent confidence interval: #&gt; 0.9212852 Inf #&gt; sample estimates: #&gt; mean difference #&gt; 1.351351 # 得点の差のt値の計算 (確認用) m &lt;- mean(s3 - s2) v &lt;- var(s3 - s2) tt &lt;- m / sqrt(v / (length(s3))) # var.test(x, y, ratio = 1, # alternative = c(&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;), # conf.level = 0.95, ...) # Performs an F test to compare the variances of two samples from normal populations. # aaa &lt;- table(testdat[, c(&quot;MF&quot;, &quot;AS&quot;)]) aaa &lt;- table(MF, AS) chisq.test(aaa) #&gt; #&gt; Pearson&#39;s Chi-squared test #&gt; #&gt; data: aaa #&gt; X-squared = 0.18986, df = 2, p-value = 0.9094 # aaa &lt;- table(testdat[, c(&quot;MS&quot;, &quot;AS&quot;)]) aaa &lt;- table(MS, AS) chisq.test(aaa) #&gt; #&gt; Pearson&#39;s Chi-squared test #&gt; #&gt; data: aaa #&gt; X-squared = 16.044, df = 6, p-value = 0.01352 detach(testdat2) "],["分散分析-anova.html", "4 分散分析 (ANOVA) 4.1 1元配置ANOVA 4.2 2元配置ANOVA 4.3 分析例 4.4 分散分析と平均値の多重比較*", " 4 分散分析 (ANOVA) 授業では, 多群における平均値の差の検定の方法として, (1元配置) 分散分析 (ANOVA) を紹介した. \\(k\\)個の群 (グループ) があり, それぞれの真の平均値が \\(\\mu_1,\\mu_2,...,\\mu_k\\)であるとする. ANOVA帰無仮説 (\\(H_0\\)): 全群の平均値が等しい (\\(\\mu_1=\\mu_2=...=\\mu_k\\)) 対立仮説 (\\(H_1\\)): 平均値の異なる群が少なくとも一つある (\\(\\mu_i \\neq \\mu_j, i \\neq j\\)) 4.1 1元配置ANOVA ANOVAの実行には, 1元配置ANOVAに特化したR関数であるoneway.anova()の他, より一般のANOVAに適用できるaov(), anova()が利用できる. 以下では, 仮想データセット (3群, \\(k=3\\)) について, aov()を使用した実行例を示す. # コード出所: ChatGPT-4の出力をもとに編集 # 仮想データセットの作成 (数値例1) # グループ数: k=3 k &lt;- 3 ttt1 &lt;- c(8, 7, 9, 6, 8) ttt2 &lt;- c(7, 5, 4) ttt3 &lt;- c(6, 2, 1, 3) # &quot;縦型形式&quot;データセットの作成 dat1 &lt;- data.frame(grp = c(rep(&quot;ttt1&quot;, length(ttt1)), rep(&quot;ttt2&quot;, length(ttt2)), rep(&quot;ttt3&quot;, length(ttt3))), resp = c(ttt1, ttt2, ttt3)) # ANOVAの実行 res_aov &lt;- aov(resp ~ grp, data = dat1) # 実行結果の表示 summary(res_aov) #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; grp 2 47.13 23.567 8.887 0.0074 ** #&gt; Residuals 9 23.87 2.652 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (自主課題) 実行結果の解釈をしてみよう. ANOVAの計算ロジックの確認* つぎに, 1元配置ANOVAが実際に行っている計算をRコードで書いたものを紹介する. これは, ANOVAの理論を理解を深めることを目的にしたもので, ANOVAの実践には不要であり, 読み飛ばしても構わない. # 確認用 (飛ばしてOK) library(tidyverse) # 全体平方和 (SST) の計算 avg &lt;- mean(dat1$resp) # 全体平均 # (mean(dat1$resp^2) - avg^2) * length(dat1$resp) sst &lt;- sum((dat1$resp - avg)^2) # 全体平方和 # 群内平方和 (SSW) の計算 # (※) 発展的なコード # library(tidyverse) ssw_j &lt;- dat1 %&gt;% group_by(grp) %&gt;% summarize(ssq = (mean(resp^2) - mean(resp)^2) * length(resp)) ssw &lt;- sum(ssw_j$ssq) # 群内平方和 # 群間平方和 (SSB) の計算 n_vec &lt;- c(length(ttt1), length(ttt2), length(ttt3)) avg_j &lt;- dat1 %&gt;% group_by(grp) %&gt;% summarize(avg = mean(resp)) ssb &lt;- sum((avg_j$avg - avg)^2 * n_vec) # 群間平方和 # または, ssb &lt;- sst - ssw # 群間平方和 # 自由度 (df) # SST: length(dat1$resp) - 1, # SSB: K - 1, # SSW: length(dat1$resp) - K # 平均平方和の計算 msb &lt;- ssb / (k - 1) msw &lt;- ssw / (length(dat1$resp) - k) # F値の計算 f_val &lt;- msb / msw # p値の計算 pf(f_val, k - 1, length(dat1$resp) - k, lower.tail = F) #&gt; [1] 0.007402874 (自主課題) 関数aov()の実行結果と見比べてみよう. 4.2 2元配置ANOVA 授業ではカバーしていないが, 2つの因子が反応変数に与える影響を 調べる2元配置ANOVAの実行コード例を紹介する. ここでは, 別の仮想データセット (2群) を用意する. # コード出所: ChatGPT-4の出力をもとに編集 # 仮想データセットの作成 (数値例2) fctA_val &lt;- c(&quot;a1&quot;, &quot;a2&quot;, &quot;a3&quot;, &quot;a4&quot;) fctB_val &lt;- c(&quot;b1&quot;, &quot;b2&quot;, &quot;b3&quot;) val &lt;- matrix(c(8, 7, 6, 7, 5, 2, 6, 4, 3, 8, 6, 2), nrow = 4, byrow = T) colnames(val) &lt;- fctB_val rownames(val) &lt;- fctA_val # &quot;縦型形式&quot;データセットに変形 dat2 &lt;- as.data.frame(val) %&gt;% rownames_to_column(var = &quot;fctA&quot;) %&gt;% pivot_longer(cols = b1:b3, names_to = &quot;fctB&quot;, values_to = &quot;resp&quot;) %&gt;% data.frame() # SST # sum((val - mean(val))^2) # ANOVAの実行 res_aov &lt;- aov(resp ~ fctA + fctB, data = dat2) # 実行結果の表示 summary(res_aov) #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; fctA 3 12.67 4.222 4.343 0.05988 . #&gt; fctB 2 32.17 16.083 16.543 0.00362 ** #&gt; Residuals 6 5.83 0.972 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # interactionなし # res_aov &lt;- aov(resp ~ fctA * fctB, data = dat2) # summary(res_aov) (自主課題) 実行結果の解釈をしてみよう. 4.3 分析例 データ1: 2業種口コミサイト(仮想データ) t検定と結果の比較を行う. データの読み込み, 並べ替え x &lt;- read.csv(&quot;dat_1-1.csv&quot;) # 口コミサイト(仮想データ) # 2つの業種(A, B), 各20社 # 各企業に対する(元)従業員による平均評価点(1--5) head(x) #&gt; A B #&gt; 1 2.257 4.065 #&gt; 2 4.273 4.771 #&gt; 3 4.205 2.793 #&gt; 4 3.251 3.003 #&gt; 5 1.534 3.250 #&gt; 6 3.327 3.390 # xの整形 (横型 → 縦型) xvec &lt;- as.vector(as.matrix(x)) # xをベクトル化 yvec &lt;- c(rep(&quot;A&quot;, 20), rep(&quot;B&quot;, 20)) xdf &lt;- data.frame(score = xvec, type = yvec) str(xdf) #&gt; &#39;data.frame&#39;: 40 obs. of 2 variables: #&gt; $ score: num 2.26 4.27 4.21 3.25 1.53 ... #&gt; $ type : chr &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ... xdf$score; xdf[, &quot;score&quot;]; xdf[, 1] #&gt; [1] 2.257 4.273 4.205 3.251 1.534 3.327 3.596 3.421 2.544 2.110 2.262 2.550 #&gt; [13] 2.790 2.033 3.688 3.440 3.797 3.644 2.906 1.230 4.065 4.771 2.793 3.003 #&gt; [25] 3.250 3.390 2.834 2.171 4.654 3.051 4.789 4.373 3.325 3.028 4.127 2.796 #&gt; [37] 2.015 4.379 3.380 3.793 #&gt; [1] 2.257 4.273 4.205 3.251 1.534 3.327 3.596 3.421 2.544 2.110 2.262 2.550 #&gt; [13] 2.790 2.033 3.688 3.440 3.797 3.644 2.906 1.230 4.065 4.771 2.793 3.003 #&gt; [25] 3.250 3.390 2.834 2.171 4.654 3.051 4.789 4.373 3.325 3.028 4.127 2.796 #&gt; [37] 2.015 4.379 3.380 3.793 #&gt; [1] 2.257 4.273 4.205 3.251 1.534 3.327 3.596 3.421 2.544 2.110 2.262 2.550 #&gt; [13] 2.790 2.033 3.688 3.440 3.797 3.644 2.906 1.230 4.065 4.771 2.793 3.003 #&gt; [25] 3.250 3.390 2.834 2.171 4.654 3.051 4.789 4.373 3.325 3.028 4.127 2.796 #&gt; [37] 2.015 4.379 3.380 3.793 # 代替的方法 # stack(x) ## library(tidyverse) ## gather関数(横型→縦型), spread関数(縦型→横型) 1元ANOVAの実行 # 1元ANOVA attach(xdf) res_aov &lt;- aov(score ~ type) summary(res_aov) ### 代替アプローチ (1) oneway.test(score ~ type) # デフォルト：等分散を仮定しない oneway.test(score ~ type, var.equal = T) # 等分散を仮定 ### 代替アプローチ (2) res_lm = lm(score ~ type) res_anova &lt;- anova(res_lm) res_anova # summary()を使わずに出力 detach(xdf) # 等分散検定 var.test(x$A, x$B) boxplot(x) # t検定との比較 t.test(x$A, x$B) t.test(x$A, x$B, var.equal = T) ##t.test(x$A, x$B, paired=T) データ2: 統計小テストデータ KBS専門科目「ビジネス統計」履修者に対して行った, 統計学に対する小テストの結果と, 各受験者のデモグラフィック情報から成るデータセットである. これに対して, ANOVAを実行してみる. year(学年), MF(性別:男性1女性2), AS(文理:文系1その他2理系3), math(数学履修年数), work(勤務年数), stat(統計学経験0-2), MS(経営科学好き嫌い0-3), s4(4級相当得点), s3(3級相当得点), s2(2級相当得点) testdat &lt;- read.csv(&quot;BS_stattest.csv&quot;, header = F) colnames(testdat) &lt;- c(&quot;year&quot;, &quot;MF&quot;, &quot;AS&quot;, &quot;math&quot;, &quot;work&quot;, &quot;stat&quot;, &quot;MS&quot;, &quot;s4&quot;, &quot;s3&quot;, &quot;s2&quot;) # 総合得点の計算, 列に追加 score &lt;- apply(testdat[, c(&quot;s4&quot;, &quot;s3&quot;, &quot;s2&quot;)], 1, sum) testdat2 &lt;- cbind(testdat, score) # モダンな方法 # library(tidyverse) # testdat2 &lt;- testdat %&gt;% mutate(score = s4 + s3 + s2) str(testdat2) #&gt; &#39;data.frame&#39;: 37 obs. of 11 variables: #&gt; $ year : int 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ MF : int 1 1 1 1 2 1 1 1 1 1 ... #&gt; $ AS : int 1 2 1 1 1 1 1 1 1 3 ... #&gt; $ math : int 13 6 8 7 5 5 5 6 8 10 ... #&gt; $ work : int 7 6 4 4 1 5 4 7 0 8 ... #&gt; $ stat : int 1 0 1 0 0 0 0 0 0 0 ... #&gt; $ MS : int 2 3 1 1 1 1 0 1 1 2 ... #&gt; $ s4 : int 7 8 7 7 7 5 5 7 8 7 ... #&gt; $ s3 : int 8 8 6 9 5 8 3 7 6 8 ... #&gt; $ s2 : int 7 8 6 7 4 7 2 6 7 5 ... #&gt; $ score: int 22 24 19 23 16 20 10 20 21 20 ... # → 得点以外の変数も数値(整数)で入力されている # 分割表(クロス集計表) table(testdat2[, c(&quot;MF&quot;, &quot;AS&quot;)]) #&gt; AS #&gt; MF 1 2 3 #&gt; 1 17 3 9 #&gt; 2 4 1 3 # 相関係数 cor(testdat2[, c(&quot;math&quot;, &quot;s2&quot;)]) #&gt; math s2 #&gt; math 1.00000000 0.04241976 #&gt; s2 0.04241976 1.00000000 cor(testdat2[, c(&quot;MS&quot;, &quot;s4&quot;, &quot;s3&quot;, &quot;s2&quot;)]) #&gt; MS s4 s3 s2 #&gt; MS 1.0000000 0.2592955 0.4922877 0.2600562 #&gt; s4 0.2592955 1.0000000 0.1315443 0.1245631 #&gt; s3 0.4922877 0.1315443 1.0000000 0.6435060 #&gt; s2 0.2600562 0.1245631 0.6435060 1.0000000 cor(testdat2[, c(&quot;MS&quot;, &quot;math&quot;)]) #&gt; MS math #&gt; MS 1.0000000 0.2864297 #&gt; math 0.2864297 1.0000000 # 箱ひげ図 attach(testdat2) par(mfrow=c(1,2)) boxplot(score ~ factor(MF)) # 2-level factor boxplot(score ~ factor(MS)) # 4-level factor aaa &lt;- table(testdat2[, c(&quot;MF&quot;, &quot;AS&quot;)]) chisq.test(aaa) #&gt; #&gt; Pearson&#39;s Chi-squared test #&gt; #&gt; data: aaa #&gt; X-squared = 0.18986, df = 2, p-value = 0.9094 aaa &lt;- table(testdat2[, c(&quot;MS&quot;, &quot;AS&quot;)]) chisq.test(aaa) #&gt; #&gt; Pearson&#39;s Chi-squared test #&gt; #&gt; data: aaa #&gt; X-squared = 16.044, df = 6, p-value = 0.01352 1元ANOVA # 1-way ANOVA summary(aov(score ~ factor(MS))) #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; factor(MS) 3 108.2 36.07 3.282 0.0329 * #&gt; Residuals 33 362.8 10.99 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # 代替的アプローチ anova(lm(score ~ factor(MS))) #&gt; Analysis of Variance Table #&gt; #&gt; Response: score #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; factor(MS) 3 108.22 36.074 3.2817 0.03293 * #&gt; Residuals 33 362.75 10.992 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 2元ANOVA 2因子の場合には, 1因子の場合と異なり, 因子間に交互作用 (相互作用) が存在する場合がある. 関数interaction.plotにより交互作用プロットを作図することで, 存在の有無を目で確認することができる. 関数aov()を使用した実行例: ### 2-way ANOVA # 交互作用プロット interaction.plot(factor(MS), factor(MF), score) summary(aov(score ~ factor(MS) + factor(MF))) # 主効果項のみ (交互作用項なし) #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; factor(MS) 3 108.2 36.07 3.571 0.0246 * #&gt; factor(MF) 1 39.5 39.49 3.909 0.0567 . #&gt; Residuals 32 323.3 10.10 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(aov(score ~ factor(MS) * factor(MF))) # 交互作用項有 #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; factor(MS) 3 108.22 36.07 3.727 0.0217 * #&gt; factor(MF) 1 39.49 39.49 4.080 0.0524 . #&gt; factor(MS):factor(MF) 2 32.92 16.46 1.701 0.1997 #&gt; Residuals 30 290.35 9.68 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 代替的に, 関数anova()を使用した実行例: # 代替的アプローチ anova(lm(score ~ factor(MS) + factor(AS))) # 主効果項のみ (交互作用項なし) #&gt; Analysis of Variance Table #&gt; #&gt; Response: score #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; factor(MS) 3 108.22 36.074 3.3147 0.03267 * #&gt; factor(AS) 2 25.38 12.689 1.1659 0.32493 #&gt; Residuals 31 337.37 10.883 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(lm(score ~ factor(AS) + factor(MS))) # 分解順の影響 #&gt; Analysis of Variance Table #&gt; #&gt; Response: score #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; factor(AS) 2 3.50 1.748 0.1607 0.85229 #&gt; factor(MS) 3 130.10 43.368 3.9849 0.01642 * #&gt; Residuals 31 337.37 10.883 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(lm(score ~ factor(MS) * factor(AS))) # 交互作用項有 #&gt; Analysis of Variance Table #&gt; #&gt; Response: score #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; factor(MS) 3 108.22 36.074 3.3063 0.03397 * #&gt; factor(AS) 2 25.38 12.689 1.1629 0.32672 #&gt; factor(MS):factor(AS) 2 20.96 10.478 0.9603 0.39460 #&gt; Residuals 29 316.42 10.911 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 lm(score~factor(MS) + factor(MF)) #&gt; #&gt; Call: #&gt; lm(formula = score ~ factor(MS) + factor(MF)) #&gt; #&gt; Coefficients: #&gt; (Intercept) factor(MS)1 factor(MS)2 factor(MS)3 factor(MF)2 #&gt; 15.552 2.693 3.820 8.448 -2.604 detach(testdat2) (自主課題) 実行結果の解釈をしてみよう. “対応のある分散分析”を実行してみよう. (ヒント: 山田・杉澤・村井 (2008)『Rによるやさしい統計学』「第7章 分散分析」を参照せよ.) データ3: 経営科学アンケート KBS専門科目「ビジネス統計」履修者に対して行った, MBA基礎科目「経営科学」に対するフィードバックの回答と, 各受験者のデモグラフィック情報から成るデータセットである. グループ(group), 負荷(workload), 難易度(difficulty), 有用性(usefulness), 貢献度(contribution), 数学スキル(Math), エクセルスキル(Excel), 文理(AS), 業務経験(Work), 性別(MF) testdat &lt;- read.csv(&quot;FB_dist.csv&quot;, header=T) attach(testdat) # 実行1 aov(difficulty ~ Math) #&gt; Call: #&gt; aov(formula = difficulty ~ Math) #&gt; #&gt; Terms: #&gt; Math Residuals #&gt; Sum of Squares 20.02254 52.62250 #&gt; Deg. of Freedom 1 113 #&gt; #&gt; Residual standard error: 0.6824118 #&gt; Estimated effects may be unbalanced # anova(lm(difficulty ~ Math)) # 実行2 aov(difficulty ~ factor(Math)) #&gt; Call: #&gt; aov(formula = difficulty ~ factor(Math)) #&gt; #&gt; Terms: #&gt; factor(Math) Residuals #&gt; Sum of Squares 21.81657 50.82847 #&gt; Deg. of Freedom 4 110 #&gt; #&gt; Residual standard error: 0.6797624 #&gt; Estimated effects may be unbalanced # anova(lm(difficulty ~ factor(Math))) detach(testdat) (自主課題) 実行結果の解釈をしてみよう. “対応のある分散分析”を実行してみよう. (ヒント: 山田・杉澤・村井 (2008)『Rによるやさしい統計学』「第7章 分散分析」を参照せよ.) 4.4 分散分析と平均値の多重比較* 授業では, 多群における平均値の差の検定の方法として, (1元配置) 分散分析 (ANOVA) を紹介したが, その中で, 2群のt検定を繰り返し実行する方法についての質問があった. (独立な) 検定を繰り返し実行すると実質的な有意水準が低下し, 帰無仮説を想定より棄却しやすくなる現象”多重検定問題”について 言及した. また, 分散分析の結果について追加分析の必要性についても言及した. このように3群以上を比較したい場合に, 分散分析（ANOVA）を実施し, 全体差ありと判定された後に多重比較法 (ペア比較) を実施することをPost-hoc test (事後検定)と呼ぶ. -多重比較法 (MCP): - 複数の仮説 (平均値の差など) を同時に検定する際に, 全体の誤り率 (FWERやFDRなど) を制御するための統計的方法の総称 ここでは, ANOVAによって, 群間に平均値の差ありと判断された後に実践される方法として, Tukeyによる HSD（Honestly Significant Difference）検定について紹介する. これは 一元配置ANOVAの全ペアに対して, 平均の差について同時に検定・同時信頼区間を与える多重比較法である (有意水準の補正を内包). FWER (family-wise error rate, ファミリー内誤差率）を α にコントロール する. (すなわち, 別途 Bonferroni 等の補正は不要) HSD検定は, 群サイズが異なるにも適用可能であり, Tukey–Kramer法とも呼ばれる. FWER: ある多重検定の集合（family）に含まれる仮説群のうち, 少なくとも一つでも誤って棄却する確率: \\(FWER=\\)P(少なくとも1つの誤検出) 前提：独立性・正規性・等分散性. 等分散でない場合 (Games–Howell), 正規性の仮定なし (ノンパラメトリック) の場合 (Steel-Dwass) 以下の出所: ChatGPT (GPT-5) の出力を一部加工 Rによる事後分析の実行例 1) 基本：ANOVA → TukeyHSD set.seed(1) dat &lt;- data.frame( group = rep(LETTERS[1:4], times = c(20, 22, 18, 25)), # サイズ不等でも可 y = c(rnorm(20, 0, 3), rnorm(22, 1, 3), rnorm(18, 3, 3), rnorm(25, -1, 3)) ) # 一元配置 ANOVA fit &lt;- aov(y ~ group, data = dat) # Tukey（R base の同時比較：不等サイズなら内部で Tukey–Kramer） tk &lt;- TukeyHSD(fit) # 同時CIと同時p値（FWER制御） print(tk) #&gt; Tukey multiple comparisons of means #&gt; 95% family-wise confidence level #&gt; #&gt; Fit: aov(formula = y ~ group, data = dat) #&gt; #&gt; $group #&gt; diff lwr upr p adj #&gt; B-A 0.3537944 -1.8739998 2.5815887 0.9754905 #&gt; C-A 2.9607318 0.6180284 5.3034353 0.0073568 #&gt; D-A -1.3820488 -3.5452555 0.7811580 0.3430124 #&gt; C-B 2.6069374 0.3152300 4.8986448 0.0192974 #&gt; D-B -1.7358432 -3.8437158 0.3720294 0.1432848 #&gt; D-C -4.3427806 -6.5717533 -2.1138079 0.0000124 # 可視化 plot(tk) # 各比較の平均差と95%信頼区間が横棒で表示される 参考: ggplot2を用いた可視化の例 library(emmeans) emm &lt;- emmeans(fit, ~ group) pairs &lt;- pairs(emm, adjust = &quot;tukey&quot;) plot(pairs) # latticeベースの図 # ggplot2 に変換 df &lt;- as.data.frame(confint(pairs)) library(ggplot2) ggplot(df, aes(contrast, estimate, ymin = lower.CL, ymax = upper.CL)) + geom_pointrange() + geom_hline(yintercept = 0, linetype = 2) + coord_flip() + theme_minimal() 2) 推定周辺平均（EMM）経由の一般化（推奨） # install.packages(&quot;emmeans&quot;) library(emmeans) emm &lt;- emmeans(fit, ~ group) res &lt;- pairs(emm, adjust = &quot;tukey&quot;) # 同時p値（FWER制御） conf &lt;- confint(pairs(emm), adjust = &quot;tukey&quot;) # 同時CI res #&gt; contrast estimate SE df t.ratio p.value #&gt; A - B -0.354 0.849 81 -0.417 0.9755 #&gt; A - C -2.961 0.893 81 -3.315 0.0074 #&gt; A - D 1.382 0.825 81 1.676 0.3430 #&gt; B - C -2.607 0.874 81 -2.984 0.0193 #&gt; B - D 1.736 0.804 81 2.160 0.1433 #&gt; C - D 4.343 0.850 81 5.111 &lt;.0001 #&gt; #&gt; P value adjustment: tukey method for comparing a family of 4 estimates conf #&gt; contrast estimate SE df lower.CL upper.CL #&gt; A - B -0.354 0.849 81 -2.582 1.874 #&gt; A - C -2.961 0.893 81 -5.303 -0.618 #&gt; A - D 1.382 0.825 81 -0.781 3.545 #&gt; B - C -2.607 0.874 81 -4.899 -0.315 #&gt; B - D 1.736 0.804 81 -0.372 3.844 #&gt; C - D 4.343 0.850 81 2.114 6.572 #&gt; #&gt; Confidence level used: 0.95 #&gt; Conf-level adjustment: tukey method for comparing a family of 4 estimates emmeans() は共変量調整付きモデル (例：線形モデル、GLM、混合効果) でも同様に, 調整済み平均差の Tukey を適用でき, 実務で汎用的である. 3) 単純な「ペアごとの t 検定」を多重比較補正つきで一括実行 Tukey ではなく,「普通のペア t 検定＋p 値補正」をしたい場合: # 等分散仮定の t 検定を全ペアで実施し、p値を同時に補正 ptt &lt;- pairwise.t.test(dat$y, dat$group, p.adjust.method = &quot;holm&quot;, pool.sd = TRUE) ptt #&gt; #&gt; Pairwise comparisons using t tests with pooled SD #&gt; #&gt; data: dat$y and dat$group #&gt; #&gt; A B C #&gt; B 0.6781 - - #&gt; C 0.0069 0.0150 - #&gt; D 0.1952 0.1011 1.3e-05 #&gt; #&gt; P value adjustment method: holm p.adjust.method には “bonferroni”, “holm”, “BH”（FDR）など. こちらは 「t検定＋補正」 であり, Tukey そのものではない. (分布と臨界値が異なる). FWER を厳密に制御したい ANOVA 後の全対比較には, 通常 Tukey を選ぶ. 4) 等分散が怪しいとき（参考：Games–Howell） # install.packages(&quot;rstatix&quot;) library(rstatix) gh &lt;- games_howell_test(dat, y ~ group) # Welch型 + 不等分散・不等サイズに強い gh #&gt; # A tibble: 6 × 8 #&gt; .y. group1 group2 estimate conf.low conf.high p.adj p.adj.signif #&gt; * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 y A B 0.354 -1.82 2.53 0.972 ns #&gt; 2 y A C 2.96 0.654 5.27 0.007 ** #&gt; 3 y A D -1.38 -3.71 0.945 0.396 ns #&gt; 4 y B C 2.61 0.451 4.76 0.013 * #&gt; 5 y B D -1.74 -3.91 0.440 0.16 ns #&gt; 6 y C D -4.34 -6.65 -2.04 0.0000579 **** 等分散仮定を置かない多重比較として実務で広く用いられる (FWER近似制御). "],["線形回帰分析.html", "5 線形回帰分析 5.1 重回帰分析の基本操作 5.2 変数の選択 5.3 説明変数に質的変数を含む回帰 5.4 説明変数に質的変数を含む回帰 (2)", " 5 線形回帰分析 はじめに, コードの可読性を高めるため, パッケージtidyverseをロードしておく. 例えば, tidyverse内にあるパッケージmagrittrの提供する機能であるパイプ (演算子) %&gt;% を関数head()と組合せて使用し, 出力量を抑える. library(tidyverse) 5.1 重回帰分析の基本操作 データ1: 1ルーム賃貸マンション - 1ルーム賃貸マンション, 家賃データ, 50件 (仮想データ) - rent: 月額家賃 (円) - area: 専有面積 (平米) - yrs: 築後年数 (年) - dist: 最寄駅からの徒歩距離 (m) データの読み込み rentdat &lt;- read.csv(&quot;rentdat.csv&quot;, header = T) head(rentdat) # R標準の記法 #&gt; rent area yrs dist #&gt; 1 60000 18.45 8.73 837.46 #&gt; 2 61000 19.84 13.33 520.86 #&gt; 3 74000 22.45 8.26 433.77 #&gt; 4 77000 26.81 5.94 1192.32 #&gt; 5 59000 17.62 3.85 815.17 #&gt; 6 86000 26.68 4.19 373.87 # または, パイプ (%&gt;%) を利用して, # rentdat %&gt;% head() 実行に先立ち, pairs()やcor()を使い, 変数間の従属性や, 相関係数の大きさを確認する. pairs(rentdat) cor(rentdat) #&gt; rent area yrs dist #&gt; rent 1.0000000 0.84098526 -0.16885266 -0.36727009 #&gt; area 0.8409853 1.00000000 0.05454398 -0.02291733 #&gt; yrs -0.1688527 0.05454398 1.00000000 -0.05812975 #&gt; dist -0.3672701 -0.02291733 -0.05812975 1.00000000 # パイプ (%&gt;%) を利用しても良い # rendat %&gt;% pairs() # rentdat %&gt;5 cor() 回帰実行 関数lm()を使用して最小二乗法による適合を行う. 実行結果はsummary()で確認する. res_lm &lt;- lm(rent ~ ., data = rentdat) summary(res_lm) #&gt; #&gt; Call: #&gt; lm(formula = rent ~ ., data = rentdat) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -6732 -2379 -1016 2286 7256 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 32261.469 3652.405 8.833 1.81e-11 *** #&gt; area 2397.144 142.744 16.793 &lt; 2e-16 *** #&gt; yrs -745.440 159.275 -4.680 2.55e-05 *** #&gt; dist -12.443 1.733 -7.180 4.89e-09 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 3531 on 46 degrees of freedom #&gt; Multiple R-squared: 0.8838, Adjusted R-squared: 0.8762 #&gt; F-statistic: 116.6 on 3 and 46 DF, p-value: &lt; 2.2e-16 実行結果の取り出し # 回帰係数の取り出し coef(res_lm) # 関数の利用 #&gt; (Intercept) area yrs dist #&gt; 32261.46944 2397.14374 -745.44017 -12.44341 res_lm$coef # 省略形による指示可能 #&gt; (Intercept) area yrs dist #&gt; 32261.46944 2397.14374 -745.44017 -12.44341 # res_lm$coefficients # 適合値 (予測値) の取り出し fitted(res_lm) %&gt;% head() # head()により, 最初の6行のみ表示 (デフォルト) #&gt; 1 2 3 4 5 6 #&gt; 59560.22 63402.81 74522.43 77264.45 61485.70 88441.65 #res_lm$fitted # 残差の取り出し resid(res_lm) # 関数の利用 #&gt; 1 2 3 4 5 6 7 #&gt; 439.7807 -2402.8084 -522.4320 -264.4500 -2485.7017 -2441.6518 -1029.7327 #&gt; 8 9 10 11 12 13 14 #&gt; 5176.2213 -6732.0408 2427.4126 376.6794 4555.9941 -1754.1073 1534.2533 #&gt; 15 16 17 18 19 20 21 #&gt; 647.1293 -2143.6007 5146.9150 4191.1927 2626.1939 5839.5466 1751.7939 #&gt; 22 23 24 25 26 27 28 #&gt; -5750.1494 -2990.2238 -5767.8528 -2307.8140 -3235.3851 -114.3409 -3829.2694 #&gt; 29 30 31 32 33 34 35 #&gt; 715.0065 -3332.8895 7256.4052 2590.8913 3537.0448 1388.8917 6999.9397 #&gt; 36 37 38 39 40 41 42 #&gt; -3321.0903 -2191.1362 5489.4176 -3603.0246 -1025.7680 -1625.3943 -1887.0020 #&gt; 43 44 45 46 47 48 49 #&gt; -1650.4408 1863.8094 -1133.3323 -2663.5164 5523.2614 -1554.5109 -1311.7071 #&gt; 50 #&gt; -1006.4070 res_lm$resid %&gt;% head() #&gt; 1 2 3 4 5 6 #&gt; 439.7807 -2402.8084 -522.4320 -264.4500 -2485.7017 -2441.6518 # res_lm$residuals # 回帰係数の信頼区間の計算 confint(res_lm) #&gt; 2.5 % 97.5 % #&gt; (Intercept) 24909.55913 39613.379763 #&gt; area 2109.81414 2684.473338 #&gt; yrs -1066.04436 -424.835985 #&gt; dist -15.93178 -8.955039 モデル診断 plot(res_lm$fitted.values, rentdat$rent) # モデル診断: y観測値 vs y適合値 abline(a = 0, b = 1) plot(res_lm$fitted.values, res_lm$residuals) # モデル診断: y適合値 vs 残差 abline(h = 0) par(mfrow=c(2,2)) plot(res_lm) # → resid(res_lm) 適合モデルを使った予測 内挿予測 (適合値の計算) predict(res_lm) %&gt;% head() #&gt; 1 2 3 4 5 6 #&gt; 59560.22 63402.81 74522.43 77264.45 61485.70 88441.65 外挿予測 例. 専有面積=18.8平米, 築後年数=13年, 駅距離=800m, または100mの物件の賃料は? new &lt;- data.frame(area = 18.8, dist = c(800, 100), yrs = 13) predict(res_lm, newdata = new) #&gt; 1 2 #&gt; 57682.32 66392.71 #predict.lm(res_lm, newdata = new) #res_lm$residuals # resid(res_lm) また, summary()には最小二乗推定の各種結果が格納されている. str(summary(res_lm)) #&gt; List of 11 #&gt; $ call : language lm(formula = rent ~ ., data = rentdat) #&gt; $ terms :Classes &#39;terms&#39;, &#39;formula&#39; language rent ~ area + yrs + dist #&gt; .. ..- attr(*, &quot;variables&quot;)= language list(rent, area, yrs, dist) #&gt; .. ..- attr(*, &quot;factors&quot;)= int [1:4, 1:3] 0 1 0 0 0 0 1 0 0 0 ... #&gt; .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 #&gt; .. .. .. ..$ : chr [1:4] &quot;rent&quot; &quot;area&quot; &quot;yrs&quot; &quot;dist&quot; #&gt; .. .. .. ..$ : chr [1:3] &quot;area&quot; &quot;yrs&quot; &quot;dist&quot; #&gt; .. ..- attr(*, &quot;term.labels&quot;)= chr [1:3] &quot;area&quot; &quot;yrs&quot; &quot;dist&quot; #&gt; .. ..- attr(*, &quot;order&quot;)= int [1:3] 1 1 1 #&gt; .. ..- attr(*, &quot;intercept&quot;)= int 1 #&gt; .. ..- attr(*, &quot;response&quot;)= int 1 #&gt; .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; #&gt; .. ..- attr(*, &quot;predvars&quot;)= language list(rent, area, yrs, dist) #&gt; .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:4] &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; #&gt; .. .. ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;rent&quot; &quot;area&quot; &quot;yrs&quot; &quot;dist&quot; #&gt; $ residuals : Named num [1:50] 440 -2403 -522 -264 -2486 ... #&gt; ..- attr(*, &quot;names&quot;)= chr [1:50] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... #&gt; $ coefficients : num [1:4, 1:4] 32261.5 2397.1 -745.4 -12.4 3652.4 ... #&gt; ..- attr(*, &quot;dimnames&quot;)=List of 2 #&gt; .. ..$ : chr [1:4] &quot;(Intercept)&quot; &quot;area&quot; &quot;yrs&quot; &quot;dist&quot; #&gt; .. ..$ : chr [1:4] &quot;Estimate&quot; &quot;Std. Error&quot; &quot;t value&quot; &quot;Pr(&gt;|t|)&quot; #&gt; $ aliased : Named logi [1:4] FALSE FALSE FALSE FALSE #&gt; ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;(Intercept)&quot; &quot;area&quot; &quot;yrs&quot; &quot;dist&quot; #&gt; $ sigma : num 3531 #&gt; $ df : int [1:3] 4 46 4 #&gt; $ r.squared : num 0.884 #&gt; $ adj.r.squared: num 0.876 #&gt; $ fstatistic : Named num [1:3] 117 3 46 #&gt; ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;value&quot; &quot;numdf&quot; &quot;dendf&quot; #&gt; $ cov.unscaled : num [1:4, 1:4] 1.069834 -0.035212 -0.017107 -0.000183 -0.035212 ... #&gt; ..- attr(*, &quot;dimnames&quot;)=List of 2 #&gt; .. ..$ : chr [1:4] &quot;(Intercept)&quot; &quot;area&quot; &quot;yrs&quot; &quot;dist&quot; #&gt; .. ..$ : chr [1:4] &quot;(Intercept)&quot; &quot;area&quot; &quot;yrs&quot; &quot;dist&quot; #&gt; - attr(*, &quot;class&quot;)= chr &quot;summary.lm&quot; summary(res_lm)$r.squared # R2 #&gt; [1] 0.8837688 # summary(res_lm)[&quot;r.squared&quot;] # 別の指定方法 summary(res_lm)$adj.r.squared # 補正R2 #&gt; [1] 0.8761885 summary(res_lm)$coef #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 32261.46944 3652.405183 8.832938 1.805979e-11 #&gt; area 2397.14374 142.744411 16.793258 3.002396e-21 #&gt; yrs -745.44017 159.275119 -4.680205 2.548844e-05 #&gt; dist -12.44341 1.733012 -7.180222 4.893016e-09 標準化 (偏) 回帰係数 あらかじめ変数を標準化しておいてからlm()を実行すると, 標準化偏回帰係数が得られる. # 標準(化)回帰係数 srentdat &lt;- scale(rentdat) # scale()の返り値はリスト型 → データフレームへ変換 srentdat &lt;- data.frame(srentdat) sres_lm &lt;- lm(rent ~ area + yrs + dist, data = srentdat) summary(sres_lm) #&gt; #&gt; Call: #&gt; lm(formula = rent ~ area + yrs + dist, data = srentdat) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -0.6708 -0.2371 -0.1013 0.2278 0.7231 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -5.959e-16 4.976e-02 0.00 1 #&gt; area 8.456e-01 5.035e-02 16.79 &lt; 2e-16 *** #&gt; yrs -2.360e-01 5.042e-02 -4.68 2.55e-05 *** #&gt; dist -3.616e-01 5.036e-02 -7.18 4.89e-09 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.3519 on 46 degrees of freedom #&gt; Multiple R-squared: 0.8838, Adjusted R-squared: 0.8762 #&gt; F-statistic: 116.6 on 3 and 46 DF, p-value: &lt; 2.2e-16 # 偏回帰係数 vs 標準(化)偏回帰係数 summary(res_lm)[&quot;coefficients&quot;] #&gt; $coefficients #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 32261.46944 3652.405183 8.832938 1.805979e-11 #&gt; area 2397.14374 142.744411 16.793258 3.002396e-21 #&gt; yrs -745.44017 159.275119 -4.680205 2.548844e-05 #&gt; dist -12.44341 1.733012 -7.180222 4.893016e-09 summary(sres_lm)[&quot;coefficients&quot;] #&gt; $coefficients #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -5.958724e-16 0.04976173 -1.197451e-14 1.000000e+00 #&gt; area 8.455702e-01 0.05035176 1.679326e+01 3.002396e-21 #&gt; yrs -2.359937e-01 0.05042380 -4.680205e+00 2.548844e-05 #&gt; dist -3.616101e-01 0.05036197 -7.180222e+00 4.893016e-09 # # 確認 y_sd &lt;- sd(rentdat$rent) x_sd &lt;- apply(rentdat[, -1], 2, sd) res_lm$coef[ -1] * x_sd / y_sd #&gt; area yrs dist #&gt; 0.8455702 -0.2359937 -0.3616101 5.2 変数の選択 データ2: ボストン市内住宅物件価格データ Boston housingデータセットは, Harrison and Rubinfeld (78) で分析に使用された. これは, ボストン地域の住宅に関するデータセットで, もともとは, アメリカ合衆国国勢調査局 (U.S. Census Service) によって収集されたものに基づいている. 今日までに, 統計学・機械学習の教育や研究で広く利用されている. データセットの各行 (レコード) は, ボストン標準大都市統計地域 (Boston Standard Metropolitan Statistical Area, SMSA) 内の1つの国勢調査区（census tract）に対応する. 各国勢調査区は複数の住宅を含む地域単位であるため, 各行は個別の住宅1軒を表すものではない. Harrison, D., &amp; Rubinfeld, D. L. (1978). Hedonic prices and the demand for clean air. Journal of Environmental Economics and Management, 5(1), 81–102. (7/7/25) 変数disの訳がミスリーディングだったため, 訂正いたします. (旧)雇用センター → (新)雇用中心地 (employment centers) - Boston Housingデータセット - crim: 地域の一人当たり犯罪率 - zn: 25,000平方フィート以上の住宅用地の割合 - indus: 地域の非小売業の土地の割合 - chas: チャールズ川のダミー変数 (1: 川沿い, 0: それ以外) - nox: 窒素酸化物濃度（1000万ppm） - rm: 住宅の平均部屋数 - age: 1940年以前に建設された持ち家の割合 - dis: ボストンの5つの雇用中心地 (employment centers) までの距離の加重平均 - rad: 放射状高速道路へのアクセス指数 - tax: 10,000米ドル当たりの固定資産税率 - ptratio: 地域の生徒数・教師数比率 - b: 人種的指標, 1000(B - 0.63)^2, (Bは地域の黒人の割合) - lstat: 低所得者層の割合 - medv: 持ち家住宅の中央値（1000ドル単位） - 506件 x 14変数 (オリジナル版) - 本セクションで使用するバージョンの出所: http://lib.stat.cmu.edu/datasets/boston 注意: Harrison and Rubinfeld (78) の原文には, “employment centers&quot;に関する明確な説明はないものの, `dis`の定義として, “Weighted distances to five employment centers in the Boston region. According to traditional theories of urban land rent gradients, housing values should be higher near employment renters. DIS is entered in logarithm form; the expected sign is negative.”(p.97) とある. また, 不動産市場の文献における“accessibility to employment centers&quot;等の用法を調べる限りにおいて, “employment centers&quot;をいわゆる“職業安定所&quot;と解釈するのは誤りで, むしろ, (ビジネスが集まり労働人口の多い) &quot;雇用の中心地&quot;, “雇用集積地&quot;等と解釈するのが適切と考えられる. # library(MASS) # Bostonデータセット # housing &lt;- Boston # 変数bではなくblack housing &lt;- read.csv(&quot;boston_housing.csv&quot;, header = T) housing %&gt;% head() #&gt; crim zn indus chas nox rm age dis rad tax ptratio b lstat #&gt; 1 0.00632 18 2.31 0 0.538 6.575 65.2 4.0900 1 296 15.3 396.90 4.98 #&gt; 2 0.02731 0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 396.90 9.14 #&gt; 3 0.02729 0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 392.83 4.03 #&gt; 4 0.03237 0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 394.63 2.94 #&gt; 5 0.06905 0 2.18 0 0.458 7.147 54.2 6.0622 3 222 18.7 396.90 5.33 #&gt; 6 0.02985 0 2.18 0 0.458 6.430 58.7 6.0622 3 222 18.7 394.12 5.21 #&gt; medv #&gt; 1 24.0 #&gt; 2 21.6 #&gt; 3 34.7 #&gt; 4 33.4 #&gt; 5 36.2 #&gt; 6 28.7 chasはダミー変数 (0/1) のため, 一旦除去して変数間の相関等を調べる. ライブラリcorrplotの関数corrplot()を使うと, 相関係数のヒートマップを作成することができる. # 散布図行列 pairs(housing[, -4]) # chas (バイナリ) を除去 round(cor(housing[, -4]), 2) # chas(バイナリ)を除去 #&gt; crim zn indus nox rm age dis rad tax ptratio b #&gt; crim 1.00 -0.20 0.41 0.42 -0.22 0.35 -0.38 0.63 0.58 0.29 -0.39 #&gt; zn -0.20 1.00 -0.53 -0.52 0.31 -0.57 0.66 -0.31 -0.31 -0.39 0.18 #&gt; indus 0.41 -0.53 1.00 0.76 -0.39 0.64 -0.71 0.60 0.72 0.38 -0.36 #&gt; nox 0.42 -0.52 0.76 1.00 -0.30 0.73 -0.77 0.61 0.67 0.19 -0.38 #&gt; rm -0.22 0.31 -0.39 -0.30 1.00 -0.24 0.21 -0.21 -0.29 -0.36 0.13 #&gt; age 0.35 -0.57 0.64 0.73 -0.24 1.00 -0.75 0.46 0.51 0.26 -0.27 #&gt; dis -0.38 0.66 -0.71 -0.77 0.21 -0.75 1.00 -0.49 -0.53 -0.23 0.29 #&gt; rad 0.63 -0.31 0.60 0.61 -0.21 0.46 -0.49 1.00 0.91 0.46 -0.44 #&gt; tax 0.58 -0.31 0.72 0.67 -0.29 0.51 -0.53 0.91 1.00 0.46 -0.44 #&gt; ptratio 0.29 -0.39 0.38 0.19 -0.36 0.26 -0.23 0.46 0.46 1.00 -0.18 #&gt; b -0.39 0.18 -0.36 -0.38 0.13 -0.27 0.29 -0.44 -0.44 -0.18 1.00 #&gt; lstat 0.46 -0.41 0.60 0.59 -0.61 0.60 -0.50 0.49 0.54 0.37 -0.37 #&gt; medv -0.39 0.36 -0.48 -0.43 0.70 -0.38 0.25 -0.38 -0.47 -0.51 0.33 #&gt; lstat medv #&gt; crim 0.46 -0.39 #&gt; zn -0.41 0.36 #&gt; indus 0.60 -0.48 #&gt; nox 0.59 -0.43 #&gt; rm -0.61 0.70 #&gt; age 0.60 -0.38 #&gt; dis -0.50 0.25 #&gt; rad 0.49 -0.38 #&gt; tax 0.54 -0.47 #&gt; ptratio 0.37 -0.51 #&gt; b -0.37 0.33 #&gt; lstat 1.00 -0.74 #&gt; medv -0.74 1.00 # pairs(housing) # round(cor(housing), 2) library(corrplot) corrplot(cor(housing[, -4])) # corrplot # corrplot(cor(housing)) # corrplot 4変数に絞り込み res_lm1 = lm(medv ~ crim + rm + tax + lstat, data = housing) summary(res_lm1) #&gt; #&gt; Call: #&gt; lm(formula = medv ~ crim + rm + tax + lstat, data = housing) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -16.383 -3.497 -1.149 1.825 30.716 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -1.414928 3.178364 -0.445 0.6564 #&gt; crim -0.061579 0.035562 -1.732 0.0840 . #&gt; rm 5.248721 0.439664 11.938 &lt;2e-16 *** #&gt; tax -0.005018 0.001922 -2.611 0.0093 ** #&gt; lstat -0.534835 0.050258 -10.642 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.458 on 501 degrees of freedom #&gt; Multiple R-squared: 0.6506, Adjusted R-squared: 0.6478 #&gt; F-statistic: 233.2 on 4 and 501 DF, p-value: &lt; 2.2e-16 anova(res_lm1) #&gt; Analysis of Variance Table #&gt; #&gt; Response: medv #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; crim 1 6440.8 6440.8 216.206 &lt; 2.2e-16 *** #&gt; rm 1 16709.7 16709.7 560.915 &lt; 2.2e-16 *** #&gt; tax 1 1267.3 1267.3 42.542 1.693e-10 *** #&gt; lstat 1 3373.6 3373.6 113.247 &lt; 2.2e-16 *** #&gt; Residuals 501 14924.8 29.8 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # update関数でモデル更新: 変数ptratio追加 res_lm2 &lt;- update(res_lm1, . ~ . + ptratio) summary(res_lm2) #&gt; #&gt; Call: #&gt; lm(formula = medv ~ crim + rm + tax + lstat + ptratio, data = housing) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -14.3602 -3.1111 -0.9237 1.6569 30.4116 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 16.7488084 4.0001180 4.187 3.34e-05 *** #&gt; crim -0.0593795 0.0339830 -1.747 0.0812 . #&gt; rm 4.6349234 0.4292367 10.798 &lt; 2e-16 *** #&gt; tax -0.0008196 0.0019328 -0.424 0.6717 #&gt; lstat -0.5280046 0.0480346 -10.992 &lt; 2e-16 *** #&gt; ptratio -0.8731668 0.1251429 -6.977 9.59e-12 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.215 on 500 degrees of freedom #&gt; Multiple R-squared: 0.6816, Adjusted R-squared: 0.6784 #&gt; F-statistic: 214.1 on 5 and 500 DF, p-value: &lt; 2.2e-16 anova(res_lm2) #&gt; Analysis of Variance Table #&gt; #&gt; Response: medv #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; crim 1 6440.8 6440.8 236.784 &lt; 2.2e-16 *** #&gt; rm 1 16709.7 16709.7 614.301 &lt; 2.2e-16 *** #&gt; tax 1 1267.3 1267.3 46.591 2.540e-11 *** #&gt; lstat 1 3373.6 3373.6 124.026 &lt; 2.2e-16 *** #&gt; ptratio 1 1324.2 1324.2 48.684 9.589e-12 *** #&gt; Residuals 500 13600.6 27.2 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # 変数zn追加 res_lm3 &lt;- update(res_lm2, . ~ . + zn) summary(res_lm3) #&gt; #&gt; Call: #&gt; lm(formula = medv ~ crim + rm + tax + lstat + ptratio + zn, data = housing) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -14.4790 -3.1374 -0.8754 1.6871 30.3185 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 17.3073953 4.0780517 4.244 2.62e-05 *** #&gt; crim -0.0584021 0.0340274 -1.716 0.0867 . #&gt; rm 4.6460026 0.4297290 10.811 &lt; 2e-16 *** #&gt; tax -0.0008832 0.0019358 -0.456 0.6484 #&gt; lstat -0.5354553 0.0491813 -10.887 &lt; 2e-16 *** #&gt; ptratio -0.8958719 0.1291910 -6.934 1.27e-11 *** #&gt; zn -0.0081367 0.0114124 -0.713 0.4762 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.218 on 499 degrees of freedom #&gt; Multiple R-squared: 0.6819, Adjusted R-squared: 0.6781 #&gt; F-statistic: 178.3 on 6 and 499 DF, p-value: &lt; 2.2e-16 anova(res_lm3) #&gt; Analysis of Variance Table #&gt; #&gt; Response: medv #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; crim 1 6440.8 6440.8 236.5506 &lt; 2.2e-16 *** #&gt; rm 1 16709.7 16709.7 613.6973 &lt; 2.2e-16 *** #&gt; tax 1 1267.3 1267.3 46.5455 2.601e-11 *** #&gt; lstat 1 3373.6 3373.6 123.9040 &lt; 2.2e-16 *** #&gt; ptratio 1 1324.2 1324.2 48.6356 9.826e-12 *** #&gt; zn 1 13.8 13.8 0.5083 0.4762 #&gt; Residuals 499 13586.7 27.2 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # 変数nox追加, zn除去 res_lm4 &lt;- update(res_lm3, . ~ . + nox - zn) summary(res_lm4) #&gt; #&gt; Call: #&gt; lm(formula = medv ~ crim + rm + tax + lstat + ptratio + nox, #&gt; data = housing) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -14.2389 -3.1372 -0.9454 1.6680 30.4687 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 17.2649269 4.2731659 4.040 6.18e-05 *** #&gt; crim -0.0596990 0.0340256 -1.755 0.080 . #&gt; rm 4.6382386 0.4297223 10.794 &lt; 2e-16 *** #&gt; tax -0.0004089 0.0022705 -0.180 0.857 #&gt; lstat -0.5216846 0.0514382 -10.142 &lt; 2e-16 *** #&gt; ptratio -0.8844707 0.1294545 -6.832 2.44e-11 *** #&gt; nox -1.0363053 2.9989281 -0.346 0.730 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.22 on 499 degrees of freedom #&gt; Multiple R-squared: 0.6817, Adjusted R-squared: 0.6779 #&gt; F-statistic: 178.1 on 6 and 499 DF, p-value: &lt; 2.2e-16 目的変数medvと説明変数lstatには, 明らかに非線形な関係性が見られる. そこで, lstatに非線形変換を施すことで, 適合度が改善できる可能性がある. pairs(housing[, c(&quot;medv&quot;, &quot;crim&quot;, &quot;rm&quot;, &quot;tax&quot;, &quot;lstat&quot;)]) round(cor(housing[, c(&quot;medv&quot;, &quot;crim&quot;, &quot;rm&quot;, &quot;tax&quot;, &quot;lstat&quot;)]), 2) #&gt; medv crim rm tax lstat #&gt; medv 1.00 -0.39 0.70 -0.47 -0.74 #&gt; crim -0.39 1.00 -0.22 0.58 0.46 #&gt; rm 0.70 -0.22 1.00 -0.29 -0.61 #&gt; tax -0.47 0.58 -0.29 1.00 0.54 #&gt; lstat -0.74 0.46 -0.61 0.54 1.00 # 変数lstatの逆数を新変数invlstatとして定義し, モデルに追加 data2 &lt;- data.frame(housing, invlstat = 1 / housing$lstat) res_lm5 &lt;- lm(medv ~ crim + rm + tax + ptratio + invlstat, data = data2) plot(housing$medv, 1 / housing$lstat) summary(res_lm5) #&gt; #&gt; Call: #&gt; lm(formula = medv ~ crim + rm + tax + ptratio + invlstat, data = data2) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -14.9062 -2.6032 -0.5276 2.1041 31.2592 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 6.665018 3.295520 2.022 0.0437 * #&gt; crim -0.119564 0.030121 -3.969 8.26e-05 *** #&gt; rm 3.609393 0.394880 9.140 &lt; 2e-16 *** #&gt; tax -0.002272 0.001693 -1.342 0.1802 #&gt; ptratio -0.665188 0.114156 -5.827 1.01e-08 *** #&gt; invlstat 60.465938 3.762069 16.073 &lt; 2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 4.719 on 500 degrees of freedom #&gt; Multiple R-squared: 0.7393, Adjusted R-squared: 0.7367 #&gt; F-statistic: 283.6 on 5 and 500 DF, p-value: &lt; 2.2e-16 crimもmedvと非線形な関係があるため, これを適当に非線形変換することで更に改善できる余地がある (各自で試して欲しい). 標準的なモデル選択規準であるAICやBICは, 関数AIC(), BIC()によって計算することができる. # AIC, BICの計算 AIC(res_lm5, res_lm2) #&gt; df AIC #&gt; res_lm5 7 3014.149 #&gt; res_lm2 7 3115.379 BIC(res_lm5, res_lm2) #&gt; df BIC #&gt; res_lm5 7 3043.735 #&gt; res_lm2 7 3144.965 AIC, BIC双方とも, res_lm5はres_lm2より望ましいことを示している. 関数anova()を使って, 分散分析によって (包含関係になる) モデル間の比較をすることができる. # 追加 (除去) した変数群の有意性 (例) anova(res_lm1, res_lm3, test = &quot;F&quot;) # F検定 #&gt; Analysis of Variance Table #&gt; #&gt; Model 1: medv ~ crim + rm + tax + lstat #&gt; Model 2: medv ~ crim + rm + tax + lstat + ptratio + zn #&gt; Res.Df RSS Df Sum of Sq F Pr(&gt;F) #&gt; 1 501 14925 #&gt; 2 499 13587 2 1338.1 24.572 6.636e-11 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(res_lm3, res_lm1, test = &quot;F&quot;) # 実質的に同一 #&gt; Analysis of Variance Table #&gt; #&gt; Model 1: medv ~ crim + rm + tax + lstat + ptratio + zn #&gt; Model 2: medv ~ crim + rm + tax + lstat #&gt; Res.Df RSS Df Sum of Sq F Pr(&gt;F) #&gt; 1 499 13587 #&gt; 2 501 14925 -2 -1338.1 24.572 6.636e-11 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # anova(res_lm1, res_lm2, test = &quot;LRT&quot;) # 尤度比検定 ステップワイズ法による変数選択 ステップワイズ法は, 関数lm()の実行結果オブジェクトを, 関数step()に入力として与えることで実行することができる. # step(); AICによって決定 # scope: モデルサーチの範囲 (追加や削除を検討するべき変数を指定) # scope指定ない場合: # - directionのデフォルトは, 変数減少法 (後方削除) # - モデルサーチ上限 (upper) は, 初期モデル # scope指定ある場合: # - directionのデフォルトは, 変数増減法 # - scopeがリストでなく, 単一式で与えらている場合, upperモデルと解釈 (lowerは欠損) res_lm_all = lm(medv ~ ., data = housing) # → 13変数 res_lm_all_2 = lm(medv ~ 1, data = housing) # → y切片のみ (変数なし) # step(res_lm5) # 変数減少法 (scopeない場合のデフォルト) #&gt; Start: AIC=1576.18 #&gt; medv ~ crim + rm + tax + ptratio + invlstat #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - tax 1 40.1 11175 1576.0 #&gt; &lt;none&gt; 11134 1576.2 #&gt; - crim 1 350.9 11485 1589.9 #&gt; - ptratio 1 756.1 11891 1607.4 #&gt; - rm 1 1860.5 12995 1652.4 #&gt; - invlstat 1 5752.7 16887 1784.9 #&gt; #&gt; Step: AIC=1576 #&gt; medv ~ crim + rm + ptratio + invlstat #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; &lt;none&gt; 11175 1576.0 #&gt; - crim 1 643.8 11818 1602.3 #&gt; - ptratio 1 951.0 12126 1615.3 #&gt; - rm 1 1847.9 13023 1651.4 #&gt; - invlstat 1 6153.6 17328 1796.0 #&gt; #&gt; Call: #&gt; lm(formula = medv ~ crim + rm + ptratio + invlstat, data = data2) #&gt; #&gt; Coefficients: #&gt; (Intercept) crim rm ptratio invlstat #&gt; 6.6395 -0.1399 3.5960 -0.7113 61.4172 #step(res_lm5, direction = &quot;forward&quot;) # 変数増加法 (上限は初期モデル) #step(res_lm5, direction = &quot;both&quot;) # 変数増減法 (上限は初期モデル) # 採用する変数の上限・下限の指定 step(res_lm_all, scope = list(lower = ~ crim + rm)) # 下限のモデルを指定. 変数増減法 #&gt; Start: AIC=1589.64 #&gt; medv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + #&gt; tax + ptratio + b + lstat #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - age 1 0.06 11079 1587.7 #&gt; - indus 1 2.52 11081 1587.8 #&gt; &lt;none&gt; 11079 1589.6 #&gt; - chas 1 218.97 11298 1597.5 #&gt; - tax 1 242.26 11321 1598.6 #&gt; - zn 1 257.49 11336 1599.3 #&gt; - b 1 270.63 11349 1599.8 #&gt; - rad 1 479.15 11558 1609.1 #&gt; - nox 1 487.16 11566 1609.4 #&gt; - ptratio 1 1194.23 12273 1639.4 #&gt; - dis 1 1232.41 12311 1641.0 #&gt; - lstat 1 2410.84 13490 1687.3 #&gt; #&gt; Step: AIC=1587.65 #&gt; medv ~ crim + zn + indus + chas + nox + rm + dis + rad + tax + #&gt; ptratio + b + lstat #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - indus 1 2.52 11081 1585.8 #&gt; &lt;none&gt; 11079 1587.7 #&gt; - chas 1 219.91 11299 1595.6 #&gt; - tax 1 242.24 11321 1596.6 #&gt; - zn 1 260.32 11339 1597.4 #&gt; - b 1 272.26 11351 1597.9 #&gt; - rad 1 481.09 11560 1607.2 #&gt; - nox 1 520.87 11600 1608.9 #&gt; - ptratio 1 1200.23 12279 1637.7 #&gt; - dis 1 1352.26 12431 1643.9 #&gt; - lstat 1 2718.88 13798 1696.7 #&gt; #&gt; Step: AIC=1585.76 #&gt; medv ~ crim + zn + chas + nox + rm + dis + rad + tax + ptratio + #&gt; b + lstat #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; &lt;none&gt; 11081 1585.8 #&gt; - chas 1 227.21 11309 1594.0 #&gt; - zn 1 257.82 11339 1595.4 #&gt; - b 1 270.82 11352 1596.0 #&gt; - tax 1 273.62 11355 1596.1 #&gt; - rad 1 500.92 11582 1606.1 #&gt; - nox 1 541.91 11623 1607.9 #&gt; - ptratio 1 1206.45 12288 1636.0 #&gt; - dis 1 1448.94 12530 1645.9 #&gt; - lstat 1 2723.48 13805 1695.0 #&gt; #&gt; Call: #&gt; lm(formula = medv ~ crim + zn + chas + nox + rm + dis + rad + #&gt; tax + ptratio + b + lstat, data = housing) #&gt; #&gt; Coefficients: #&gt; (Intercept) crim zn chas nox rm #&gt; 36.341145 -0.108413 0.045845 2.718716 -17.376023 3.801579 #&gt; dis rad tax ptratio b lstat #&gt; -1.492711 0.299608 -0.011778 -0.946525 0.009291 -0.522553 step(res_lm_all_2, scope = list(upper = ~ crim + rm)) # 上限のモデルを指定. 変数増減法 #&gt; Start: AIC=2246.51 #&gt; medv ~ 1 #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; + rm 1 20654.4 22062 1914.2 #&gt; + crim 1 6440.8 36276 2165.8 #&gt; &lt;none&gt; 42716 2246.5 #&gt; #&gt; Step: AIC=1914.19 #&gt; medv ~ rm #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; + crim 1 2496.1 19566 1855.4 #&gt; &lt;none&gt; 22062 1914.2 #&gt; - rm 1 20654.4 42716 2246.5 #&gt; #&gt; Step: AIC=1855.43 #&gt; medv ~ rm + crim #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; &lt;none&gt; 19566 1855.4 #&gt; - crim 1 2496.1 22062 1914.2 #&gt; - rm 1 16709.7 36276 2165.8 #&gt; #&gt; Call: #&gt; lm(formula = medv ~ rm + crim, data = housing) #&gt; #&gt; Coefficients: #&gt; (Intercept) rm crim #&gt; -29.2447 8.3911 -0.2649 step(res_lm1, scope = list(upper = ~ crim + rm + tax + lstat + ptratio + b, lower = ~ crim + rm)) #&gt; Start: AIC=1722.43 #&gt; medv ~ crim + rm + tax + lstat #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; + ptratio 1 1324.2 13601 1677.4 #&gt; + b 1 255.6 14669 1715.7 #&gt; &lt;none&gt; 14925 1722.4 #&gt; - tax 1 203.1 15128 1727.3 #&gt; - lstat 1 3373.6 18298 1823.5 #&gt; #&gt; Step: AIC=1677.41 #&gt; medv ~ crim + rm + tax + lstat + ptratio #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; + b 1 306.0 13295 1667.9 #&gt; - tax 1 4.9 13606 1675.6 #&gt; &lt;none&gt; 13601 1677.4 #&gt; - ptratio 1 1324.2 14925 1722.4 #&gt; - lstat 1 3286.7 16887 1784.9 #&gt; #&gt; Step: AIC=1667.9 #&gt; medv ~ crim + rm + tax + lstat + ptratio + b #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - tax 1 3.06 13298 1666.0 #&gt; &lt;none&gt; 13295 1667.9 #&gt; - b 1 306.02 13601 1677.4 #&gt; - ptratio 1 1374.66 14669 1715.7 #&gt; - lstat 1 2849.76 16144 1764.2 #&gt; #&gt; Step: AIC=1666.01 #&gt; medv ~ crim + rm + lstat + ptratio + b #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; &lt;none&gt; 13298 1666.0 #&gt; + tax 1 3.06 13295 1667.9 #&gt; - b 1 307.85 13606 1675.6 #&gt; - ptratio 1 1478.71 14776 1717.4 #&gt; - lstat 1 3001.77 16299 1767.0 #&gt; #&gt; Call: #&gt; lm(formula = medv ~ crim + rm + lstat + ptratio + b, data = housing) #&gt; #&gt; Coefficients: #&gt; (Intercept) crim rm lstat ptratio b #&gt; 11.615006 -0.038921 4.788176 -0.495139 -0.877249 0.009593 # 上限・下限を同時に指定. 変数増減法 # ----------------------------------------# 多重共線性のチェックについて VIF (Variance Inflation Factor) によって, 説明変数間の多重共線性 (マルチコ) の 有無を確認することができる. VIFによる多重共線性への対応に関する慣用ルールとして, 5以上の値を持つ説明変数は要注意, 10以上の変数は除去するのが良いとされている. # VIF # install.packages(&quot;car&quot;) # or RStudio, Tools → Install packages, library(car) # &quot;Companion to Applied Regression&quot; package vif(res_lm_all) # 全13説明変数についてVIFを計算 #&gt; crim zn indus chas nox rm age dis #&gt; 1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 #&gt; rad tax ptratio b lstat #&gt; 7.484496 9.008554 1.799084 1.348521 2.941491 → 変数taxがVIF値が最大 (9.01) と10に近いことから, これを除いてVIF値を再計算してみる. vif(update(res_lm_all, . ~ . - tax)) # VIF値最大のtaxを除いてVIF値を再計算 #&gt; crim zn indus chas nox rm age dis #&gt; 1.791940 2.184240 3.226015 1.058220 4.369271 1.923075 3.098044 3.954446 #&gt; rad ptratio b lstat #&gt; 2.837494 1.788839 1.347564 2.940800 → taxの次に大きかったradのVIF値が大きく減少. 上でみたとおり, もともと, taxとradは相関が高かった (0.91). 線形回帰分析におけるマルチコを考慮した変数選択法としては, 例えば, Kariya, Kurata and Hayashi (2024) の提案した”Empirically Effective Modelling Methodology (EEM-M)“がある. 参考文献 Kariya, Kurata and Hayashi (2024). “A Modelling Framework for Regression with Collinearity”, Journal of Statistical Planning and Inference, 228 (1), Pages 95-115. EEM-Mアプリ 5.3 説明変数に質的変数を含む回帰 データセット#3: 高速道路事故データ - Hoffstedt’s Highway accident data - adt：1日の平均交通量（単位：千台) - trks：総交通量に占めるトラック交通量の割合 - lane：交通の総車線数 - acpt：1マイルあたりのアクセスポイント数 - sigs: 1マイルあたりの信号付きインターチェンジの数 - itg：1マイルあたりの高速道路型インターチェンジの数 - slim：1973年の制限速度 - lwid: 車線幅（フィート単位） - shld: 車道の外側路肩の幅（フィート単位) - htype: 道路の種類または道路の財源を示す指標: &quot;mc&quot;: メジャーコレクター (major collector), &quot;fai&quot;: 州間 (interstate) 高速道路, &quot;pa&quot;: 地域・都市間主要幹線 (principal arterial) 道路, &quot;ma&quot;; 地域・都市内主要幹線 (major arterial) 道路 - rate: 1973年の事故発生率（百万車両マイル当たり） - 注) htypeは4-水準因子 - 参考文献: Weisberg (2014), Applied Linear Regression, 4th Ed., Wiley. library(alr4) data(Highway) str(Highway) #&gt; &#39;data.frame&#39;: 39 obs. of 12 variables: #&gt; $ adt : int 69 73 49 61 28 30 46 25 43 23 ... #&gt; $ trks : int 8 8 10 13 12 6 8 9 12 7 ... #&gt; $ lane : int 8 4 4 6 4 4 4 4 4 4 ... #&gt; $ acpt : num 4.6 4.4 4.7 3.8 2.2 24.8 11 18.5 7.5 8.2 ... #&gt; $ sigs : num 0 0 0 0 0 1.84 0.7 0.38 1.39 1.21 ... #&gt; $ itg : num 1.2 1.43 1.54 0.94 0.65 0.34 0.47 0.38 0.95 0.12 ... #&gt; $ slim : int 55 60 60 65 70 55 55 55 50 50 ... #&gt; $ len : num 4.99 16.11 9.75 10.65 20.01 ... #&gt; $ lwid : int 12 12 12 12 12 12 12 12 12 12 ... #&gt; $ shld : int 10 10 10 10 10 10 8 10 4 5 ... #&gt; $ htype: Factor w/ 4 levels &quot;mc&quot;,&quot;fai&quot;,&quot;pa&quot;,..: 2 2 2 2 2 3 3 3 3 3 ... #&gt; $ rate : num 4.58 2.86 3.02 2.29 1.61 6.87 3.85 6.12 3.29 5.88 ... Highway %&gt;% head() #&gt; adt trks lane acpt sigs itg slim len lwid shld htype rate #&gt; 1 69 8 8 4.6 0.00 1.20 55 4.99 12 10 fai 4.58 #&gt; 2 73 8 4 4.4 0.00 1.43 60 16.11 12 10 fai 2.86 #&gt; 3 49 10 4 4.7 0.00 1.54 60 9.75 12 10 fai 3.02 #&gt; 4 61 13 6 3.8 0.00 0.94 65 10.65 12 10 fai 2.29 #&gt; 5 28 12 4 2.2 0.00 0.65 70 20.01 12 10 fai 1.61 #&gt; 6 30 6 4 24.8 1.84 0.34 55 5.97 12 10 pa 6.87 ライブラリcorrplotの関数corrplot.mixed()を使うと, 相関係数のヒートマップと相関係数の値を同時に表示するプロットを作成することができる. library(corrplot) cor_hw &lt;- cor(cbind(Highway$rate, Highway[, -(11:12)])) # htypeを除去 corrplot.mixed(cor_hw) # OLS回帰 res_lm &lt;- lm(rate ~ ., data = Highway) summary(res_lm) #&gt; #&gt; Call: #&gt; lm(formula = rate ~ ., data = Highway) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -1.99564 -0.62039 -0.05676 0.61741 2.54998 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 13.658212 6.872719 1.987 0.0579 . #&gt; adt -0.004038 0.033945 -0.119 0.9063 #&gt; trks -0.100150 0.114726 -0.873 0.3910 #&gt; lane 0.026675 0.283834 0.094 0.9259 #&gt; acpt 0.066588 0.042569 1.564 0.1303 #&gt; sigs 0.713644 0.525213 1.359 0.1864 #&gt; itg -0.475478 1.282742 -0.371 0.7140 #&gt; slim -0.123778 0.081683 -1.515 0.1422 #&gt; len -0.064751 0.033369 -1.940 0.0637 . #&gt; lwid -0.133813 0.597917 -0.224 0.8247 #&gt; shld 0.014113 0.162174 0.087 0.9313 #&gt; htypefai 0.543592 1.728270 0.315 0.7557 #&gt; htypepa -1.009777 1.105612 -0.913 0.3698 #&gt; htypema -0.548025 0.975623 -0.562 0.5793 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.198 on 25 degrees of freedom #&gt; Multiple R-squared: 0.7605, Adjusted R-squared: 0.636 #&gt; F-statistic: 6.107 on 13 and 25 DF, p-value: 5.733e-05 # 多重共線性のチェック (VIF) # install.packages(&quot;car&quot;) # or RStudio, Tools → Install packages, library(car) # &quot;Companion to Applied Regression&quot; package vif(res_lm) #&gt; GVIF Df GVIF^(1/(2*Df)) #&gt; adt 10.563911 1 3.250217 #&gt; trks 1.931277 1 1.389704 #&gt; lane 3.947949 1 1.986945 #&gt; acpt 4.164627 1 2.040742 #&gt; sigs 2.929007 1 1.711434 #&gt; itg 7.362521 1 2.713397 #&gt; slim 6.041264 1 2.457898 #&gt; len 1.706597 1 1.306368 #&gt; lwid 1.966483 1 1.402313 #&gt; shld 6.417952 1 2.533368 #&gt; htype 28.984452 3 1.752646 データセットHighwayは質的変数 (htype) を含んでいることから, VIFを拡張したGVIF (Generalized VIF) を算出している. GVIFを (変数間で比較できるように) 自由度調整した値GVIF\\(^{1/(2D_f)}\\)は, 量的変数の場合オリジナルのVIFの平方根を取ったものに対応している. そこで, この自由度調整済GVIF\\(^{1/(2D_f)}\\)は, \\(\\sqrt{5} \\approx 2.24\\)越えで要注意, \\(\\sqrt{10} \\approx 3.16\\)越えで除去を検討というのが一つの目安となる ただ, そもそもVIFの5, 10が慣用的な閾値に過ぎないことから, 2.24や3.16といった小数の値まで厳密に評価する合理性は乏しい. そこで, (使いやすくかつ覚えやすくするため) 数字を丸めて, 例えば, 「2までなら安全, 2〜5で要注意, 5越えたら除去を検討する」 等がより実践的である. (自主課題) ステップワイズを実行し, モデルの結果を解釈してみよう. 5.4 説明変数に質的変数を含む回帰 (2) 関数lm()の結果を関数anova()に入力することで, 分散分析表を作成する. 関数anova()は各説明変数 (量的変数, 質的変数どちらについても) ごとに変動性を分割して \\(F\\) 値および \\(p\\) 値を計算する詳細な分散分析表 (ANOVA table) を作成する. また, 二つ以上の包含関係 (ネスト) のある回帰モデルの適合結果オブジェクトを同時に引数として与えることで, 追加 (あるいは除去される) 変数群の持つ有意性を一括して調べることが出来る. データセット#5: 収入データ (仮想) - income.csv - 月収 (万円) - キャリア年数 (年) - 能力試験 (点) - 業種 (A/B) dat1 &lt;- read.csv(&quot;income.csv&quot;) # キャリア年数を説明変数とする単回帰 lm1_mod0 &lt;- lm(月収 ~ キャリア年数, dat = dat1) summary(lm1_mod0) #&gt; #&gt; Call: #&gt; lm(formula = 月収 ~ キャリア年数, data = dat1) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -16.8581 -3.4759 -0.7415 4.5299 13.6303 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 19.8952 1.8298 10.87 &lt;2e-16 *** #&gt; キャリア年数 1.9703 0.1362 14.46 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.495 on 98 degrees of freedom #&gt; Multiple R-squared: 0.681, Adjusted R-squared: 0.6778 #&gt; F-statistic: 209.2 on 1 and 98 DF, p-value: &lt; 2.2e-16 anova(lm1_mod0) # ANOVA表 #&gt; Analysis of Variance Table #&gt; #&gt; Response: 月収 #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; キャリア年数 1 6316.7 6316.7 209.23 &lt; 2.2e-16 *** #&gt; Residuals 98 2958.6 30.2 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # 業種 (質的変数) を説明変数に追加 # 交互作用項なしモデル # lm1_mod1 &lt;- lm(月収 ~ キャリア年数 + 業種, dat = dat1) lm1_mod1 &lt;- update(lm1_mod0, . ~ . + 業種) summary(lm1_mod1) #&gt; #&gt; Call: #&gt; lm(formula = 月収 ~ キャリア年数 + 業種, data = dat1) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -14.4436 -3.5628 -0.6921 3.5165 12.6536 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 20.7222 1.7727 11.690 &lt; 2e-16 *** #&gt; キャリア年数 1.9900 0.1306 15.233 &lt; 2e-16 *** #&gt; 業種B -3.5982 1.1499 -3.129 0.00232 ** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.264 on 97 degrees of freedom #&gt; Multiple R-squared: 0.7103, Adjusted R-squared: 0.7043 #&gt; F-statistic: 118.9 on 2 and 97 DF, p-value: &lt; 2.2e-16 anova(lm1_mod1) # ANOVA表 #&gt; Analysis of Variance Table #&gt; #&gt; Response: 月収 #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; キャリア年数 1 6316.7 6316.7 227.9980 &lt; 2.2e-16 *** #&gt; 業種 1 271.2 271.2 9.7906 0.002317 ** #&gt; Residuals 97 2687.4 27.7 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(lm1_mod1)の出力結果から, この回帰分析のベースラインは 業種Aであることが分かる. すなわち, ２つ目の回帰係数業種Bは, 業種Bに属することによる相対効果, すなわち, 業種BのAに対する目的変数月収の平均値の差分を表している. 具体的には, 回帰係数の切片項の値が \\(20.7222\\) が業種Aの切片となっていて, 一方, 業種Bは \\(20.7222-3.5982=17.1240\\) を切片に持つと読むことができる. 業種の違いによるキャリア年数の影響度 (傾き) の違い を調べるためには, 業種とキャリア年数の2つの項を持つモデルに両者の交互作用項を加え, その交互作用項の有意性 (\\(t\\)値に基づく\\(p\\)値) を確認すれば良い. # 交互作用項有りモデル lm1_mod1_2 &lt;- lm(月収 ~ キャリア年数 * 業種, dat = dat1) summary(lm1_mod1_2) #&gt; #&gt; Call: #&gt; lm(formula = 月収 ~ キャリア年数 * 業種, data = dat1) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -13.2175 -3.7691 -0.8021 3.4916 13.2028 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 19.3523 2.0795 9.306 4.6e-15 *** #&gt; キャリア年数 2.0980 0.1563 13.424 &lt; 2e-16 *** #&gt; 業種B 0.9930 3.8464 0.258 0.797 #&gt; キャリア年数:業種B -0.3537 0.2828 -1.250 0.214 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.248 on 96 degrees of freedom #&gt; Multiple R-squared: 0.7149, Adjusted R-squared: 0.706 #&gt; F-statistic: 80.24 on 3 and 96 DF, p-value: &lt; 2.2e-16 anova(lm1_mod1_2) #&gt; Analysis of Variance Table #&gt; #&gt; Response: 月収 #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; キャリア年数 1 6316.7 6316.7 229.3230 &lt; 2.2e-16 *** #&gt; 業種 1 271.2 271.2 9.8475 0.002259 ** #&gt; キャリア年数:業種 1 43.1 43.1 1.5637 0.214160 #&gt; Residuals 96 2644.3 27.5 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 回帰係数の結果表より, 交互作用項は有意な差があるとは言えない, すなわち, 業種の違いによる回帰係数の差は認められなかった (十分な証拠が得られなかった). 代替的に, 業種とキャリア年数の交互作用ありモデルとなしモデルについてlm()をそれぞれ走らせ, 二つの結果をanova()に同時に与えることで両者の変動性に有意な差があるかを調べても良い. anova(lm1_mod1, lm1_mod1_2) # ANOVA表 #&gt; Analysis of Variance Table #&gt; #&gt; Model 1: 月収 ~ キャリア年数 + 業種 #&gt; Model 2: 月収 ~ キャリア年数 * 業種 #&gt; Res.Df RSS Df Sum of Sq F Pr(&gt;F) #&gt; 1 97 2687.4 #&gt; 2 96 2644.3 1 43.073 1.5637 0.2142 先述の交互作用項の\\(t\\)値に基づいた\\(p\\)値と等価な結果が得られた. lm1_mod2 &lt;- update(lm1_mod1, . ~ . + 能力試験) summary(lm1_mod2) #&gt; #&gt; Call: #&gt; lm(formula = 月収 ~ キャリア年数 + 業種 + 能力試験, #&gt; data = dat1) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -14.8763 -3.9926 -0.6659 3.7814 12.0713 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 23.92898 3.52028 6.797 9e-10 *** #&gt; キャリア年数 1.98691 0.13060 15.214 &lt; 2e-16 *** #&gt; 業種B -3.77837 1.16192 -3.252 0.00158 ** #&gt; 能力試験 -0.06343 0.06017 -1.054 0.29444 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.261 on 96 degrees of freedom #&gt; Multiple R-squared: 0.7136, Adjusted R-squared: 0.7046 #&gt; F-statistic: 79.72 on 3 and 96 DF, p-value: &lt; 2.2e-16 anova(lm1_mod2) # ANOVA表 #&gt; Analysis of Variance Table #&gt; #&gt; Response: 月収 #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; キャリア年数 1 6316.7 6316.7 228.2596 &lt; 2e-16 *** #&gt; 業種 1 271.2 271.2 9.8018 0.00231 ** #&gt; 能力試験 1 30.8 30.8 1.1113 0.29444 #&gt; Residuals 96 2656.6 27.7 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 次に, 関数anova()により, ネスト関係にある二つのモデルlm1_mod0, lm1_mod2を比較する. # モデル比較 # anova(lm1_mod0, lm1_mod1, lm1_mod2) anova(lm1_mod0, lm1_mod2) #&gt; Analysis of Variance Table #&gt; #&gt; Model 1: 月収 ~ キャリア年数 #&gt; Model 2: 月収 ~ キャリア年数 + 業種 + 能力試験 #&gt; Res.Df RSS Df Sum of Sq F Pr(&gt;F) #&gt; 1 98 2958.6 #&gt; 2 96 2656.6 2 302 5.4566 0.005695 ** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 二つの変数, 業種, 能力試験は一括して, 偶然 (サンプリング・エラー) とはみなせないような体系的な (有意な) 変動を持つことを示している. すなわち, 業種と能力試験は説明変数に加えておいた方が良いと判断される. さらに, モデル選択規準AICによっても, これらを説明変数に持つlm1_mod2の方が望ましいことを示している. AIC(lm1_mod0, lm1_mod2) #&gt; df AIC #&gt; lm1_mod0 3 628.5190 #&gt; lm1_mod2 5 621.7522 “コントラスト”の設定変更* 関数options()のパラメータの一つであるコントラスト (contrasts) を変えることで, パラメータの持つ意味が, すなわち, 解釈が変わる. Rのデフォルトは, 処置対比 (“contr.treatment”). #options(&quot;contrasts&quot;) #options(&quot;contrasts&quot; = c(&quot;contr.treatment&quot;, &quot;contr.poly&quot;)) # デフォルト #options(&quot;contrasts&quot; = c(&quot;contr.sum&quot;, &quot;contr.poly&quot;)) 零和対比 (“contr.sum) に変更した場合について結果を, 上と比較してみよう. options(&quot;contrasts&quot; = c(&quot;contr.sum&quot;, &quot;contr.poly&quot;)) lm1_mod2_2 &lt;- update(lm1_mod1, . ~ . + 能力試験) summary(lm1_mod2_2) #&gt; #&gt; Call: #&gt; lm(formula = 月収 ~ キャリア年数 + 業種 + 能力試験, #&gt; data = dat1) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -14.8763 -3.9926 -0.6659 3.7814 12.0713 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 22.03980 3.45054 6.387 6.02e-09 *** #&gt; キャリア年数 1.98691 0.13060 15.214 &lt; 2e-16 *** #&gt; 業種1 1.88919 0.58096 3.252 0.00158 ** #&gt; 能力試験 -0.06343 0.06017 -1.054 0.29444 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.261 on 96 degrees of freedom #&gt; Multiple R-squared: 0.7136, Adjusted R-squared: 0.7046 #&gt; F-statistic: 79.72 on 3 and 96 DF, p-value: &lt; 2.2e-16 ここでは, 回帰係数業種1が業種Aに対応, 一方, 業種2 (業種B) は省略されている. 零和条件から, 業種2の係数は\\(-1.88919\\)であることが分かる. すなわち, 切片項の値が \\(22.03980\\) であることから, 業種Aの切片の値は, \\(22.03980+1.88919=23.92899\\), 一方, Bは, \\(23.92899-1.88919=20.15061\\) となると読める. すなわち, 先のデフォルトの処置対比の場合の切片の値と一致していることが確認される. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
