[["index.html", "ビジネス統計 1 はじめに 1.1 コースの目的 1.2 統計学/統計的手法の学習について 1.3 ビジネス応用における統計学の最近の趨勢 1.4 本書内の記載の注意点", " ビジネス統計 林 高樹 2025-12-09 (内容は随時更新されます) 1 はじめに 1.1 コースの目的 近年の”データサイエンス”分野の発展の中で, その支柱の分野の一つとしての 統計学の重要性が高まっている. いまや, 簡単なデータ分析であれば, ChatGPTなどの生成AIが, 人間に代わって分析を行ったり, 分析のためのプログラムを書いたりしてくれる時代となった. しかしながら, 少なくとも現状の技術水準では生成AIの出力の 正確性は保証されておらず, 生成AIの出力の正しさを確認できるのは人間である. 当面は, 人間の手による分析, 人間の頭による分析方法論の正しい理解や結果の解釈が求められるだろう. AIブームより前に始まっていた”ビッグデータ”の時代において, 多種多様かつ大量の, 組織内・外のデータを分析する技術は 会社経営において決定的に重要となっている. 一方, 統計学やデータサイエンスの一人の初学者としては, いきなり 高度なITスキルを前提とする最新の機械学習系手法を駆使したビッグデータ“解析を行おうとするのでなく, まずは, 古典的な統計的方法論を正しく“スモール・データセット”に応用できるようになることが重要である. 本コースは, 多変量解析を中心にさまざまな統計的データ分析の手法を学び, これらの手法を学術研究や実務に応用できるようになるための基盤作り目指す. 統計ソフトウェアRを利用しながら学んでいく. 1.2 統計学/統計的手法の学習について 学習の目標 統計学/統計的手法は, サイエンスとアートの二つの側面があることから, 的確な応用を行うためには, 両面を同時に, バランスよく学ぶ必要がある. 統計理論を正確に理解する 数学的に正しい概念や手続きの理解 統計手法の実践 (運用法) を学ぶ 業務経験や知識, 統計的手法の経験則, 費用対効果等の判断 ※ 統計学は数学(の一分野)ではない (数学を道具として学問体系が作られいる) 一方の理解が不十分であると, 妥当な分析が行えず, 不正確あるいは間違った分析結果や解釈につながるリスクがある. 統計学/統計的手法の学習方法 3つの要素・ルート, それに対応した教科書・参考書がある. すなわち, 入門 (文章&amp;図表主体): 手法の概念, 用途, 特徴の大雑把な理解を図る 理論 (数式主体): 手法の理論的・技術的側面, 詳細の正確な理解を図る 実習 (コード主体): 手を動かすことで手法を経験, 実践力をつける 統計学を学習するにあたっては, 理解の段階に応じて, これらの要素を, 少しずつ万遍なく学びながら, “スパイラル”状に次の段階に進んでいくのが最も効果的であると筆者は考える. すなわち, 理論の学習を全くやらず, 入門と実習のみを学習するようなアプローチは, 理論的理解のないまま統計的分析を実践することになるため危険である. 筆者の経験上, プログラミングの得意な”エンジニア系”のデータサイエンティストにはそのような傾向を持つ人が少なからずいると感じている. 書籍ごとに目的や想定する読者層は異なり, それに対応するようにこれらの要素の割合が異なる. したがって, 学習者は自身の学習目的に照らして適切な本を選択する必要がある. 本書では, Rを用いながら代表的な統計手法を学んでいく. 統計学の教科書例 Rコードによる分析例を示しながら, 各手法や理論の解説を行っている書籍は 多数存在するが, バランス良くこれらを配置していると筆者が感じる教科書のタイトルを幾つか紹介する. 【統計学/R】 山田剛史, 杉澤武俊, 村井潤一郎 (2008), Rによるやさしい統計学, オーム社 【データ分析/R】 Kosuke Imai (2017), Quantitative Social Science: An Introduction, Princeton University Press (今井耕介(著), 粕谷祐子, 原田勝孝, 久保浩樹 (訳) (2018), 社会科学のためのデータ分析入門(上)(下), 岩波書店) 【機械学習/R】 R. James, G., Witten, D., Hastie, T., Tibshirani (2013), An Introduction to Statistical Learning: with Applications in R, Wiley. (James他(著), 落海浩, 首藤信通 (訳) (2018), Rによる統計的学習入門, 朝倉書店) 参考として, 次の書籍は, コードを載せずまた数式を使った説明も殆どなしに, 文章主体で (計量経済学の) 手法の概念や分析結果の解釈の仕方を平易に説明している良書である. 山本 勲 (2015), 実証分析のための計量経済学, 中央経済社 統計学/統計的手法の学習ステップ 入門・初級ステップ - レベル①: ソフトウェアを正しく動かせる - 目的に応じた適切な手法の選択 - 適切なデータの加工, ソフトウェアの操作 - 出力結果 (帳票, 図表) の正しい見方 - レベル②: 手法の背後にある理論を理解する - (②A) 概念や定義の正しい理解 (言葉やイメージ) - (②B) 数式による厳密な理解 ※ ①の達成度を高めるためには, ②の理解を高める必要 ① ⇒ ② ⇔ ① 中級ステップ - 特定の分野における (計量経済学, 心理学, 疫学, …) 統計的手法の理解と実践が出来るようになる 1.3 ビジネス応用における統計学の最近の趨勢 統計学, さらには中核分野として内包するデータサイエンス分野において扱う 対象データの特徴として以下のような傾向がみられる 大規模化 (“ビッグデータ”) レコード数 n → 大, 変数の数 p → 大 データ数より説明変数が多い場合も （“n&lt;p問題”) 従来の統計学: 「n 小・中規模」, かつ, 「n&gt;p」 高頻度・高速化 (従来) 四半期・月次… → 1日内, 秒, ミリ秒, …, リアルタイム 非構造化 画像, 音声, テキスト等 自動化 衛星画像, アクセスログ, IoTデータ, ウェアラブル・データ, … “マルチモーダル”化 テキスト・画像・音声・動画など複数の種類のデータを一括して処理 (AIによる)自動生成 一方, 経営(学)分野への応用の観点では次のような傾向がある. 文章や発言内容の自然言語処理・テキスト解析技術の重要性の高まり BERT, GPT-4, … 生成系AI技術の活用 文章 (ChatGPT, BARD), 画像 (DALLE, Stable Solution), 音楽 (Stable Audio, Suno), 等 複数のデータソースの有機的な組合せ活用の重要性 財務諸表等の“ハードデータ” × SNS等から得られた“ソフトデータ” 外部ソース・データ ×社内業務データ … 1.4 本書内の記載の注意点 読者への注) パス名は、各自のPC環境に応じて適宜変更すること "],["r言語の基本.html", "2 R言語の基本 2.1 Rの基本プログラミング 2.2 データの型や構造 2.3 データの操作・演算 2.4 R関数 2.5 データの可視化 2.6 ファイル入出力 2.7 パッケージtidyverse", " 2 R言語の基本 2.1 Rの基本プログラミング 主な参考文献： 金 (2017),『Rによるデータサイエンス』, 森北出版. 山田他 (2008),『Rによるやさしい統計学』, オーム社. Venables, Smith, and R Development Core Team (03), R入門. http://minato.sip21c.org/swtips/R-jp-docs/R-intro-170.jp.pdf R Core Team (2024). R Language Definition, R Foundation for Statistical Computing. https://cran.r-project.org/doc/manuals/r-release/R-lang.pdf 本コースは, 基本的なRプログラミングにもっぱら限定 よりモダンなプログラミング (本コース終了後) → tidyverse 例. 松村他, 『RユーザーのためのRStudio[実践]入門]』, 技術評論社 Rコーディングスタイルの例 Google, “Google’s R Style Guide”, https://google.github.io/styleguide/Rguide.html Hadley Wickham, “Tidyverse Style Guide”, https://style.tidyverse.org/ 2.1.1 基本操作 数値 (ベクトル), 演算の直接評価 2 + 3 ## [1] 5 c(1, 2, 3, 4) ## [1] 1 2 3 4 1:4 ## [1] 1 2 3 4 変数xに値を格納. 変数xに対する演算 - 基本形: 変数名 &lt;- 代入する値 x &lt;- c(1, 2, 3, 4, 5) x = c(1, 2, 3, 4, 5) x ## [1] 1 2 3 4 5 (x &lt;- c(1, 2, 3, 4, 5)) # 代入と表示を同時に実行 ## [1] 1 2 3 4 5 x^2 ## [1] 1 4 9 16 25 x**2 ## [1] 1 4 9 16 25 xに関数を適用 mean(x) ## [1] 3 var(x) ## [1] 2.5 sd(x) ## [1] 1.581139 summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1 2 3 3 4 5 # sqrt, summary, ... その他, R言語の基本 - 空白は無視される - Pythonと異なり, インデントは意味を持たない - 一行に二つのコマンドを入力する場合は, 間をセミコロン (;) で区切る - 行頭がシャープ (#) で始まる行は丸々無視される (コメント行) - 中括弧 {} は通常、コードのブロックを作成するために使用. 主に, 条件文、ループ、関数などのブロック構造を定義する際に使用. 2.1.2 基本構文 Rでは, データの加工や分析を行う際などに, 分析者自らの手で処理の手順をプログラミングをすることができる. forループ # 繰り返し処理 (forループ) for (変数名 in 変数のリスト){ 1回分の処理内容 } ``` #### if文 {-} ``` # 条件分岐 (if文) # if (条件式) 処理1 else 処理2 x &lt;- 0 for(i in 1:10){x &lt;- x + i} x # aaa &lt;- c(1, 3, 5) for (a in aaa) print (a) ## [1] 55 ## [1] 1 ## [1] 3 ## [1] 5 2.1.3 自作関数 同様な処理を”パラメータ”を変えながら何度も実行する場合は, 関数を作っておくと便利である. ※ 関数に付与する名前として, Rですでに使われている関数名や, Rで特別な意味を持つ値 (T, Fなど)は避けること # 自作関数の作成 関数名 &lt;- function(引数1, 引数2, ...){ 処理内容 } myfunc &lt;- function(y){ x &lt;- 0 for (i in 1:y) x &lt;- x + i return(x) } # 実行例 myfunc(10) myfunc2 &lt;- function(y){ if(y &gt; 10) print(&quot;yes&quot;) else print(&quot;no&quot;) } # 実行例 myfunc2(5) ## [1] 55 ## [1] &quot;no&quot; 2.1.4 パッケージのインストール &amp; 読み込み #lda # lda関数 → このままだエラー発生 library() # インストール済パッケージ一覧 search() # 読み込み済みパッケージ一覧 library(MASS) # MASSパッケージの読み込み(ロード) search() # アタッチされたパッケージのリスト表示 lda install.packages(&quot;DAAG&quot;) # http://cran.r-project.org # http://cran.r-project.org/web/packages/googleVis/index.html # パッケージインストローラー 2.1.5 ヘルプ 関数のヘルプ R関数helpを使うか, RStudioのプルダウンメニューやHelpペインを使用 help(&quot;fivenum&quot;) # 関数fivenumのヘルプ ?fivenum 2.2 データの型や構造 ここで, R言語の基礎を理解するのに重要な二つの概念について, 初心者を念頭に正確性を犠牲にしながら概要について述べる. 実際はここでの記載よりもはるかに複雑で, 技術的にも難易度が高い. 包括的かつ技術的に正確な内容については, 例えば, https://adv-r.hadley.nz を参照されたい. 2.2.1 データの値の種類 (“データ型”) Rでは, データの取る値の主要な種類 (type) として, 実数型 (double), 整数型 (integer), 文字列型 (character), 論理型 (logical) がある. また, 実数型, 整数型はまとめて数値型 (numeric) とも呼ばれる. 初心者は, 実数型と整数型の違いは気にしなくても良い. # 実数型 3.14 2.718 # 整数型 1L 5L # 文字列型 &quot;KBS&quot; &quot;日吉&quot; # 論理型 TRUE # または, T FALSE # または, F 次に, データの値の種類として, 上記以外に応用上知っておきたいものとして, 因子型 (factor),日付型 (Date)がある. 因子型は, カテゴリーデータに対して, 日付型は日付や時刻を表すデータに対して使うことができる. Rでは, カテゴリーデータ (ベクトル) を因子型としてオブジェクトに格納しておけば, その後の統計分析においてわざわざダミー変数を作る操作は (おおむね) 不要となる. また, 日付型として格納したデータは日付や時間に関する処理において効果を発揮する. 与えられたデータに対して, R組み込み関数である factor(), as.Date() を適用することでこれらの型に変換することができる. 少しだけ発展的な内容になるが, 因子型は整数型を値に持つベクトル, 日付型は実数型を値に持つベクトルとしてR内部で扱われる (ベクトルやオブジェクトについては次に述べる). # 因子型 factor(c(&quot;L&quot;, &quot;M&quot;, &quot;H&quot;, &quot;M&quot;, &quot;L&quot;, &quot;M&quot;)) # L/M/Hの3水準の因子型ベクトル (長さ5) # 日付型 as.Date(&quot;2023-10-02&quot;) 2.2.2 データの配列の仕方 (“データ構造”) Rでは, データの配置の仕方の種類の主要なものとして, ベクトル (vector), リスト (list), 行列 (matrix), 配列(array), データフレーム (data frame) などがある. 分析に応じて, 適切なデータの構造にして処理を行う必要がある. # ベクトル c(3.14, 2.718) c(&quot;KBS&quot;, &quot;日吉&quot;) # リスト list(&quot;KBS&quot;, 1962L, 1:10) # 行列 matrix(1:8, nrow = 2, byrow = T) # 配列 arra(1:12, c(2, 3, 2)) # データフレーム data.frame(name = c(&quot;Steve&quot;, &quot;Top&quot;), income = c(40000, 50000)) ちなみに, これらの”データ構造”には階層関係があり, 行列や配列はベクトルの特別な場合, リストはベクトルの特別な場合, データフレームはリストの特別な場合である. データフレームは, リスト (異なる種類のデータを同時に要素として持つ) でありながら, リストの各要素 (ベクトル) の長さが等しく, 2次元の行列の形式にデータが並べられたものである. R言語では, ベクトルが最も基本的な”データ構造”である. Rを用いた統計分析では, データフレームを用いるケースが非常に多いため, データフレームを使えるようになることが必須である. Rで分析を行う場合には, データや関数 (データ処理するための手続きを書いたコード) をオブジェクト (object) と呼ばれる”箱”に名前を付けて一旦格納し, その名前を呼び出す形で処理を実行するのが便利である. 量的変数や質的変数を同時に持つデータセットの分析には, データフレームが便利である. オブジェクトにはクラス (class) というオブジェクトの持つデータ構造の種類の属性が付与される. なお, Rには, type, class, modeの3つの”型”が存在し混乱しやすい. 初心者は違いを気にする必要はなく, 大雑把に, 上の”データ型”は関数 typeof(), “データ構造”は関数 class() により調べることができると知っていれば十分である. 興味のある読者は以下を参照: https://cran.r-project.org/doc/manuals/r-release/R-lang.html#Attributes 特別な値 Rにおける分析において注意や対処が必要な, データの取り得る特別な値として, - NA (欠損値) - NULL (非存在) - NaN (非数値) - Inf (無限大) これらの値をテストする関数が用意されている. is.na() is.null() is.nan() is.infinite() is.finite() # NAの含まれている例 x &lt;- c(1, NA, 3, 4, 5) x == NA ## [1] NA NA NA NA NA is.na(x) ## [1] FALSE TRUE FALSE FALSE FALSE mean(x) ## [1] NA mean(x, na.rm = T) ## [1] 3.25 # NULLの含まれている例 x &lt;- 1:5 names(x) &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;) x ## a b c d e ## 1 2 3 4 5 names(x) &lt;- NULL x ## [1] 1 2 3 4 5 # NaN, Infの発生例 0 / 0 ## [1] NaN 1 / 0 ## [1] Inf 上の”データ型”や”データ構造”を調べる関数も用意されている. # 整数値を持つ行列の例 abc &lt;- matrix(1:8, nrow = 2, byrow = T) is.numeric(abc) ## [1] TRUE is.integer(abc) ## [1] TRUE is.matrix(abc) ## [1] TRUE typeof(abc) ## [1] &quot;integer&quot; class(abc) ## [1] &quot;matrix&quot; &quot;array&quot; mode(abc) ## [1] &quot;numeric&quot; str(abc) ## int [1:2, 1:4] 1 5 2 6 3 7 4 8 2.2.3 ベクトル # 以下は, 互いに等価 aaa &lt;- c(2, 4, 6, 8) # 変数aaaに数値ベクトル(2,4,6,8)を割り当てる aaa = c(2, 4, 6, 8) aaa = seq(2, 8, 2) c(2, 4, 6, 8) -&gt; aaa aaa &lt;- 1:4 * 2 # assign(&quot;aaa&quot;, c(2, 4, 6, 8)) # 値を割り当てる際に, &quot;環境&quot;を指定することができる # ベクトルの長さ length(aaa) # 文字列ベクトル bbb &lt;- c(&quot;東京&quot;, &quot;埼玉&quot;, &quot;千葉&quot;, &quot;神奈川&quot;) # ベクトルの各要素に名前 (ラベル) を付与 names(aaa) &lt;- bbb # ベクトル要素の取り出し bbb[1] bbb[c(2, 4)] bbb[c(T, F, T, F)] # インデックスの値がT (TRUE) の要素の取り出し bbb[c(T, F, F)] # 注意 ## [1] 4 ## [1] &quot;東京&quot; ## [1] &quot;埼玉&quot; &quot;神奈川&quot; ## [1] &quot;東京&quot; &quot;千葉&quot; ## [1] &quot;東京&quot; &quot;神奈川&quot; 2.2.4 行列 matrix(0, 3, 4) # 全要素0の3x4-行列 matrix(0:4, 3, 4) # 行列の値に使うベクトル (0:4) の長さと 行数 (3)・列数 (4) が不一致 ## Warning in matrix(0:4, 3, 4): data length [5] is not a sub-multiple or multiple ## of the number of rows [3] ## [,1] [,2] [,3] [,4] ## [1,] 0 0 0 0 ## [2,] 0 0 0 0 ## [3,] 0 0 0 0 ## [,1] [,2] [,3] [,4] ## [1,] 0 3 1 4 ## [2,] 1 4 2 0 ## [3,] 2 0 3 1 ccc &lt;- matrix(c(3, 2, 1, 6, 5, 4), 2, 3) # 2x3-行列 ccc[1, 1] # (1, 1)成分 ## [1] 3 ccc[1, ] # 第1行(行ベクトル) ## [1] 3 1 5 ccc[, 1] # 第1列(列ベクトル) ## [1] 3 2 ccc[-2, ] # 2行目を除く → 2x4-行列 ## [1] 3 1 5 ccc[, -2] # 2列目を除く → 3x3-行列 ## [,1] [,2] ## [1,] 3 5 ## [2,] 2 4 dim(ccc); nrow(ccc); ncol(ccc) # セミコロン(;)により, 複数のコマンドを1行に収め, 順次実行 ## [1] 2 3 ## [1] 2 ## [1] 3 ccc[2, 3] &lt;- 10 # (2, 3)成分に値10を代入 # 行列にラベルを付与 colnames(ccc) &lt;- c(&quot;大阪&quot;, &quot;京都&quot;, &quot;名古屋&quot;) # 列ラベル rownames(ccc) &lt;- c(&quot;2012&quot;, &quot;2013&quot;) # 行ラベル ccc ## 大阪 京都 名古屋 ## 2012 3 1 5 ## 2013 2 6 10 t(ccc) # 転置 ## 2012 2013 ## 大阪 3 2 ## 京都 1 6 ## 名古屋 5 10 2.2.5 リスト ベクトル, 行列, 配列, リスト等の異なる型(&amp;異なる長さ)のオブジェクトを一つにまとめたオブジェクト L1 &lt;- list(rep(&quot;A&quot;, 3), 1:0, matrix(1:8, 2, 4)) L1[[1]] # 1番目の要素(変数)の取り出し k &lt;- list (name = &quot;Taro&quot;, salary = 50000, male = T) k2 &lt;- list (&quot;Taro&quot;, 50000, T) # 要素名 (タグ)なしの場合 k$sal # 要素名は省略形可 # リストはベクトルの一種 (recursive vector) # 一方, 通常のベクトルはatomic vector (それ以上分解できない) # vector()からリスト生成する場合 z &lt;- vector (mode = &quot;list&quot;) z[[&quot;abd&quot;]] &lt;- 5 k[1:2] # 元のリストの部分リスト k2 &lt;- k[2] class(k2); str(k2) k2a &lt;- k[[2]] # 2番目の要素(変数)の取り出し (要素の型を持つ結果を返す) # k[[1:2]] # --&gt; エラー class(k2a); str(k2a) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; ## [1] 50000 ## $name ## [1] &quot;Taro&quot; ## ## $salary ## [1] 50000 ## ## [1] &quot;list&quot; ## List of 1 ## $ salary: num 50000 ## [1] &quot;numeric&quot; ## num 50000 リストの要素追加・削除 z &lt;- list(a = &quot;abcd&quot;, b = 10) z$c &lt;- &quot;piano&quot; z[[4]] &lt;- 15 z[5:6] &lt;- c(TRUE, FALSE) z$b &lt;- NULL # xxx &lt;- 1:10 yyy &lt;- 0.5 * xxx + rnorm(10) lm_res &lt;- lm(yyy ~ xxx) is.list(lm_res) lm_res[[1]] lm_res$coef lm_res[&quot;coefficients&quot;] ## [1] TRUE ## (Intercept) xxx ## -0.4724826 0.5009129 ## (Intercept) xxx ## -0.4724826 0.5009129 ## $coefficients ## (Intercept) xxx ## -0.4724826 0.5009129 2.2.6 データフレーム リストの特別な場合 長さが等しい複数のベクトルを要素に持つリスト 数値と文字列などの異なるデータが混在するデータを行列のように扱える Rにおける様々な統計分析において多用される kids &lt;- c(&quot;taro&quot;, &quot;hanako&quot;) ages &lt;- c(10, 8) d &lt;- data.frame(kids, ages, stringsAsFactors = FALSE) # 注: stringsAsFactors = T: 文字ベクトルをfactorとして扱う d str(d) # 以下の3つは等価な操作 d[[1]] # データフレームの第1列 (リストの一番目の要素) を取り出す(→ 文字列ベクトル) d$kids # 変数(kids)のように取り出す d[, 1] # 行列のように操作 (--&gt; 便利) # ただし, d[1] # 第1列をデータフレーム (リスト) として取り出す df1 &lt;- data.frame(letters[1:3], 3:1) rownames(df1) &lt;- c(&quot;大阪&quot;, &quot;京都&quot;, &quot;名古屋&quot;) colnames(df1) &lt;- c(&quot;方言種類&quot;, &quot;順位&quot;) class(df1) is.vector(df1) ## kids ages ## 1 taro 10 ## 2 hanako 8 ## &#39;data.frame&#39;: 2 obs. of 2 variables: ## $ kids: chr &quot;taro&quot; &quot;hanako&quot; ## $ ages: num 10 8 ## [1] &quot;taro&quot; &quot;hanako&quot; ## [1] &quot;taro&quot; &quot;hanako&quot; ## [1] &quot;taro&quot; &quot;hanako&quot; ## kids ## 1 taro ## 2 hanako ## [1] &quot;data.frame&quot; ## [1] FALSE 2.3 データの操作・演算 2.3.1 ベクトルの結合, ソート vec1 = 1:4 vec2 = 2:5 rbind(vec1, vec2) # ベクトルの行方向への結合 cbind(vec1, vec2) # べクトルの列方向への結合 vec3 &lt;- c(2, 5, 1, 3) sort(vec3) # 昇順 rev(vec3) # 順番を逆転させる ccc[, order(ccc[&quot;2012&quot;, ])] ccc[, sort.list(ccc[&quot;2012&quot;, ])] ## [,1] [,2] [,3] [,4] ## vec1 1 2 3 4 ## vec2 2 3 4 5 ## vec1 vec2 ## [1,] 1 2 ## [2,] 2 3 ## [3,] 3 4 ## [4,] 4 5 ## [1] 1 2 3 5 ## [1] 3 1 5 2 ## 京都 大阪 名古屋 ## 2012 1 3 5 ## 2013 6 2 10 ## 京都 大阪 名古屋 ## 2012 1 3 5 ## 2013 6 2 10 2.3.2 二項演算 x &lt;- c(1, 3, 5, 2); y &lt;- c(-3, 1, -1, -2) x + y x * y x / y x ^ 2 x &lt; y ## [1] -2 4 4 0 ## [1] -3 3 -5 -4 ## [1] -0.3333333 3.0000000 -5.0000000 -1.0000000 ## [1] 1 9 25 4 ## [1] FALSE FALSE FALSE FALSE 2.3.3 論理演算 lx &lt;- c(T, T, F); ly &lt;- c(F, F, F) lx &amp; ly lx &amp;&amp; ly # 最初の要素間の論理演算が成り立つと, 以降の演算は行わない lx | ly lx || ly # 最初の要素間の論理演算が成り立つと, 以降の演算は行わない 2.3.4 条件式 # ==, &gt;, &lt;, &gt;=, &lt;= # &amp;&amp;, || 2.3.5 行列演算 A &lt;- matrix(c(1, 2, 3, 4, 5, 6), 3, 2) B &lt;- matrix(c(2, 1, -1, -2), 2, 2) A %*% B # 行列の積 # diag # 対角行列 # solve # 逆行列 ## [,1] [,2] ## [1,] 6 -9 ## [2,] 9 -12 ## [3,] 12 -15 2.4 R関数 2.4.1 数学基本関数 # sum; sqrt; abs # exp; log; log10; log2; sin; cos # round; ceiling; floor 2.4.2 基本統計量の計算 # mean, max; min; range; median; quantile # var; sd # summary # table # cov; cor 統計 &lt;- c(rep(&quot;好き&quot;, 8), rep(&quot;嫌い&quot;, 7)) 数学 &lt;- c(rep(&quot;好き&quot;, 6), rep(&quot;嫌い&quot;, 9)) table(統計, 数学) # クロス集計表, ベクトルは同一長 ## 数学 ## 統計 好き 嫌い ## 好き 6 2 ## 嫌い 0 7 2.4.3 確率分布 # dxxx(q) # 確率密度, q:確率点 # pxxx(q) # 累積確率, q:確率点 # qxxx(p) # 確率点, p:確率 # rxxx(n) # 乱数, n:個数 # ------------------------------------------------------------------ # xxx部分: # unif(x, min, max) # 一様分布 # norm(x, mean, sd) # 正規分布 # exp(x, rate) # 指数分布 # binom(x, size, prob) # ２項分布 # pois(x, lambda) # ポアソン分布 # t(x, df) # t分布 # chisq(x, df) # カイ2乗分布 # f(x, df1, df2) # F分布 curve(xを含んだ式, from = xの左端点, to = xの右端点) # 関数のグラフ描画 curve(dnorm(x, mean = 0, sd = 1), from = -4, to = 4) curve(dnorm(x, mean = 1, sd = 2), from = -4, to = 4, add = T) # 問：t分布(自由度4)の形状は? 2.4.4 その他便利な関数 # sweep # scale # ifelse ifelse(統計 == &quot;好き&quot;, 1, 0) # apply (X, MARGIN, FUN, ...) apply(ccc, 1, sum) apply(ccc, 2, sum) colMeans(ccc) rowMeans(ccc) ## [1] 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 ## 2012 2013 ## 9 18 ## 大阪 京都 名古屋 ## 5 7 15 ## 大阪 京都 名古屋 ## 2.5 3.5 7.5 ## 2012 2013 ## 3 6 2.5 データの可視化 # 棒グラフ barplot(ccc) barplot(ccc, beside = T) barplot(ccc, beside = T, col = c(&quot;lightblue&quot;, &quot;lavender&quot;), main = &quot;test&quot;) # apply(ccc, 1, pie) # pie # hist # 折れ線グラフ (行列の各列(変数)の同時プロット) matplot(ccc, type = &quot;l&quot;) matplot(t(ccc), type = &quot;l&quot;) # 箱ひげ図 boxplot(ccc) boxplot(t(ccc)) # 散布図 # plot pairs(ccc) #install.packages(&quot;car&quot;) #library(car) #scatterplot(ccc) # install.packages(&quot;scatterplot3d&quot;); library(scatterplot3d) # scatterplot3d # その他のグラフ # coplot; mosaic plot; stars; faces; persp; image; contour # その他 # windows() #新しいグラフィック・ウィンドウを開く # par(mfrow = c(2, 2)) # より洗練されたグラフ. やや難易度が高いがモダンなアプローチ # install.packages(&quot;ggplot2&quot;, dependencies = T) library(ggplot2) ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Petal.Length)) + geom_point(aes(colour = Species)) + geom_smooth(method = &quot;lm&quot;, colour = &quot;lightblue&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; 2.6 ファイル入出力 2.6.1 テキストファイル読み込み ファイル読み込み用のR関数には, ファイルの格納場所 (パス) とファイル名を知らせる必要がある パスを省略すると, 現在のディレクトリ (getwd関数で確認可能) 下でファイルを探す. もし, 存在しなければ, エラーとなる ここでは, あらかじめ, 各自のPCのデスクトップ上に, “BStat_2025”という名前のフォルダ (ディレクトリ) を作成していると想定 ファイルは, カンマ(, )で区切られたcsv形式や, タブで区切られたtsv形式の テキストファイルであるとする. data1 &lt;- read.table(&quot;/[パス名]/ファイル名&quot;, header = T, row.names = 1) # オプション # header = T: 1行目が列ラベル # row.names = 1: 1列目が行ラベル または, file.path関数を使ってファイルの格納されているパス(経路)を指定しても良い. # ユーザー(yamada)が, デスクトップ(Desktop)フォルダの下に授業用フォルダ(BStat_2025)を作成した場合のパスの指定 fpath &lt;- file.path(&quot;~&quot;, &quot;Desktop&quot;, &quot;BStat_2025&quot;) # または fpath &lt;- file.path(&quot;Users&quot;, &quot;yamada&quot;, &quot;Desktop&quot;, &quot;BStat_2025&quot;) # ifile &lt;- file.path(fpath, &quot;) # 例えば, # data1 &lt;- read.table(&quot;/Users/[アカウント名]/Desktop/BStat_2025/data.txt&quot;, sep = &quot;, &quot;) # または # data1 &lt;- read.csv(&quot;/Users/[アカウント名]/Desktop/BStat_2025/data.txt&quot;, header = T, row.names = 1) # 代替的に # data2 &lt;- scan(&quot;/Users/[アカウント名]/Desktop/BStat_2025/data.txt&quot;, sep = &quot;, &quot;) #matrix(scan(&quot;/Users/[アカウント名]/Desktop/BStat_2025/data.txt&quot;, sep = &quot;, &quot;), 3, 4, byrow = T) # matrix(scan(&quot;/Users/[アカウント名]/Desktop/BStat_2025/data.txt&quot;, sep = &quot;, &quot;), 3, 4) # scan()において, 数値, 文字が混在している場合, 列ごとにデータ属性を指定する必要 # data2.txt # a 1 2 # b 2 3 # c 3 4 # data3 &lt;- scan(&quot;/Users/[アカウント名]/Desktop/BStat_2025/data.txt&quot;, sep = &quot;, &quot;, list(x = &quot;&quot;, y = 0, z = 0)) # data.frame(data2) # データフレーム化 # パスを指定せずに, テキストファイルの置かれているフォルダに移動してから # ファイル名のみを使って読み込んでも良い # setwd(&quot;/Users/[アカウント名]/Desktop/BStat_2025&quot;) data1 &lt;- read.table(&quot;data.txt&quot;, sep = &quot;, &quot;) # パッケージ&quot;foreign&quot;により, SAS, SPSS等のファイル形式のデータの読み込みが可能 2.6.2 テキストファイル書き出し write(data3, &quot;/[パス名]/ファイル名&quot;) # 例えば, # write.table(data3, &quot;/Users/[アカウント名]/Desktop/BStat_2025/data3_out1.txt&quot;) # write.table(data3, &quot;/Users/[アカウント名]/Desktop/BStat_2025/data3_out1.txt&quot;, append = T) # write.csv() # sink(&quot;/Users/[アカウント名]/Desktop/BStat_2025/data3_out1.txt&quot;) data1; data2 sink() 2.7 パッケージtidyverse tidyverseは, Hadley Wickhamによって開発が進められているRパッケージ (群) である. データのインポート, 整理, 加工, 可視化, 分析を簡単かつ効率的に行うための一連のツールを提供する. tidyverseの中核をなすパッケージには以下のものがある: ggplot2: データの可視化を行うためのパッケージ. レイヤーの概念を用い, データポイント, 統計的変換, スケール, 軸, 凡例など, グラフの各要素を個別に定義し, 組み合わせることができる. これにより, 高度にカスタマイズされたグラフを容易に作成可能. dplyr: データの操作と変形を行うためのパッケージ. フィルタリング, 並べ替え, 集約など, データフレームに対する一般的な操作を簡単かつ直感的に行うための関数を提供. tidyr: データの整理と整形を簡単にするためのパッケージ. データセットのレイアウトを整形する等のクリーニングのタスクに対応しながら,「tidy形式」としてデータを再構築するツールを提供. 例えば, データを多数の列に広げる「wide形式」と データをより少ない列にまとめるが行を増やす「long形式」間の変換, 欠損値への適切な対処, 一列を複数列に分割あるいは複数列を一列に結合する等の処理. readr: さまざまな形式のテキストデータ (例えば, csv, tsv形式) を読み込み, Rのデータフレームとして効率的にインポートするためのパッケージ. 標準のR関数よりも高速に動作し, ファイルの読み込み時によくある問題 (データ型の自動認識, 欠損値の扱い等) をより柔軟に処理. さらに, 便利な機能を持つパッケージとして, purrr: リストと関数型プログラミングを扱うためのパッケージ. リストの操作, 要素の繰り返し処理, 条件に基づく要素の抽出など, 複雑なデータ構造の操作を簡単にする関数を提供. tibble: データフレームをより現代的かつ柔軟に扱うためのパッケージで, 印刷時の見やすさ, 列名の非標準的な文字の扱い, サブセット操作の改善など, データフレームを強化し使いやすさを改善. stringr: 文字列データの操作を行うためのパッケージ. Rの標準文字列操作機能よりも一貫性と可読性に優れたインターフェースを提供し、文字列の検索, 置換, 分割, 結合などのタスクを簡単に行うことが可能. forcats: 因子 (カテゴリカルデータ) を扱うためのパッケージ. 因子水準の順序変更, 要約, 結合, 分離など, 因子型のデータを操作するための便利な関数を提供. lubridate: 日付と時刻のデータを扱うためのパッケージ. 日付や時刻の加算・減算, 部分的な抽出, 時間差の計算など, 操作を直感的かつ効率的にするための関数を提供. tidyverseは, データを「tidy」（整然とした）形式で扱うことに焦点を当てている. tidyデータの原則では, 各変数が列に, 各観測値が行に, 各種類の観測単位がテーブルに配置される. この原則に従うことで, データ分析がより直感的で効率的になる. tidyverseパッケージは, Rでのデータ分析作業を容易にし, コードをより読みやすく, 書きやすくすることを指向している. それぞれのパッケージは単独で使用することも出来るが, 一緒に使用することでより使い勝手が向上し便利である. データサイエンスにおける日常的なタスクを簡潔に, かつ効率的に行うための強力なツールセットと言える. tidyverseのホームページ https://tidyverse.tidyverse.org/ 2.7.1 Q: Rの初心者はtidyverseから勉強することは可能か? 今日では, R言語の基本を学ばずにいきなりtidyverseから勉強することは, tidyverseを入口としてR言語の学習を始めるユーザーも多いと思われる. 特にデータ分析やデータサイエンスに焦点を当てている初心者にとっては代替的な選択肢である. tidyverseは, データの取り扱いを直感的かつ効率的にすることを目的として設計されており, その構文は初心者にとって学びやすいように工夫されている. tidyverseの利点: 直感的な構文: tidyverseの関数は覚えやすく, 理解しやすい構文を持っているため, R言語の初心者でも扱いやすい. データ分析のワークフローを強化: tidyverseはデータのインポート, 整理, 加工, 可視化, 分析という一連のデータ分析プロセスに対応するツールを提供する. これにより, データ分析の基本的な流れを簡単に学ぶことができる. 広範なコミュニティとサポート: tidyverseはRユーザー内に多くの熱狂的なファンがいて, コミュニティを形成している. オンラインでのサポートや学習リソースが豊富にある. 注意点: R言語の基本概念の理解の必要性: R言語の基本的な概念 (変数の割り当て, 関数の使用方法, データ型など) は、tidyverseを効率的に使用するためにも理解しておく必要あり. 限定的な機能: tidyverseだけではカバーできないR言語の機能も多くある. すなわち, tidyverseから学習を始めても, いずれはR言語のより広範な機能やパッケージにも目を向けることが必要. tidyverseに含まれるパッケージを利用すると, 確かに多くの複雑な処理が簡潔かつエレガントに書けたりすることがあり, その機能性を実感することも多い. しかし, Rプログラミングを行っている中で, R言語の基本を知らないと困るようなことの方が多い. よって, 筆者の考えでは, Rの初学者はいきなりtidyverse系を軸に学習を開始するよりは, tidyverseに含まれるパッケージはあくまでRの多数あるパッケージの一部であると位置付けて, 標準的なRを学びながらこれらのパッケージの用法を学ぶというスタンスで良い. 本コースは, tidyverse系の扱いはこのような方針に従って進めるものとする. 2.7.2 tidyverseの基本 サンプルコードの出所: ChatGPT (GPT-4) 以下, tidyverseの基本を理解するために, データのインポート, 加工, そして可視化のステップを含むシンプルなチュートリアルを紹介する. ここでは, tidyverseの中のreadr, dplyr, ggplot2の三つのパッケージを使用する. これらはtidyverseの中で最もよく使用されるパッケージである. ステップ 1: tidyverseをインストールして読み込む まず, tidyverseパッケージをインストールし, ライブラリに読み込む. # tidyverseパッケージのインストール # install.packages(&quot;tidyverse&quot;) # ライブラリに読み込む library(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.4 ✔ readr 2.1.5 ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ ggplot2 3.5.1 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.3 ✔ tidyr 1.3.1 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors ステップ 2: データをインポートする tidyverseには様々なサンプルデータが含まれている. ここでは, mtcarsデータセットを使用する. mtcarsは, 1974年のMotor Trend US誌に掲載された32台の自動車に関するデータである. # mtcarsデータセットを使用する data &lt;- mtcars ステップ 3: データを加工する dplyrを使用してデータを加工する. ここでは, mpg（ガロンあたりのマイル数）が20を超える車両のみを選択し、cyl（シリンダー数）ごとの平均mpgを計算する. # dplyrを使ってデータをフィルタリングし、集約する filtered_data &lt;- data %&gt;% filter(mpg &gt; 20) %&gt;% group_by(cyl) %&gt;% summarise(mean_mpg = mean(mpg)) # 結果を表示 print(filtered_data) ## # A tibble: 2 × 2 ## cyl mean_mpg ## &lt;dbl&gt; &lt;dbl&gt; ## 1 4 26.7 ## 2 6 21.1 ステップ 4: データを可視化する 最後に, ggplot2を使ってデータの可視化を行う. ここでは, cylごとのmean_mpgを棒グラフで表示する. # ggplot2を使ってデータを可視化 ggplot(filtered_data, aes(x = factor(cyl), y = mean_mpg)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;blue&quot;) + theme_minimal() + labs(title = &quot;Cylinder-wise Mean MPG&quot;, x = &quot;Number of Cylinders&quot;, y = &quot;Mean MPG&quot;) 関数ggplot()の別の使用例として, マルチパネル化した散布図を示す. # データを可視化する # mpgとhpの関係を示す散布図を作成し、cylごとに異なるパネルに表示する ggplot(data, aes(x = mpg, y = hp)) + geom_point() + facet_wrap(~cyl) + theme_minimal() + theme(panel.background = element_rect(fill = &quot;gray&quot;)) + labs(title = &quot;Scatterplot of MPG vs HP by Cylinder&quot;, x = &quot;Miles Per Gallon (MPG)&quot;, y = &quot;Horsepower (HP)&quot;) "],["仮説検定.html", "3 仮説検定 3.1 平均値の差の検定 3.2 カイ二乗検定 3.3 分析例: 統計テストデータ", " 3 仮説検定 本章では, 仮説検定の中でも, 実務において良く用いられる, 平均値の差の検定 独立性の検定 について説明する. 独立性の検定と共通点のある 適合度検定 についても説明する. 3.1 平均値の差の検定 t検定は, Rで標準的に用意されている関数t.test()を用いて行う. # t.test(x, y = NULL, # alternative = c(&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;), # mu = 0, paired = FALSE, var.equal = FALSE, # conf.level = 0.95, ...) # # # One Sample t-test # Performs one and two sample t-tests on vectors of data. 平均値の差の検定 (ペア検定) ペア検定を実施する場合には, t.test()の引数pairedを真 (TRUE, またはT) に設定する. デフォルトは,paired = FALSE (F) である (ペア検定ではない). ここでは, Rにデフォルトで収録されているデータセットsleepを用いる. データセット: sleep sleep: 睡眠薬の効果を調べる実験データ - 患者10名, 2種類の睡眠薬の比較 (コントロールに対する睡眠時間の増加分) - (extra, group, ID) 20件, 患者(ID) 10名 # help(sleep) # Data which show the effect of two soporific drugs (increase in hours of sleep compared to control) on 10 patients. head(sleep); tail(sleep) #&gt; extra group ID #&gt; 1 0.7 1 1 #&gt; 2 -1.6 1 2 #&gt; 3 -0.2 1 3 #&gt; 4 -1.2 1 4 #&gt; 5 -0.1 1 5 #&gt; 6 3.4 1 6 #&gt; extra group ID #&gt; 15 -0.1 2 5 #&gt; 16 4.4 2 6 #&gt; 17 5.5 2 7 #&gt; 18 1.6 2 8 #&gt; 19 4.6 2 9 #&gt; 20 3.4 2 10 最初に, Rへの指示を簡潔にするために, attach(sleep) を実行する. これにより, sleep内に含まれる変数extra,group,IDについては, それらがsleepの変数であることをその都度教えてなくても, Rは認識できるようになる (Rの“サーチパス”に加わる). attach(sleep) par(mfrow = c(1, 2)) plot(extra); hist(extra) 両側検定, 片側検定は, 引数alternativeの値 (“two.sided”, “less”, “greater”) で指定する. デフォルトはalteranative=\"two.sided\", すなわち, 両側検定である. # t.test(extra) # 注) デフォルトはpaired = F (ペア検定ではない) # 両側検定 t.test(extra[group == 1], extra[group == 2], paired = T) #&gt; #&gt; Paired t-test #&gt; #&gt; data: extra[group == 1] and extra[group == 2] #&gt; t = -4.0621, df = 9, p-value = 0.002833 #&gt; alternative hypothesis: true mean difference is not equal to 0 #&gt; 95 percent confidence interval: #&gt; -2.4598858 -0.7001142 #&gt; sample estimates: #&gt; mean difference #&gt; -1.58 片側検定は, 右側はalternative = \"greater\", 左側はalternative = \"greater\"で指定する. # 片側検定 t.test(extra[group == 1], extra[group == 2], paired = T, alternative = &quot;greater&quot;) # 片側 (右側) 検定) #&gt; #&gt; Paired t-test #&gt; #&gt; data: extra[group == 1] and extra[group == 2] #&gt; t = -4.0621, df = 9, p-value = 0.9986 #&gt; alternative hypothesis: true mean difference is greater than 0 #&gt; 95 percent confidence interval: #&gt; -2.293005 Inf #&gt; sample estimates: #&gt; mean difference #&gt; -1.58 t.test(extra[group == 1], extra[group == 2], paired = T, alternative = &quot;less&quot;) # 片側 (左側) 検定) #&gt; #&gt; Paired t-test #&gt; #&gt; data: extra[group == 1] and extra[group == 2] #&gt; t = -4.0621, df = 9, p-value = 0.001416 #&gt; alternative hypothesis: true mean difference is less than 0 #&gt; 95 percent confidence interval: #&gt; -Inf -0.8669947 #&gt; sample estimates: #&gt; mean difference #&gt; -1.58 boxplot(sleep) # bad example #boxplot(extra) boxplot(extra ~ group) par(mfrow = c(1, 1)) 平均値の差に関するt検定は, 二つの標本 \\((x_1,x_2,...,x_{n_1})\\), \\((y_1,y_2,...,y_{n_2})\\) の差\\(\\mu_1-\\mu_2\\)がゼロか否かを評価する統計的な手続きである (ここで, 未知の真の平均はそれぞれ, \\(\\mu_1\\), \\(\\mu_2\\)と表記する). ペア検定は, さらに, この2標本のサイズが等しく (\\(n_1=n_2\\equiv n\\)), しかも, 各々のデータ点がペア \\((x_i,y_i)\\) として扱える (“対応がある”) ような特別な場合である. 対応のあるケースの例としては, 同一の企業からデータを2種類, あるいは2時点について採取した場合である. このようなペアのケースにおける平均値の差の検定は, 2標本のまま扱うのではなく, 各データ点ペアの差 \\((x_i-y_i)\\) を取ることで1標本に集約した上で, 1標本に対する平均値ゼロのt検定として行う. すなわち, 帰無仮説\\(H_0: \\mu_1-\\mu_2=0\\)に対して, 対立仮説は, 両側検定: \\(H_1: \\mu_1-\\mu_2 \\neq 0\\) 片側 (右側) 検定: \\(H_1: \\mu_1-\\mu_2&gt;0\\) 片側 (左側) 検定: \\(H_1: \\mu_1-\\mu_2&lt;0\\) である. 上のsleepデータセットのケースにおいては, 標本平均の差 (\\(\\bar{x}_1-\\bar{x}_2\\))の大きさが\\(-1.58\\)であり, 対応する分散の大きさ (上の結果では表示されていない) に比べて十分に小さい (帰無仮説であるゼロ からマイナス方向に遠く離れている). その結果, 両側検定では, p値は0.002833となり, 1%有意水準 (\\(\\alpha=0.01\\)) でも帰無仮説は棄却される こととなった. 一方, 対立仮説として, 右側 \\(H_1: \\mu_1-\\mu_2&gt;0\\) を採用した場合には, 標本から計算される平均値の差は正の値となることが期待され (\\(\\bar{x}_1-\\bar{x}_2&gt;0\\)), これは, データセットsleepから計算された値 (\\(-1.58\\)) とは明らかに整合的ではない. このことは, 対応するp値が\\(0.9986\\)と 1に近く, 帰無仮説を棄却できない大きさとなっていることに表れている. 平均値の差の検定 ペアを構成しない一般の2標本の平均値の差に関するt検定においては, 2標本の持つ未知の分散の大きさが等しいかが問題になる. t.test()のデフォルトでは等分散性が成立しない (var.equal = FALSE) 設定となっている. この時は, Welch検定が実行される. もちろん, 等分散性が成立する場合には, var.equalの値はTまたはTRUE と指定せねばならない. この時は, 未知の分散はプール化された (“pooled”) 推定値を持ちいたt検定が行われる. A/Bテストデータ (仮想データ) デザイン A と B のどちらが平均的な購入意向を高めるか - id, group (A:旧デザイン, B:新デザイン), purchase_intent (1--7) - 標本サイズ: nA=50, nB=60 abtest_df &lt;- read.csv(&quot;purchase_1-7.csv&quot;, header = T) boxplot(purchase_intent ~ group, data = abtest_df) 等分散性の検定は, var.test()を用いることができる # 等分散性の検定 (F検定) var.test(purchase_intent ~ group, data = abtest_df) #&gt; #&gt; F test to compare two variances #&gt; #&gt; data: purchase_intent by group #&gt; F = 1.1253, num df = 49, denom df = 59, p-value = 0.6606 #&gt; alternative hypothesis: true ratio of variances is not equal to 1 #&gt; 95 percent confidence interval: #&gt; 0.6592874 1.9472951 #&gt; sample estimates: #&gt; ratio of variances #&gt; 1.125316 # ※ 省略されることも多い その上で, Welch検定か, プール化された分散推定値を用いるt検定を実行する. 上の等分散性検定の結果より, 帰無仮説 (2標本の分散は等しい, \\(\\sigma_1^2=\\sigma_2^2\\)) は棄却されないことから (p値\\(=0.6606\\)), 等分散の場合について実行すれば良い. # 等分散の場合 (Pooled variance使用するt検定) t.test(purchase_intent ~ group, var.equal = T, data = abtest_df) #&gt; #&gt; Two Sample t-test #&gt; #&gt; data: purchase_intent by group #&gt; t = -4.3526, df = 108, p-value = 3.067e-05 #&gt; alternative hypothesis: true difference in means between group A and group B is not equal to 0 #&gt; 95 percent confidence interval: #&gt; -1.1449141 -0.4284192 #&gt; sample estimates: #&gt; mean in group A mean in group B #&gt; 4.480000 5.266667 しかし, 繰り返し仮説検定を行うことによる弊害 (多重検定問題) を回避するため, 等分散性の検定を行わず, いきなりWelch検定を実行することも多い. # 等分散でない場合 (Welch t検定) t.test(purchase_intent ~ group, data = abtest_df) #&gt; #&gt; Welch Two Sample t-test #&gt; #&gt; data: purchase_intent by group #&gt; t = -4.3291, df = 101.99, p-value = 3.505e-05 #&gt; alternative hypothesis: true difference in means between group A and group B is not equal to 0 #&gt; 95 percent confidence interval: #&gt; -1.1470981 -0.4262352 #&gt; sample estimates: #&gt; mean in group A mean in group B #&gt; 4.480000 5.266667 自主課題: Q. 以上の検定結果を解釈しなさい. Q. 片側検定にするには? 対立仮説の方向はどちらか? Q. 購入意向 (1-7) は順序尺度では? Q. 正規分布を前提にしたt検定を使用して良いか? 正規性の確認方法 t検定は, 標本を抽出するもととなる母集団の確率分布が 正規分布であるというのが分布に関する基本的な仮定である. したがって, t検定の実行に先立ち, 標本が正規分布に従うか否かを調べ, t検定の前提条件が満たされているかどうかを確認することが求められる. これは, 標本数が小さい時には特に注意する必要がある. # ヒストグラム作成 A &lt;- abtest_df[abtest_df$group == &quot;A&quot;, &quot;purchase_intent&quot;] B &lt;- abtest_df[abtest_df$group == &quot;B&quot;, &quot;purchase_intent&quot;] hist(A, col = rgb(1, 0.5, 0, 0.5)); hist(B, col = rgb(0, 0.5, 1, 0.5), add = T) # 正規性の検定 ks.test(A, &quot;pnorm&quot;); ks.test(B, &quot;pnorm&quot;) # コルモゴロフ・スミルノフ (Kolmogorov-Smirnov) 検定 #&gt; #&gt; Asymptotic one-sample Kolmogorov-Smirnov test #&gt; #&gt; data: A #&gt; D = 0.99865, p-value &lt; 2.2e-16 #&gt; alternative hypothesis: two-sided #&gt; #&gt; Asymptotic one-sample Kolmogorov-Smirnov test #&gt; #&gt; data: B #&gt; D = 0.99865, p-value &lt; 2.2e-16 #&gt; alternative hypothesis: two-sided shapiro.test(A); shapiro.test(B) # シャピロ・ウィルク (Shapiro-Wilk) 検定 #&gt; #&gt; Shapiro-Wilk normality test #&gt; #&gt; data: A #&gt; W = 0.89953, p-value = 0.0004638 #&gt; #&gt; Shapiro-Wilk normality test #&gt; #&gt; data: B #&gt; W = 0.89262, p-value = 7.16e-05 qqnorm(A); qqnorm(B) # q-qプロット なお, 関数ggplot()を使うことで, よりエレガントなプロットを描くことが出来る. # install.packages(&quot;ggplot2&quot;) # 必要に応じてインストール library(ggplot2) ggplot(abtest_df, aes(x = purchase_intent, fill = group)) + geom_histogram(position = &quot;identity&quot;, alpha = 0.5, bins = 7, # ビンの数を適宜指定 color = &quot;grey&quot;) + # 枠線の色 (任意) scale_fill_manual(values = c(&quot;A&quot; = &quot;orange&quot;, &quot;B&quot; = &quot;lightblue&quot;)) + labs(title = &quot;Histogram of Two Groups (Overlapped)&quot;, x = &quot;Purchase Intent (1-7)&quot;, y = &quot;Count&quot;) + theme_minimal() 平均値の差の検定 (ノンパラメトリック検定) 上記のように, t検定は標本を抽出するもととなる母集団の確率分布が 正規分布であるというのが分布に関する基本的な仮定である. しかしその一方で, 標本数が十分に大きい時は, 母集団の確率分布が 正規分布である必要はない. それは, 中心極限定理の働きによって, 検定統計量が帰無仮説の下では 近似的にStudent t分布に従うことが理論的に示されるためである. 一方, 標本数が大きくない場合には, 母集団の確率分布が正規分布でない場合には, t検定の前提条件から乖離する状況となるため, 別の検定法を用いることが適切である. ノンパラメトリック検定法は, 分布形状に関する仮定によらない, ロバスト (頑強) な検定方法である. Wilcoxonの順位和検定 (Mann–WhitneyのU検定) ここでは, 2群の平均値の差に関する t検定の代替的手法として, ウィルコクソン (Wilcoxon) の順位和検定を紹介する. 2群の中央値の差を調べる 外れ値に対して頑強 wilcox.test(purchase_intent ~ group, data = abtest_df) #&gt; #&gt; Wilcoxon rank sum test with continuity correction #&gt; #&gt; data: purchase_intent by group #&gt; W = 861.5, p-value = 5.992e-05 #&gt; alternative hypothesis: true location shift is not equal to 0 # A,Bを定義しておいた場合, 以下の実行も可能 # wilcox.test(A, B) 自主課題: Q. 以上の検定結果を解釈しなさい. 3.2 カイ二乗検定 # chisq.test(x, y = NULL, correct = TRUE, # p = rep(1 / length(x), length(x)), rescale.p = FALSE, simulate.p.value = FALSE, B = 2000) # chisq.test performs chi-squared contingency table tests and goodness-of-fit tests. 3.2.1 独立性検定 # 演習用データの作成 (実務では, ファイルを読み込む) d1 &lt;- matrix(c(rep(c(&quot;a1&quot;, &quot;b1&quot;), 76), rep(c(&quot;a1&quot;, &quot;b2&quot;), 15), rep(c(&quot;a1&quot;, &quot;b3&quot;), 41)), byrow = T, ncol = 2) d2 &lt;- matrix(c(rep(c(&quot;a2&quot;, &quot;b1&quot;), 95), rep(c(&quot;a2&quot;, &quot;b2&quot;), 30), rep(c(&quot;a2&quot;, &quot;b3&quot;), 85)), byrow = T, ncol = 2) d3 &lt;- matrix(c(rep(c(&quot;a3&quot;, &quot;b1&quot;), 135), rep(c(&quot;a3&quot;, &quot;b2&quot;), 70), rep(c(&quot;a3&quot;, &quot;b3&quot;), 95)), byrow = T, ncol = 2) d4 &lt;- matrix(c(rep(c(&quot;a4&quot;, &quot;b1&quot;), 69), rep(c(&quot;a4&quot;, &quot;b2&quot;), 10), rep(c(&quot;a4&quot;, &quot;b3&quot;), 29)), byrow = T, ncol = 2) data2 &lt;- rbind(d1, d2, d3, d4) colnames(data2) &lt;- c(&quot;A&quot;, &quot;B&quot;) # 商品種類(A), 販売チャネル(B) head(data2) #&gt; A B #&gt; [1,] &quot;a1&quot; &quot;b1&quot; #&gt; [2,] &quot;a1&quot; &quot;b1&quot; #&gt; [3,] &quot;a1&quot; &quot;b1&quot; #&gt; [4,] &quot;a1&quot; &quot;b1&quot; #&gt; [5,] &quot;a1&quot; &quot;b1&quot; #&gt; [6,] &quot;a1&quot; &quot;b1&quot; table(data2) # marginal totals #&gt; data2 #&gt; a1 a2 a3 a4 b1 b2 b3 #&gt; 132 210 300 108 375 125 250 (tbl &lt;- table(data2[, 1], data2[, 2])) # contingency table #&gt; #&gt; b1 b2 b3 #&gt; a1 76 15 41 #&gt; a2 95 30 85 #&gt; a3 135 70 95 #&gt; a4 69 10 29 chisq.test(data2[, &quot;A&quot;], data2[, &quot;B&quot;]) #&gt; #&gt; Pearson&#39;s Chi-squared test #&gt; #&gt; data: data2[, &quot;A&quot;] and data2[, &quot;B&quot;] #&gt; X-squared = 27.661, df = 6, p-value = 0.0001088 # chisq.test(data2[, 1], data2[, 2]) # または chisq.test(tbl) #&gt; #&gt; Pearson&#39;s Chi-squared test #&gt; #&gt; data: tbl #&gt; X-squared = 27.661, df = 6, p-value = 0.0001088 # A/Bテスト # サイト導線A/Bとで, コンバージョンへの効果を比較 # 有 無 # サイト導線A 50 131 # サイト導線B 23 35 ABdat &lt;- matrix(c(50, 131, 23, 35), ncol = 2, byrow = T) chisq.test(ABdat) #&gt; #&gt; Pearson&#39;s Chi-squared test with Yates&#39; continuity correction #&gt; #&gt; data: ABdat #&gt; X-squared = 2.4566, df = 1, p-value = 0.117 # 導線Aでコンバージョンしない人が10人増えた # 有 無 # サイト導線A 50 141 # サイト導線B 23 35 ABdat2 &lt;- matrix(c(50, 141, 23, 35), ncol = 2, byrow = T) chisq.test(ABdat2) #&gt; #&gt; Pearson&#39;s Chi-squared test with Yates&#39; continuity correction #&gt; #&gt; data: ABdat2 #&gt; X-squared = 3.2764, df = 1, p-value = 0.07028 # Fisherの正確確率検定 fisher.test(ABdat2) #&gt; #&gt; Fisher&#39;s Exact Test for Count Data #&gt; #&gt; data: ABdat2 #&gt; p-value = 0.06915 #&gt; alternative hypothesis: true odds ratio is not equal to 1 #&gt; 95 percent confidence interval: #&gt; 0.2796541 1.0568106 #&gt; sample estimates: #&gt; odds ratio #&gt; 0.5410443 3.2.2 適合度検定 # メンデルのデータ（エンドウの交雑実験） # 種子の特徴(形質), 黄色・丸い, 黄色・しわ, 緑色・丸い, 緑色・しわ obs &lt;- c(315, 101, 108, 32 ) # 観測度数 prob &lt;- c(9, 3, 3, 1) / 16 # 理論確率分布 chisq.test(obs, p = prob) # obs と prob を用いたカイ二乗検定 #&gt; #&gt; Chi-squared test for given probabilities #&gt; #&gt; data: obs #&gt; X-squared = 0.47002, df = 3, p-value = 0.9254 # 確認用 ex &lt;- prob * sum(obs) chisq &lt;- sum((obs - ex) ^ 2 / ex) pval &lt;- 1 - pchisq(chisq, 3) 3.3 分析例: 統計テストデータ testdat &lt;- read.csv(&quot;BS_stattest.csv&quot;, header = F) # year(学年), MF(性別:男性1女性2), AS(文理:文系1その他2理系3), # math(数学履修年数), work(勤務年数), stat(統計学経験0-2), MS(経営科学好き嫌い0-3), # s4(4級相当得点), s3(3級相当得点), s2(2級相当得点) colnames(testdat) &lt;- c(&quot;year&quot;, &quot;MF&quot;, &quot;AS&quot;, &quot;math&quot;, &quot;work&quot;, &quot;stat&quot;, &quot;MS&quot;, &quot;s4&quot;, &quot;s3&quot;, &quot;s2&quot;) score &lt;- apply(testdat[, c(&quot;s4&quot;, &quot;s3&quot;, &quot;s2&quot;)], 1, sum) testdat2 &lt;- cbind(testdat, score) # モダンな方法 # library(tidyverse) # testdat2 &lt;- testdat %&gt;% mutate(score = s4 + s3 + s2) # データの要約 # attach()を使わない場合: # table(testdat2$MF) # table(testdat2[, c(&quot;MF&quot;, &quot;AS&quot;)]) # table(testdat2$s3) attach(testdat2) table(MF) #&gt; MF #&gt; 1 2 #&gt; 29 8 table(MF, AS) #&gt; AS #&gt; MF 1 2 3 #&gt; 1 17 3 9 #&gt; 2 4 1 3 table(s3) #&gt; s3 #&gt; 3 4 5 6 7 8 9 #&gt; 3 2 5 10 5 10 2 summary(score) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 10.00 16.00 18.00 17.97 20.00 24.00 fivenum(score) #&gt; [1] 10 16 18 20 24 hist(score) # 相関係数 # cor(testdat[, c(&quot;math&quot;, &quot;s2&quot;)]) # cor(testdat[, c(&quot;math&quot;, &quot;s3&quot;)]) # cor(testdat[, c(&quot;math&quot;, &quot;s4&quot;)]) cor(math, s2) #&gt; [1] 0.04241976 cor(math, s3) #&gt; [1] 0.2481946 cor(math, s4) #&gt; [1] 0.3205779 pairs(testdat[, 8:10]) # cor(testdat[, c(&quot;MS&quot;, &quot;s4&quot;, &quot;s3&quot;, &quot;s2&quot;)]) cor(cbind(MS, s4, s3, s2)) #&gt; MS s4 s3 s2 #&gt; MS 1.0000000 0.2592955 0.4922877 0.2600562 #&gt; s4 0.2592955 1.0000000 0.1315443 0.1245631 #&gt; s3 0.4922877 0.1315443 1.0000000 0.6435060 #&gt; s2 0.2600562 0.1245631 0.6435060 1.0000000 cor(MS, math) #&gt; [1] 0.2864297 # 箱ひげ図 boxplot(score ~ factor(MF)) # 2-level factor boxplot(score ~ factor(MS)) # 4-level factor # 平均値の差の検定 score_MF &lt;- split(score, factor(MF)) t.test(score_MF$&#39;1&#39;, score_MF$&#39;2&#39;) #&gt; #&gt; Welch Two Sample t-test #&gt; #&gt; data: score_MF$&quot;1&quot; and score_MF$&quot;2&quot; #&gt; t = 2.8923, df = 13.276, p-value = 0.01237 #&gt; alternative hypothesis: true difference in means is not equal to 0 #&gt; 95 percent confidence interval: #&gt; 0.884667 6.063609 #&gt; sample estimates: #&gt; mean of x mean of y #&gt; 18.72414 15.25000 # score2 &lt;- testdat[, &quot;s2&quot;] # score3 &lt;- testdat[, &quot;s3&quot;] # score4 &lt;- testdat[, &quot;s4&quot;] # t.test(score4, score3, paired = T, alternative = &quot;greater&quot;) # t.test(score3, score2, paired = T, alternative = &quot;greater&quot;) # ペア検定 t.test(s4, s3, paired = T, alternative = &quot;greater&quot;) #&gt; #&gt; Paired t-test #&gt; #&gt; data: s4 and s3 #&gt; t = 0.87426, df = 36, p-value = 0.1939 #&gt; alternative hypothesis: true mean difference is greater than 0 #&gt; 95 percent confidence interval: #&gt; -0.2516528 Inf #&gt; sample estimates: #&gt; mean difference #&gt; 0.2702703 t.test(s3, s2, paired = T, alternative = &quot;greater&quot;) #&gt; #&gt; Paired t-test #&gt; #&gt; data: s3 and s2 #&gt; t = 5.305, df = 36, p-value = 2.95e-06 #&gt; alternative hypothesis: true mean difference is greater than 0 #&gt; 95 percent confidence interval: #&gt; 0.9212852 Inf #&gt; sample estimates: #&gt; mean difference #&gt; 1.351351 # 得点の差のt値の計算 (確認用) m &lt;- mean(s3 - s2) v &lt;- var(s3 - s2) tt &lt;- m / sqrt(v / (length(s3))) # var.test(x, y, ratio = 1, # alternative = c(&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;), # conf.level = 0.95, ...) # Performs an F test to compare the variances of two samples from normal populations. # aaa &lt;- table(testdat[, c(&quot;MF&quot;, &quot;AS&quot;)]) aaa &lt;- table(MF, AS) chisq.test(aaa) #&gt; #&gt; Pearson&#39;s Chi-squared test #&gt; #&gt; data: aaa #&gt; X-squared = 0.18986, df = 2, p-value = 0.9094 # aaa &lt;- table(testdat[, c(&quot;MS&quot;, &quot;AS&quot;)]) aaa &lt;- table(MS, AS) chisq.test(aaa) #&gt; #&gt; Pearson&#39;s Chi-squared test #&gt; #&gt; data: aaa #&gt; X-squared = 16.044, df = 6, p-value = 0.01352 detach(testdat2) "],["分散分析-anova.html", "4 分散分析 (ANOVA) 4.1 1元配置ANOVA 4.2 2元配置ANOVA 4.3 分析例 4.4 分散分析と平均値の多重比較*", " 4 分散分析 (ANOVA) 授業では, 多群における平均値の差の検定の方法として, (1元配置) 分散分析 (ANOVA) を紹介した. \\(k\\)個の群 (グループ) があり, それぞれの真の平均値が \\(\\mu_1,\\mu_2,...,\\mu_k\\)であるとする. ANOVA帰無仮説 (\\(H_0\\)): 全群の平均値が等しい (\\(\\mu_1=\\mu_2=...=\\mu_k\\)) 対立仮説 (\\(H_1\\)): 平均値の異なる群が少なくとも一つある (\\(\\mu_i \\neq \\mu_j, i \\neq j\\)) 4.1 1元配置ANOVA ANOVAの実行には, 1元配置ANOVAに特化したR関数であるoneway.anova()の他, より一般のANOVAに適用できるaov(), anova()が利用できる. 以下では, 仮想データセット (3群, \\(k=3\\)) について, aov()を使用した実行例を示す. # コード出所: ChatGPT-4の出力をもとに編集 # 仮想データセットの作成 (数値例1) # グループ数: k=3 k &lt;- 3 ttt1 &lt;- c(8, 7, 9, 6, 8) ttt2 &lt;- c(7, 5, 4) ttt3 &lt;- c(6, 2, 1, 3) # &quot;縦型形式&quot;データセットの作成 dat1 &lt;- data.frame(grp = c(rep(&quot;ttt1&quot;, length(ttt1)), rep(&quot;ttt2&quot;, length(ttt2)), rep(&quot;ttt3&quot;, length(ttt3))), resp = c(ttt1, ttt2, ttt3)) # ANOVAの実行 res_aov &lt;- aov(resp ~ grp, data = dat1) # 実行結果の表示 summary(res_aov) #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; grp 2 47.13 23.567 8.887 0.0074 ** #&gt; Residuals 9 23.87 2.652 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (自主課題) 実行結果の解釈をしてみよう. ANOVAの計算ロジックの確認* つぎに, 1元配置ANOVAが実際に行っている計算をRコードで書いたものを紹介する. これは, ANOVAの理論を理解を深めることを目的にしたもので, ANOVAの実践には不要であり, 読み飛ばしても構わない. # 確認用 (飛ばしてOK) library(tidyverse) # 全体平方和 (SST) の計算 avg &lt;- mean(dat1$resp) # 全体平均 # (mean(dat1$resp^2) - avg^2) * length(dat1$resp) sst &lt;- sum((dat1$resp - avg)^2) # 全体平方和 # 群内平方和 (SSW) の計算 # (※) 発展的なコード # library(tidyverse) ssw_j &lt;- dat1 %&gt;% group_by(grp) %&gt;% summarize(ssq = (mean(resp^2) - mean(resp)^2) * length(resp)) ssw &lt;- sum(ssw_j$ssq) # 群内平方和 # 群間平方和 (SSB) の計算 n_vec &lt;- c(length(ttt1), length(ttt2), length(ttt3)) avg_j &lt;- dat1 %&gt;% group_by(grp) %&gt;% summarize(avg = mean(resp)) ssb &lt;- sum((avg_j$avg - avg)^2 * n_vec) # 群間平方和 # または, ssb &lt;- sst - ssw # 群間平方和 # 自由度 (df) # SST: length(dat1$resp) - 1, # SSB: K - 1, # SSW: length(dat1$resp) - K # 平均平方和の計算 msb &lt;- ssb / (k - 1) msw &lt;- ssw / (length(dat1$resp) - k) # F値の計算 f_val &lt;- msb / msw # p値の計算 pf(f_val, k - 1, length(dat1$resp) - k, lower.tail = F) #&gt; [1] 0.007402874 (自主課題) 関数aov()の実行結果と見比べてみよう. 4.2 2元配置ANOVA 授業ではカバーしていないが, 2つの因子が反応変数に与える影響を 調べる2元配置ANOVAの実行コード例を紹介する. ここでは, 別の仮想データセット (2群) を用意する. # コード出所: ChatGPT-4の出力をもとに編集 # 仮想データセットの作成 (数値例2) fctA_val &lt;- c(&quot;a1&quot;, &quot;a2&quot;, &quot;a3&quot;, &quot;a4&quot;) fctB_val &lt;- c(&quot;b1&quot;, &quot;b2&quot;, &quot;b3&quot;) val &lt;- matrix(c(8, 7, 6, 7, 5, 2, 6, 4, 3, 8, 6, 2), nrow = 4, byrow = T) colnames(val) &lt;- fctB_val rownames(val) &lt;- fctA_val # &quot;縦型形式&quot;データセットに変形 dat2 &lt;- as.data.frame(val) %&gt;% rownames_to_column(var = &quot;fctA&quot;) %&gt;% pivot_longer(cols = b1:b3, names_to = &quot;fctB&quot;, values_to = &quot;resp&quot;) %&gt;% data.frame() # SST # sum((val - mean(val))^2) # ANOVAの実行 res_aov &lt;- aov(resp ~ fctA + fctB, data = dat2) # 実行結果の表示 summary(res_aov) #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; fctA 3 12.67 4.222 4.343 0.05988 . #&gt; fctB 2 32.17 16.083 16.543 0.00362 ** #&gt; Residuals 6 5.83 0.972 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # interactionなし # res_aov &lt;- aov(resp ~ fctA * fctB, data = dat2) # summary(res_aov) (自主課題) 実行結果の解釈をしてみよう. 4.3 分析例 データ1: 2業種口コミサイト(仮想データ) t検定と結果の比較を行う. データの読み込み, 並べ替え x &lt;- read.csv(&quot;dat_1-1.csv&quot;) # 口コミサイト(仮想データ) # 2つの業種(A, B), 各20社 # 各企業に対する(元)従業員による平均評価点(1--5) head(x) #&gt; A B #&gt; 1 2.257 4.065 #&gt; 2 4.273 4.771 #&gt; 3 4.205 2.793 #&gt; 4 3.251 3.003 #&gt; 5 1.534 3.250 #&gt; 6 3.327 3.390 # xの整形 (横型 → 縦型) xvec &lt;- as.vector(as.matrix(x)) # xをベクトル化 yvec &lt;- c(rep(&quot;A&quot;, 20), rep(&quot;B&quot;, 20)) xdf &lt;- data.frame(score = xvec, type = yvec) str(xdf) #&gt; &#39;data.frame&#39;: 40 obs. of 2 variables: #&gt; $ score: num 2.26 4.27 4.21 3.25 1.53 ... #&gt; $ type : chr &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ... xdf$score; xdf[, &quot;score&quot;]; xdf[, 1] #&gt; [1] 2.257 4.273 4.205 3.251 1.534 3.327 3.596 3.421 2.544 2.110 2.262 2.550 #&gt; [13] 2.790 2.033 3.688 3.440 3.797 3.644 2.906 1.230 4.065 4.771 2.793 3.003 #&gt; [25] 3.250 3.390 2.834 2.171 4.654 3.051 4.789 4.373 3.325 3.028 4.127 2.796 #&gt; [37] 2.015 4.379 3.380 3.793 #&gt; [1] 2.257 4.273 4.205 3.251 1.534 3.327 3.596 3.421 2.544 2.110 2.262 2.550 #&gt; [13] 2.790 2.033 3.688 3.440 3.797 3.644 2.906 1.230 4.065 4.771 2.793 3.003 #&gt; [25] 3.250 3.390 2.834 2.171 4.654 3.051 4.789 4.373 3.325 3.028 4.127 2.796 #&gt; [37] 2.015 4.379 3.380 3.793 #&gt; [1] 2.257 4.273 4.205 3.251 1.534 3.327 3.596 3.421 2.544 2.110 2.262 2.550 #&gt; [13] 2.790 2.033 3.688 3.440 3.797 3.644 2.906 1.230 4.065 4.771 2.793 3.003 #&gt; [25] 3.250 3.390 2.834 2.171 4.654 3.051 4.789 4.373 3.325 3.028 4.127 2.796 #&gt; [37] 2.015 4.379 3.380 3.793 # 代替的方法 # stack(x) ## library(tidyverse) ## gather関数(横型→縦型), spread関数(縦型→横型) 1元ANOVAの実行 # 1元ANOVA attach(xdf) res_aov &lt;- aov(score ~ type) summary(res_aov) ### 代替アプローチ (1) oneway.test(score ~ type) # デフォルト：等分散を仮定しない oneway.test(score ~ type, var.equal = T) # 等分散を仮定 ### 代替アプローチ (2) res_lm = lm(score ~ type) res_anova &lt;- anova(res_lm) res_anova # summary()を使わずに出力 detach(xdf) # 等分散検定 var.test(x$A, x$B) boxplot(x) # t検定との比較 t.test(x$A, x$B) t.test(x$A, x$B, var.equal = T) ##t.test(x$A, x$B, paired=T) データ2: 統計小テストデータ KBS専門科目「ビジネス統計」履修者に対して行った, 統計学に対する小テストの結果と, 各受験者のデモグラフィック情報から成るデータセットである. これに対して, ANOVAを実行してみる. year(学年), MF(性別:男性1女性2), AS(文理:文系1その他2理系3), math(数学履修年数), work(勤務年数), stat(統計学経験0-2), MS(経営科学好き嫌い0-3), s4(4級相当得点), s3(3級相当得点), s2(2級相当得点) testdat &lt;- read.csv(&quot;BS_stattest.csv&quot;, header = F) colnames(testdat) &lt;- c(&quot;year&quot;, &quot;MF&quot;, &quot;AS&quot;, &quot;math&quot;, &quot;work&quot;, &quot;stat&quot;, &quot;MS&quot;, &quot;s4&quot;, &quot;s3&quot;, &quot;s2&quot;) # 総合得点の計算, 列に追加 score &lt;- apply(testdat[, c(&quot;s4&quot;, &quot;s3&quot;, &quot;s2&quot;)], 1, sum) testdat2 &lt;- cbind(testdat, score) # モダンな方法 # library(tidyverse) # testdat2 &lt;- testdat %&gt;% mutate(score = s4 + s3 + s2) str(testdat2) #&gt; &#39;data.frame&#39;: 37 obs. of 11 variables: #&gt; $ year : int 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ MF : int 1 1 1 1 2 1 1 1 1 1 ... #&gt; $ AS : int 1 2 1 1 1 1 1 1 1 3 ... #&gt; $ math : int 13 6 8 7 5 5 5 6 8 10 ... #&gt; $ work : int 7 6 4 4 1 5 4 7 0 8 ... #&gt; $ stat : int 1 0 1 0 0 0 0 0 0 0 ... #&gt; $ MS : int 2 3 1 1 1 1 0 1 1 2 ... #&gt; $ s4 : int 7 8 7 7 7 5 5 7 8 7 ... #&gt; $ s3 : int 8 8 6 9 5 8 3 7 6 8 ... #&gt; $ s2 : int 7 8 6 7 4 7 2 6 7 5 ... #&gt; $ score: int 22 24 19 23 16 20 10 20 21 20 ... # → 得点以外の変数も数値(整数)で入力されている # 分割表(クロス集計表) table(testdat2[, c(&quot;MF&quot;, &quot;AS&quot;)]) #&gt; AS #&gt; MF 1 2 3 #&gt; 1 17 3 9 #&gt; 2 4 1 3 # 相関係数 cor(testdat2[, c(&quot;math&quot;, &quot;s2&quot;)]) #&gt; math s2 #&gt; math 1.00000000 0.04241976 #&gt; s2 0.04241976 1.00000000 cor(testdat2[, c(&quot;MS&quot;, &quot;s4&quot;, &quot;s3&quot;, &quot;s2&quot;)]) #&gt; MS s4 s3 s2 #&gt; MS 1.0000000 0.2592955 0.4922877 0.2600562 #&gt; s4 0.2592955 1.0000000 0.1315443 0.1245631 #&gt; s3 0.4922877 0.1315443 1.0000000 0.6435060 #&gt; s2 0.2600562 0.1245631 0.6435060 1.0000000 cor(testdat2[, c(&quot;MS&quot;, &quot;math&quot;)]) #&gt; MS math #&gt; MS 1.0000000 0.2864297 #&gt; math 0.2864297 1.0000000 # 箱ひげ図 attach(testdat2) par(mfrow=c(1,2)) boxplot(score ~ factor(MF)) # 2-level factor boxplot(score ~ factor(MS)) # 4-level factor aaa &lt;- table(testdat2[, c(&quot;MF&quot;, &quot;AS&quot;)]) chisq.test(aaa) #&gt; #&gt; Pearson&#39;s Chi-squared test #&gt; #&gt; data: aaa #&gt; X-squared = 0.18986, df = 2, p-value = 0.9094 aaa &lt;- table(testdat2[, c(&quot;MS&quot;, &quot;AS&quot;)]) chisq.test(aaa) #&gt; #&gt; Pearson&#39;s Chi-squared test #&gt; #&gt; data: aaa #&gt; X-squared = 16.044, df = 6, p-value = 0.01352 1元ANOVA # 1-way ANOVA summary(aov(score ~ factor(MS))) #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; factor(MS) 3 108.2 36.07 3.282 0.0329 * #&gt; Residuals 33 362.8 10.99 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # 代替的アプローチ anova(lm(score ~ factor(MS))) #&gt; Analysis of Variance Table #&gt; #&gt; Response: score #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; factor(MS) 3 108.22 36.074 3.2817 0.03293 * #&gt; Residuals 33 362.75 10.992 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 2元ANOVA 2因子の場合には, 1因子の場合と異なり, 因子間に交互作用 (相互作用) が存在する場合がある. 関数interaction.plotにより交互作用プロットを作図することで, 存在の有無を目で確認することができる. 関数aov()を使用した実行例: ### 2-way ANOVA # 交互作用プロット interaction.plot(factor(MS), factor(MF), score) summary(aov(score ~ factor(MS) + factor(MF))) # 主効果項のみ (交互作用項なし) #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; factor(MS) 3 108.2 36.07 3.571 0.0246 * #&gt; factor(MF) 1 39.5 39.49 3.909 0.0567 . #&gt; Residuals 32 323.3 10.10 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(aov(score ~ factor(MS) * factor(MF))) # 交互作用項有 #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; factor(MS) 3 108.22 36.07 3.727 0.0217 * #&gt; factor(MF) 1 39.49 39.49 4.080 0.0524 . #&gt; factor(MS):factor(MF) 2 32.92 16.46 1.701 0.1997 #&gt; Residuals 30 290.35 9.68 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 代替的に, 関数anova()を使用した実行例: # 代替的アプローチ anova(lm(score ~ factor(MS) + factor(AS))) # 主効果項のみ (交互作用項なし) #&gt; Analysis of Variance Table #&gt; #&gt; Response: score #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; factor(MS) 3 108.22 36.074 3.3147 0.03267 * #&gt; factor(AS) 2 25.38 12.689 1.1659 0.32493 #&gt; Residuals 31 337.37 10.883 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(lm(score ~ factor(AS) + factor(MS))) # 分解順の影響 #&gt; Analysis of Variance Table #&gt; #&gt; Response: score #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; factor(AS) 2 3.50 1.748 0.1607 0.85229 #&gt; factor(MS) 3 130.10 43.368 3.9849 0.01642 * #&gt; Residuals 31 337.37 10.883 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(lm(score ~ factor(MS) * factor(AS))) # 交互作用項有 #&gt; Analysis of Variance Table #&gt; #&gt; Response: score #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; factor(MS) 3 108.22 36.074 3.3063 0.03397 * #&gt; factor(AS) 2 25.38 12.689 1.1629 0.32672 #&gt; factor(MS):factor(AS) 2 20.96 10.478 0.9603 0.39460 #&gt; Residuals 29 316.42 10.911 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 lm(score~factor(MS) + factor(MF)) #&gt; #&gt; Call: #&gt; lm(formula = score ~ factor(MS) + factor(MF)) #&gt; #&gt; Coefficients: #&gt; (Intercept) factor(MS)1 factor(MS)2 factor(MS)3 factor(MF)2 #&gt; 15.552 2.693 3.820 8.448 -2.604 detach(testdat2) (自主課題) 実行結果の解釈をしてみよう. “対応のある分散分析”を実行してみよう. (ヒント: 山田・杉澤・村井 (2008)『Rによるやさしい統計学』「第7章 分散分析」を参照せよ.) データ3: 経営科学アンケート KBS専門科目「ビジネス統計」履修者に対して行った, MBA基礎科目「経営科学」に対するフィードバックの回答と, 各受験者のデモグラフィック情報から成るデータセットである. グループ(group), 負荷(workload), 難易度(difficulty), 有用性(usefulness), 貢献度(contribution), 数学スキル(Math), エクセルスキル(Excel), 文理(AS), 業務経験(Work), 性別(MF) testdat &lt;- read.csv(&quot;FB_dist.csv&quot;, header=T) attach(testdat) # 実行1 aov(difficulty ~ Math) #&gt; Call: #&gt; aov(formula = difficulty ~ Math) #&gt; #&gt; Terms: #&gt; Math Residuals #&gt; Sum of Squares 20.02254 52.62250 #&gt; Deg. of Freedom 1 113 #&gt; #&gt; Residual standard error: 0.6824118 #&gt; Estimated effects may be unbalanced # anova(lm(difficulty ~ Math)) # 実行2 aov(difficulty ~ factor(Math)) #&gt; Call: #&gt; aov(formula = difficulty ~ factor(Math)) #&gt; #&gt; Terms: #&gt; factor(Math) Residuals #&gt; Sum of Squares 21.81657 50.82847 #&gt; Deg. of Freedom 4 110 #&gt; #&gt; Residual standard error: 0.6797624 #&gt; Estimated effects may be unbalanced # anova(lm(difficulty ~ factor(Math))) detach(testdat) (自主課題) 実行結果の解釈をしてみよう. “対応のある分散分析”を実行してみよう. (ヒント: 山田・杉澤・村井 (2008)『Rによるやさしい統計学』「第7章 分散分析」を参照せよ.) 4.4 分散分析と平均値の多重比較* 授業では, 多群における平均値の差の検定の方法として, (1元配置) 分散分析 (ANOVA) を紹介したが, その中で, 2群のt検定を繰り返し実行する方法についての質問があった. (独立な) 検定を繰り返し実行すると実質的な有意水準が低下し, 帰無仮説を想定より棄却しやすくなる現象”多重検定問題”について 言及した. また, 分散分析の結果について追加分析の必要性についても言及した. このように3群以上を比較したい場合に, 分散分析（ANOVA）を実施し, 全体差ありと判定された後に多重比較法 (ペア比較) を実施することをPost-hoc test (事後検定)と呼ぶ. -多重比較法 (MCP): - 複数の仮説 (平均値の差など) を同時に検定する際に, 全体の誤り率 (FWERやFDRなど) を制御するための統計的方法の総称 ここでは, ANOVAによって, 群間に平均値の差ありと判断された後に実践される方法として, Tukeyによる HSD（Honestly Significant Difference）検定について紹介する. これは 一元配置ANOVAの全ペアに対して, 平均の差について同時に検定・同時信頼区間を与える多重比較法である (有意水準の補正を内包). FWER (family-wise error rate, ファミリー内誤差率）を α にコントロール する. (すなわち, 別途 Bonferroni 等の補正は不要) HSD検定は, 群サイズが異なるにも適用可能であり, Tukey–Kramer法とも呼ばれる. FWER: ある多重検定の集合（family）に含まれる仮説群のうち, 少なくとも一つでも誤って棄却する確率: \\(FWER=\\)P(少なくとも1つの誤検出) 前提：独立性・正規性・等分散性. 等分散でない場合 (Games–Howell), 正規性の仮定なし (ノンパラメトリック) の場合 (Steel-Dwass) 以下の出所: ChatGPT (GPT-5) の出力を一部加工 Rによる事後分析の実行例 1) 基本：ANOVA → TukeyHSD set.seed(1) dat &lt;- data.frame( group = rep(LETTERS[1:4], times = c(20, 22, 18, 25)), # サイズ不等でも可 y = c(rnorm(20, 0, 3), rnorm(22, 1, 3), rnorm(18, 3, 3), rnorm(25, -1, 3)) ) # 一元配置 ANOVA fit &lt;- aov(y ~ group, data = dat) # Tukey（R base の同時比較：不等サイズなら内部で Tukey–Kramer） tk &lt;- TukeyHSD(fit) # 同時CIと同時p値（FWER制御） print(tk) #&gt; Tukey multiple comparisons of means #&gt; 95% family-wise confidence level #&gt; #&gt; Fit: aov(formula = y ~ group, data = dat) #&gt; #&gt; $group #&gt; diff lwr upr p adj #&gt; B-A 0.3537944 -1.8739998 2.5815887 0.9754905 #&gt; C-A 2.9607318 0.6180284 5.3034353 0.0073568 #&gt; D-A -1.3820488 -3.5452555 0.7811580 0.3430124 #&gt; C-B 2.6069374 0.3152300 4.8986448 0.0192974 #&gt; D-B -1.7358432 -3.8437158 0.3720294 0.1432848 #&gt; D-C -4.3427806 -6.5717533 -2.1138079 0.0000124 # 可視化 plot(tk) # 各比較の平均差と95%信頼区間が横棒で表示される 参考: ggplot2を用いた可視化の例 library(emmeans) emm &lt;- emmeans(fit, ~ group) pairs &lt;- pairs(emm, adjust = &quot;tukey&quot;) plot(pairs) # latticeベースの図 # ggplot2 に変換 df &lt;- as.data.frame(confint(pairs)) library(ggplot2) ggplot(df, aes(contrast, estimate, ymin = lower.CL, ymax = upper.CL)) + geom_pointrange() + geom_hline(yintercept = 0, linetype = 2) + coord_flip() + theme_minimal() 2) 推定周辺平均（EMM）経由の一般化（推奨） # install.packages(&quot;emmeans&quot;) library(emmeans) emm &lt;- emmeans(fit, ~ group) res &lt;- pairs(emm, adjust = &quot;tukey&quot;) # 同時p値（FWER制御） conf &lt;- confint(pairs(emm), adjust = &quot;tukey&quot;) # 同時CI res #&gt; contrast estimate SE df t.ratio p.value #&gt; A - B -0.354 0.849 81 -0.417 0.9755 #&gt; A - C -2.961 0.893 81 -3.315 0.0074 #&gt; A - D 1.382 0.825 81 1.676 0.3430 #&gt; B - C -2.607 0.874 81 -2.984 0.0193 #&gt; B - D 1.736 0.804 81 2.160 0.1433 #&gt; C - D 4.343 0.850 81 5.111 &lt;.0001 #&gt; #&gt; P value adjustment: tukey method for comparing a family of 4 estimates conf #&gt; contrast estimate SE df lower.CL upper.CL #&gt; A - B -0.354 0.849 81 -2.582 1.874 #&gt; A - C -2.961 0.893 81 -5.303 -0.618 #&gt; A - D 1.382 0.825 81 -0.781 3.545 #&gt; B - C -2.607 0.874 81 -4.899 -0.315 #&gt; B - D 1.736 0.804 81 -0.372 3.844 #&gt; C - D 4.343 0.850 81 2.114 6.572 #&gt; #&gt; Confidence level used: 0.95 #&gt; Conf-level adjustment: tukey method for comparing a family of 4 estimates emmeans() は共変量調整付きモデル (例：線形モデル、GLM、混合効果) でも同様に, 調整済み平均差の Tukey を適用でき, 実務で汎用的である. 3) 単純な「ペアごとの t 検定」を多重比較補正つきで一括実行 Tukey ではなく,「普通のペア t 検定＋p 値補正」をしたい場合: # 等分散仮定の t 検定を全ペアで実施し、p値を同時に補正 ptt &lt;- pairwise.t.test(dat$y, dat$group, p.adjust.method = &quot;holm&quot;, pool.sd = TRUE) ptt #&gt; #&gt; Pairwise comparisons using t tests with pooled SD #&gt; #&gt; data: dat$y and dat$group #&gt; #&gt; A B C #&gt; B 0.6781 - - #&gt; C 0.0069 0.0150 - #&gt; D 0.1952 0.1011 1.3e-05 #&gt; #&gt; P value adjustment method: holm p.adjust.method には “bonferroni”, “holm”, “BH”（FDR）など. こちらは 「t検定＋補正」 であり, Tukey そのものではない. (分布と臨界値が異なる). FWER を厳密に制御したい ANOVA 後の全対比較には, 通常 Tukey を選ぶ. 4) 等分散が怪しいとき（参考：Games–Howell） # install.packages(&quot;rstatix&quot;) library(rstatix) gh &lt;- games_howell_test(dat, y ~ group) # Welch型 + 不等分散・不等サイズに強い gh #&gt; # A tibble: 6 × 8 #&gt; .y. group1 group2 estimate conf.low conf.high p.adj p.adj.signif #&gt; * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 y A B 0.354 -1.82 2.53 0.972 ns #&gt; 2 y A C 2.96 0.654 5.27 0.007 ** #&gt; 3 y A D -1.38 -3.71 0.945 0.396 ns #&gt; 4 y B C 2.61 0.451 4.76 0.013 * #&gt; 5 y B D -1.74 -3.91 0.440 0.16 ns #&gt; 6 y C D -4.34 -6.65 -2.04 0.0000579 **** 等分散仮定を置かない多重比較として実務で広く用いられる (FWER近似制御). "],["線形回帰分析.html", "5 線形回帰分析 5.1 重回帰分析の基本操作 5.2 変数の選択 5.3 説明変数に質的変数を含む回帰 5.4 説明変数に質的変数を含む回帰 (2)", " 5 線形回帰分析 はじめに, コードの可読性を高めるため, パッケージtidyverseをロードしておく. 例えば, tidyverse内にあるパッケージmagrittrの提供する機能であるパイプ (演算子) %&gt;% を関数head()と組合せて使用し, 出力量を抑える. library(tidyverse) 5.1 重回帰分析の基本操作 データ1: 1ルーム賃貸マンション - 1ルーム賃貸マンション, 家賃データ, 50件 (仮想データ) - rent: 月額家賃 (円) - area: 専有面積 (平米) - yrs: 築後年数 (年) - dist: 最寄駅からの徒歩距離 (m) データの読み込み rentdat &lt;- read.csv(&quot;rentdat.csv&quot;, header = T) head(rentdat) # R標準の記法 #&gt; rent area yrs dist #&gt; 1 60000 18.45 8.73 837.46 #&gt; 2 61000 19.84 13.33 520.86 #&gt; 3 74000 22.45 8.26 433.77 #&gt; 4 77000 26.81 5.94 1192.32 #&gt; 5 59000 17.62 3.85 815.17 #&gt; 6 86000 26.68 4.19 373.87 # または, パイプ (%&gt;%) を利用して, # rentdat %&gt;% head() 実行に先立ち, pairs()やcor()を使い, 変数間の従属性や, 相関係数の大きさを確認する. pairs(rentdat) cor(rentdat) #&gt; rent area yrs dist #&gt; rent 1.0000000 0.84098526 -0.16885266 -0.36727009 #&gt; area 0.8409853 1.00000000 0.05454398 -0.02291733 #&gt; yrs -0.1688527 0.05454398 1.00000000 -0.05812975 #&gt; dist -0.3672701 -0.02291733 -0.05812975 1.00000000 # パイプ (%&gt;%) を利用しても良い # rendat %&gt;% pairs() # rentdat %&gt;5 cor() 回帰実行 関数lm()を使用して最小二乗法による適合を行う. 実行結果はsummary()で確認する. res_lm &lt;- lm(rent ~ ., data = rentdat) summary(res_lm) #&gt; #&gt; Call: #&gt; lm(formula = rent ~ ., data = rentdat) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -6732 -2379 -1016 2286 7256 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 32261.469 3652.405 8.833 1.81e-11 *** #&gt; area 2397.144 142.744 16.793 &lt; 2e-16 *** #&gt; yrs -745.440 159.275 -4.680 2.55e-05 *** #&gt; dist -12.443 1.733 -7.180 4.89e-09 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 3531 on 46 degrees of freedom #&gt; Multiple R-squared: 0.8838, Adjusted R-squared: 0.8762 #&gt; F-statistic: 116.6 on 3 and 46 DF, p-value: &lt; 2.2e-16 実行結果の取り出し # 回帰係数の取り出し coef(res_lm) # 関数の利用 #&gt; (Intercept) area yrs dist #&gt; 32261.46944 2397.14374 -745.44017 -12.44341 res_lm$coef # 省略形による指示可能 #&gt; (Intercept) area yrs dist #&gt; 32261.46944 2397.14374 -745.44017 -12.44341 # res_lm$coefficients # 適合値 (予測値) の取り出し fitted(res_lm) %&gt;% head() # head()により, 最初の6行のみ表示 (デフォルト) #&gt; 1 2 3 4 5 6 #&gt; 59560.22 63402.81 74522.43 77264.45 61485.70 88441.65 #res_lm$fitted # 残差の取り出し resid(res_lm) # 関数の利用 #&gt; 1 2 3 4 5 6 7 #&gt; 439.7807 -2402.8084 -522.4320 -264.4500 -2485.7017 -2441.6518 -1029.7327 #&gt; 8 9 10 11 12 13 14 #&gt; 5176.2213 -6732.0408 2427.4126 376.6794 4555.9941 -1754.1073 1534.2533 #&gt; 15 16 17 18 19 20 21 #&gt; 647.1293 -2143.6007 5146.9150 4191.1927 2626.1939 5839.5466 1751.7939 #&gt; 22 23 24 25 26 27 28 #&gt; -5750.1494 -2990.2238 -5767.8528 -2307.8140 -3235.3851 -114.3409 -3829.2694 #&gt; 29 30 31 32 33 34 35 #&gt; 715.0065 -3332.8895 7256.4052 2590.8913 3537.0448 1388.8917 6999.9397 #&gt; 36 37 38 39 40 41 42 #&gt; -3321.0903 -2191.1362 5489.4176 -3603.0246 -1025.7680 -1625.3943 -1887.0020 #&gt; 43 44 45 46 47 48 49 #&gt; -1650.4408 1863.8094 -1133.3323 -2663.5164 5523.2614 -1554.5109 -1311.7071 #&gt; 50 #&gt; -1006.4070 res_lm$resid %&gt;% head() #&gt; 1 2 3 4 5 6 #&gt; 439.7807 -2402.8084 -522.4320 -264.4500 -2485.7017 -2441.6518 # res_lm$residuals # 回帰係数の信頼区間の計算 confint(res_lm) #&gt; 2.5 % 97.5 % #&gt; (Intercept) 24909.55913 39613.379763 #&gt; area 2109.81414 2684.473338 #&gt; yrs -1066.04436 -424.835985 #&gt; dist -15.93178 -8.955039 モデル診断 plot(res_lm$fitted.values, rentdat$rent) # モデル診断: y観測値 vs y適合値 abline(a = 0, b = 1) plot(res_lm$fitted.values, res_lm$residuals) # モデル診断: y適合値 vs 残差 abline(h = 0) par(mfrow=c(2,2)) plot(res_lm) # → resid(res_lm) 適合モデルを使った予測 内挿予測 (適合値の計算) predict(res_lm) %&gt;% head() #&gt; 1 2 3 4 5 6 #&gt; 59560.22 63402.81 74522.43 77264.45 61485.70 88441.65 外挿予測 例. 専有面積=18.8平米, 築後年数=13年, 駅距離=800m, または100mの物件の賃料は? new &lt;- data.frame(area = 18.8, dist = c(800, 100), yrs = 13) predict(res_lm, newdata = new) #&gt; 1 2 #&gt; 57682.32 66392.71 #predict.lm(res_lm, newdata = new) #res_lm$residuals # resid(res_lm) また, summary()には最小二乗推定の各種結果が格納されている. str(summary(res_lm)) #&gt; List of 11 #&gt; $ call : language lm(formula = rent ~ ., data = rentdat) #&gt; $ terms :Classes &#39;terms&#39;, &#39;formula&#39; language rent ~ area + yrs + dist #&gt; .. ..- attr(*, &quot;variables&quot;)= language list(rent, area, yrs, dist) #&gt; .. ..- attr(*, &quot;factors&quot;)= int [1:4, 1:3] 0 1 0 0 0 0 1 0 0 0 ... #&gt; .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 #&gt; .. .. .. ..$ : chr [1:4] &quot;rent&quot; &quot;area&quot; &quot;yrs&quot; &quot;dist&quot; #&gt; .. .. .. ..$ : chr [1:3] &quot;area&quot; &quot;yrs&quot; &quot;dist&quot; #&gt; .. ..- attr(*, &quot;term.labels&quot;)= chr [1:3] &quot;area&quot; &quot;yrs&quot; &quot;dist&quot; #&gt; .. ..- attr(*, &quot;order&quot;)= int [1:3] 1 1 1 #&gt; .. ..- attr(*, &quot;intercept&quot;)= int 1 #&gt; .. ..- attr(*, &quot;response&quot;)= int 1 #&gt; .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; #&gt; .. ..- attr(*, &quot;predvars&quot;)= language list(rent, area, yrs, dist) #&gt; .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:4] &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; #&gt; .. .. ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;rent&quot; &quot;area&quot; &quot;yrs&quot; &quot;dist&quot; #&gt; $ residuals : Named num [1:50] 440 -2403 -522 -264 -2486 ... #&gt; ..- attr(*, &quot;names&quot;)= chr [1:50] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... #&gt; $ coefficients : num [1:4, 1:4] 32261.5 2397.1 -745.4 -12.4 3652.4 ... #&gt; ..- attr(*, &quot;dimnames&quot;)=List of 2 #&gt; .. ..$ : chr [1:4] &quot;(Intercept)&quot; &quot;area&quot; &quot;yrs&quot; &quot;dist&quot; #&gt; .. ..$ : chr [1:4] &quot;Estimate&quot; &quot;Std. Error&quot; &quot;t value&quot; &quot;Pr(&gt;|t|)&quot; #&gt; $ aliased : Named logi [1:4] FALSE FALSE FALSE FALSE #&gt; ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;(Intercept)&quot; &quot;area&quot; &quot;yrs&quot; &quot;dist&quot; #&gt; $ sigma : num 3531 #&gt; $ df : int [1:3] 4 46 4 #&gt; $ r.squared : num 0.884 #&gt; $ adj.r.squared: num 0.876 #&gt; $ fstatistic : Named num [1:3] 117 3 46 #&gt; ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;value&quot; &quot;numdf&quot; &quot;dendf&quot; #&gt; $ cov.unscaled : num [1:4, 1:4] 1.069834 -0.035212 -0.017107 -0.000183 -0.035212 ... #&gt; ..- attr(*, &quot;dimnames&quot;)=List of 2 #&gt; .. ..$ : chr [1:4] &quot;(Intercept)&quot; &quot;area&quot; &quot;yrs&quot; &quot;dist&quot; #&gt; .. ..$ : chr [1:4] &quot;(Intercept)&quot; &quot;area&quot; &quot;yrs&quot; &quot;dist&quot; #&gt; - attr(*, &quot;class&quot;)= chr &quot;summary.lm&quot; summary(res_lm)$r.squared # R2 #&gt; [1] 0.8837688 # summary(res_lm)[&quot;r.squared&quot;] # 別の指定方法 summary(res_lm)$adj.r.squared # 補正R2 #&gt; [1] 0.8761885 summary(res_lm)$coef #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 32261.46944 3652.405183 8.832938 1.805979e-11 #&gt; area 2397.14374 142.744411 16.793258 3.002396e-21 #&gt; yrs -745.44017 159.275119 -4.680205 2.548844e-05 #&gt; dist -12.44341 1.733012 -7.180222 4.893016e-09 標準化 (偏) 回帰係数 あらかじめ変数を標準化しておいてからlm()を実行すると, 標準化偏回帰係数が得られる. # 標準(化)回帰係数 srentdat &lt;- scale(rentdat) # scale()の返り値はリスト型 → データフレームへ変換 srentdat &lt;- data.frame(srentdat) sres_lm &lt;- lm(rent ~ area + yrs + dist, data = srentdat) summary(sres_lm) #&gt; #&gt; Call: #&gt; lm(formula = rent ~ area + yrs + dist, data = srentdat) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -0.6708 -0.2371 -0.1013 0.2278 0.7231 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -5.959e-16 4.976e-02 0.00 1 #&gt; area 8.456e-01 5.035e-02 16.79 &lt; 2e-16 *** #&gt; yrs -2.360e-01 5.042e-02 -4.68 2.55e-05 *** #&gt; dist -3.616e-01 5.036e-02 -7.18 4.89e-09 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.3519 on 46 degrees of freedom #&gt; Multiple R-squared: 0.8838, Adjusted R-squared: 0.8762 #&gt; F-statistic: 116.6 on 3 and 46 DF, p-value: &lt; 2.2e-16 # 偏回帰係数 vs 標準(化)偏回帰係数 summary(res_lm)[&quot;coefficients&quot;] #&gt; $coefficients #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 32261.46944 3652.405183 8.832938 1.805979e-11 #&gt; area 2397.14374 142.744411 16.793258 3.002396e-21 #&gt; yrs -745.44017 159.275119 -4.680205 2.548844e-05 #&gt; dist -12.44341 1.733012 -7.180222 4.893016e-09 summary(sres_lm)[&quot;coefficients&quot;] #&gt; $coefficients #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -5.958724e-16 0.04976173 -1.197451e-14 1.000000e+00 #&gt; area 8.455702e-01 0.05035176 1.679326e+01 3.002396e-21 #&gt; yrs -2.359937e-01 0.05042380 -4.680205e+00 2.548844e-05 #&gt; dist -3.616101e-01 0.05036197 -7.180222e+00 4.893016e-09 # # 確認 y_sd &lt;- sd(rentdat$rent) x_sd &lt;- apply(rentdat[, -1], 2, sd) res_lm$coef[ -1] * x_sd / y_sd #&gt; area yrs dist #&gt; 0.8455702 -0.2359937 -0.3616101 5.2 変数の選択 データ2: ボストン市内住宅物件価格データ Boston housingデータセットは, Harrison and Rubinfeld (78) で分析に使用された. これは, ボストン地域の住宅に関するデータセットで, もともとは, アメリカ合衆国国勢調査局 (U.S. Census Service) によって収集されたものに基づいている. 今日までに, 統計学・機械学習の教育や研究で広く利用されている. データセットの各行 (レコード) は, ボストン標準大都市統計地域 (Boston Standard Metropolitan Statistical Area, SMSA) 内の1つの国勢調査区（census tract）に対応する. 各国勢調査区は複数の住宅を含む地域単位であるため, 各行は個別の住宅1軒を表すものではない. Harrison, D., &amp; Rubinfeld, D. L. (1978). Hedonic prices and the demand for clean air. Journal of Environmental Economics and Management, 5(1), 81–102. (7/7/25) 変数disの訳がミスリーディングだったため, 訂正いたします. (旧)雇用センター → (新)雇用中心地 (employment centers) - Boston Housingデータセット - crim: 地域の一人当たり犯罪率 - zn: 25,000平方フィート以上の住宅用地の割合 - indus: 地域の非小売業の土地の割合 - chas: チャールズ川のダミー変数 (1: 川沿い, 0: それ以外) - nox: 窒素酸化物濃度（1000万ppm） - rm: 住宅の平均部屋数 - age: 1940年以前に建設された持ち家の割合 - dis: ボストンの5つの雇用中心地 (employment centers) までの距離の加重平均 - rad: 放射状高速道路へのアクセス指数 - tax: 10,000米ドル当たりの固定資産税率 - ptratio: 地域の生徒数・教師数比率 - b: 人種的指標, 1000(B - 0.63)^2, (Bは地域の黒人の割合) - lstat: 低所得者層の割合 - medv: 持ち家住宅の中央値（1000ドル単位） - 506件 x 14変数 (オリジナル版) - 本セクションで使用するバージョンの出所: http://lib.stat.cmu.edu/datasets/boston 注意: Harrison and Rubinfeld (78) の原文には, “employment centers&quot;に関する明確な説明はないものの, `dis`の定義として, “Weighted distances to five employment centers in the Boston region. According to traditional theories of urban land rent gradients, housing values should be higher near employment renters. DIS is entered in logarithm form; the expected sign is negative.”(p.97) とある. また, 不動産市場の文献における“accessibility to employment centers&quot;等の用法を調べる限りにおいて, “employment centers&quot;をいわゆる“職業安定所&quot;と解釈するのは誤りで, むしろ, (ビジネスが集まり労働人口の多い) &quot;雇用の中心地&quot;, “雇用集積地&quot;等と解釈するのが適切と考えられる. # library(MASS) # Bostonデータセット # housing &lt;- Boston # 変数bではなくblack housing &lt;- read.csv(&quot;boston_housing.csv&quot;, header = T) housing %&gt;% head() #&gt; crim zn indus chas nox rm age dis rad tax ptratio b lstat #&gt; 1 0.00632 18 2.31 0 0.538 6.575 65.2 4.0900 1 296 15.3 396.90 4.98 #&gt; 2 0.02731 0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 396.90 9.14 #&gt; 3 0.02729 0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 392.83 4.03 #&gt; 4 0.03237 0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 394.63 2.94 #&gt; 5 0.06905 0 2.18 0 0.458 7.147 54.2 6.0622 3 222 18.7 396.90 5.33 #&gt; 6 0.02985 0 2.18 0 0.458 6.430 58.7 6.0622 3 222 18.7 394.12 5.21 #&gt; medv #&gt; 1 24.0 #&gt; 2 21.6 #&gt; 3 34.7 #&gt; 4 33.4 #&gt; 5 36.2 #&gt; 6 28.7 chasはダミー変数 (0/1) のため, 一旦除去して変数間の相関等を調べる. ライブラリcorrplotの関数corrplot()を使うと, 相関係数のヒートマップを作成することができる. # 散布図行列 pairs(housing[, -4]) # chas (バイナリ) を除去 round(cor(housing[, -4]), 2) # chas(バイナリ)を除去 #&gt; crim zn indus nox rm age dis rad tax ptratio b #&gt; crim 1.00 -0.20 0.41 0.42 -0.22 0.35 -0.38 0.63 0.58 0.29 -0.39 #&gt; zn -0.20 1.00 -0.53 -0.52 0.31 -0.57 0.66 -0.31 -0.31 -0.39 0.18 #&gt; indus 0.41 -0.53 1.00 0.76 -0.39 0.64 -0.71 0.60 0.72 0.38 -0.36 #&gt; nox 0.42 -0.52 0.76 1.00 -0.30 0.73 -0.77 0.61 0.67 0.19 -0.38 #&gt; rm -0.22 0.31 -0.39 -0.30 1.00 -0.24 0.21 -0.21 -0.29 -0.36 0.13 #&gt; age 0.35 -0.57 0.64 0.73 -0.24 1.00 -0.75 0.46 0.51 0.26 -0.27 #&gt; dis -0.38 0.66 -0.71 -0.77 0.21 -0.75 1.00 -0.49 -0.53 -0.23 0.29 #&gt; rad 0.63 -0.31 0.60 0.61 -0.21 0.46 -0.49 1.00 0.91 0.46 -0.44 #&gt; tax 0.58 -0.31 0.72 0.67 -0.29 0.51 -0.53 0.91 1.00 0.46 -0.44 #&gt; ptratio 0.29 -0.39 0.38 0.19 -0.36 0.26 -0.23 0.46 0.46 1.00 -0.18 #&gt; b -0.39 0.18 -0.36 -0.38 0.13 -0.27 0.29 -0.44 -0.44 -0.18 1.00 #&gt; lstat 0.46 -0.41 0.60 0.59 -0.61 0.60 -0.50 0.49 0.54 0.37 -0.37 #&gt; medv -0.39 0.36 -0.48 -0.43 0.70 -0.38 0.25 -0.38 -0.47 -0.51 0.33 #&gt; lstat medv #&gt; crim 0.46 -0.39 #&gt; zn -0.41 0.36 #&gt; indus 0.60 -0.48 #&gt; nox 0.59 -0.43 #&gt; rm -0.61 0.70 #&gt; age 0.60 -0.38 #&gt; dis -0.50 0.25 #&gt; rad 0.49 -0.38 #&gt; tax 0.54 -0.47 #&gt; ptratio 0.37 -0.51 #&gt; b -0.37 0.33 #&gt; lstat 1.00 -0.74 #&gt; medv -0.74 1.00 # pairs(housing) # round(cor(housing), 2) library(corrplot) corrplot(cor(housing[, -4])) # corrplot # corrplot(cor(housing)) # corrplot 4変数に絞り込み res_lm1 = lm(medv ~ crim + rm + tax + lstat, data = housing) summary(res_lm1) #&gt; #&gt; Call: #&gt; lm(formula = medv ~ crim + rm + tax + lstat, data = housing) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -16.383 -3.497 -1.149 1.825 30.716 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -1.414928 3.178364 -0.445 0.6564 #&gt; crim -0.061579 0.035562 -1.732 0.0840 . #&gt; rm 5.248721 0.439664 11.938 &lt;2e-16 *** #&gt; tax -0.005018 0.001922 -2.611 0.0093 ** #&gt; lstat -0.534835 0.050258 -10.642 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.458 on 501 degrees of freedom #&gt; Multiple R-squared: 0.6506, Adjusted R-squared: 0.6478 #&gt; F-statistic: 233.2 on 4 and 501 DF, p-value: &lt; 2.2e-16 anova(res_lm1) #&gt; Analysis of Variance Table #&gt; #&gt; Response: medv #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; crim 1 6440.8 6440.8 216.206 &lt; 2.2e-16 *** #&gt; rm 1 16709.7 16709.7 560.915 &lt; 2.2e-16 *** #&gt; tax 1 1267.3 1267.3 42.542 1.693e-10 *** #&gt; lstat 1 3373.6 3373.6 113.247 &lt; 2.2e-16 *** #&gt; Residuals 501 14924.8 29.8 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # update関数でモデル更新: 変数ptratio追加 res_lm2 &lt;- update(res_lm1, . ~ . + ptratio) summary(res_lm2) #&gt; #&gt; Call: #&gt; lm(formula = medv ~ crim + rm + tax + lstat + ptratio, data = housing) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -14.3602 -3.1111 -0.9237 1.6569 30.4116 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 16.7488084 4.0001180 4.187 3.34e-05 *** #&gt; crim -0.0593795 0.0339830 -1.747 0.0812 . #&gt; rm 4.6349234 0.4292367 10.798 &lt; 2e-16 *** #&gt; tax -0.0008196 0.0019328 -0.424 0.6717 #&gt; lstat -0.5280046 0.0480346 -10.992 &lt; 2e-16 *** #&gt; ptratio -0.8731668 0.1251429 -6.977 9.59e-12 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.215 on 500 degrees of freedom #&gt; Multiple R-squared: 0.6816, Adjusted R-squared: 0.6784 #&gt; F-statistic: 214.1 on 5 and 500 DF, p-value: &lt; 2.2e-16 anova(res_lm2) #&gt; Analysis of Variance Table #&gt; #&gt; Response: medv #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; crim 1 6440.8 6440.8 236.784 &lt; 2.2e-16 *** #&gt; rm 1 16709.7 16709.7 614.301 &lt; 2.2e-16 *** #&gt; tax 1 1267.3 1267.3 46.591 2.540e-11 *** #&gt; lstat 1 3373.6 3373.6 124.026 &lt; 2.2e-16 *** #&gt; ptratio 1 1324.2 1324.2 48.684 9.589e-12 *** #&gt; Residuals 500 13600.6 27.2 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # 変数zn追加 res_lm3 &lt;- update(res_lm2, . ~ . + zn) summary(res_lm3) #&gt; #&gt; Call: #&gt; lm(formula = medv ~ crim + rm + tax + lstat + ptratio + zn, data = housing) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -14.4790 -3.1374 -0.8754 1.6871 30.3185 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 17.3073953 4.0780517 4.244 2.62e-05 *** #&gt; crim -0.0584021 0.0340274 -1.716 0.0867 . #&gt; rm 4.6460026 0.4297290 10.811 &lt; 2e-16 *** #&gt; tax -0.0008832 0.0019358 -0.456 0.6484 #&gt; lstat -0.5354553 0.0491813 -10.887 &lt; 2e-16 *** #&gt; ptratio -0.8958719 0.1291910 -6.934 1.27e-11 *** #&gt; zn -0.0081367 0.0114124 -0.713 0.4762 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.218 on 499 degrees of freedom #&gt; Multiple R-squared: 0.6819, Adjusted R-squared: 0.6781 #&gt; F-statistic: 178.3 on 6 and 499 DF, p-value: &lt; 2.2e-16 anova(res_lm3) #&gt; Analysis of Variance Table #&gt; #&gt; Response: medv #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; crim 1 6440.8 6440.8 236.5506 &lt; 2.2e-16 *** #&gt; rm 1 16709.7 16709.7 613.6973 &lt; 2.2e-16 *** #&gt; tax 1 1267.3 1267.3 46.5455 2.601e-11 *** #&gt; lstat 1 3373.6 3373.6 123.9040 &lt; 2.2e-16 *** #&gt; ptratio 1 1324.2 1324.2 48.6356 9.826e-12 *** #&gt; zn 1 13.8 13.8 0.5083 0.4762 #&gt; Residuals 499 13586.7 27.2 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # 変数nox追加, zn除去 res_lm4 &lt;- update(res_lm3, . ~ . + nox - zn) summary(res_lm4) #&gt; #&gt; Call: #&gt; lm(formula = medv ~ crim + rm + tax + lstat + ptratio + nox, #&gt; data = housing) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -14.2389 -3.1372 -0.9454 1.6680 30.4687 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 17.2649269 4.2731659 4.040 6.18e-05 *** #&gt; crim -0.0596990 0.0340256 -1.755 0.080 . #&gt; rm 4.6382386 0.4297223 10.794 &lt; 2e-16 *** #&gt; tax -0.0004089 0.0022705 -0.180 0.857 #&gt; lstat -0.5216846 0.0514382 -10.142 &lt; 2e-16 *** #&gt; ptratio -0.8844707 0.1294545 -6.832 2.44e-11 *** #&gt; nox -1.0363053 2.9989281 -0.346 0.730 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.22 on 499 degrees of freedom #&gt; Multiple R-squared: 0.6817, Adjusted R-squared: 0.6779 #&gt; F-statistic: 178.1 on 6 and 499 DF, p-value: &lt; 2.2e-16 目的変数medvと説明変数lstatには, 明らかに非線形な関係性が見られる. そこで, lstatに非線形変換を施すことで, 適合度が改善できる可能性がある. pairs(housing[, c(&quot;medv&quot;, &quot;crim&quot;, &quot;rm&quot;, &quot;tax&quot;, &quot;lstat&quot;)]) round(cor(housing[, c(&quot;medv&quot;, &quot;crim&quot;, &quot;rm&quot;, &quot;tax&quot;, &quot;lstat&quot;)]), 2) #&gt; medv crim rm tax lstat #&gt; medv 1.00 -0.39 0.70 -0.47 -0.74 #&gt; crim -0.39 1.00 -0.22 0.58 0.46 #&gt; rm 0.70 -0.22 1.00 -0.29 -0.61 #&gt; tax -0.47 0.58 -0.29 1.00 0.54 #&gt; lstat -0.74 0.46 -0.61 0.54 1.00 # 変数lstatの逆数を新変数invlstatとして定義し, モデルに追加 data2 &lt;- data.frame(housing, invlstat = 1 / housing$lstat) res_lm5 &lt;- lm(medv ~ crim + rm + tax + ptratio + invlstat, data = data2) plot(housing$medv, 1 / housing$lstat) summary(res_lm5) #&gt; #&gt; Call: #&gt; lm(formula = medv ~ crim + rm + tax + ptratio + invlstat, data = data2) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -14.9062 -2.6032 -0.5276 2.1041 31.2592 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 6.665018 3.295520 2.022 0.0437 * #&gt; crim -0.119564 0.030121 -3.969 8.26e-05 *** #&gt; rm 3.609393 0.394880 9.140 &lt; 2e-16 *** #&gt; tax -0.002272 0.001693 -1.342 0.1802 #&gt; ptratio -0.665188 0.114156 -5.827 1.01e-08 *** #&gt; invlstat 60.465938 3.762069 16.073 &lt; 2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 4.719 on 500 degrees of freedom #&gt; Multiple R-squared: 0.7393, Adjusted R-squared: 0.7367 #&gt; F-statistic: 283.6 on 5 and 500 DF, p-value: &lt; 2.2e-16 crimもmedvと非線形な関係があるため, これを適当に非線形変換することで更に改善できる余地がある (各自で試して欲しい). 標準的なモデル選択規準であるAICやBICは, 関数AIC(), BIC()によって計算することができる. # AIC, BICの計算 AIC(res_lm5, res_lm2) #&gt; df AIC #&gt; res_lm5 7 3014.149 #&gt; res_lm2 7 3115.379 BIC(res_lm5, res_lm2) #&gt; df BIC #&gt; res_lm5 7 3043.735 #&gt; res_lm2 7 3144.965 AIC, BIC双方とも, res_lm5はres_lm2より望ましいことを示している. 関数anova()を使って, 分散分析によって (包含関係になる) モデル間の比較をすることができる. # 追加 (除去) した変数群の有意性 (例) anova(res_lm1, res_lm3, test = &quot;F&quot;) # F検定 #&gt; Analysis of Variance Table #&gt; #&gt; Model 1: medv ~ crim + rm + tax + lstat #&gt; Model 2: medv ~ crim + rm + tax + lstat + ptratio + zn #&gt; Res.Df RSS Df Sum of Sq F Pr(&gt;F) #&gt; 1 501 14925 #&gt; 2 499 13587 2 1338.1 24.572 6.636e-11 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(res_lm3, res_lm1, test = &quot;F&quot;) # 実質的に同一 #&gt; Analysis of Variance Table #&gt; #&gt; Model 1: medv ~ crim + rm + tax + lstat + ptratio + zn #&gt; Model 2: medv ~ crim + rm + tax + lstat #&gt; Res.Df RSS Df Sum of Sq F Pr(&gt;F) #&gt; 1 499 13587 #&gt; 2 501 14925 -2 -1338.1 24.572 6.636e-11 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # anova(res_lm1, res_lm2, test = &quot;LRT&quot;) # 尤度比検定 ステップワイズ法による変数選択 ステップワイズ法は, 関数lm()の実行結果オブジェクトを, 関数step()に入力として与えることで実行することができる. # step(); AICによって決定 # scope: モデルサーチの範囲 (追加や削除を検討するべき変数を指定) # scope指定ない場合: # - directionのデフォルトは, 変数減少法 (後方削除) # - モデルサーチ上限 (upper) は, 初期モデル # scope指定ある場合: # - directionのデフォルトは, 変数増減法 # - scopeがリストでなく, 単一式で与えらている場合, upperモデルと解釈 (lowerは欠損) res_lm_all = lm(medv ~ ., data = housing) # → 13変数 res_lm_all_2 = lm(medv ~ 1, data = housing) # → y切片のみ (変数なし) # step(res_lm5) # 変数減少法 (scopeない場合のデフォルト) #&gt; Start: AIC=1576.18 #&gt; medv ~ crim + rm + tax + ptratio + invlstat #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - tax 1 40.1 11175 1576.0 #&gt; &lt;none&gt; 11134 1576.2 #&gt; - crim 1 350.9 11485 1589.9 #&gt; - ptratio 1 756.1 11891 1607.4 #&gt; - rm 1 1860.5 12995 1652.4 #&gt; - invlstat 1 5752.7 16887 1784.9 #&gt; #&gt; Step: AIC=1576 #&gt; medv ~ crim + rm + ptratio + invlstat #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; &lt;none&gt; 11175 1576.0 #&gt; - crim 1 643.8 11818 1602.3 #&gt; - ptratio 1 951.0 12126 1615.3 #&gt; - rm 1 1847.9 13023 1651.4 #&gt; - invlstat 1 6153.6 17328 1796.0 #&gt; #&gt; Call: #&gt; lm(formula = medv ~ crim + rm + ptratio + invlstat, data = data2) #&gt; #&gt; Coefficients: #&gt; (Intercept) crim rm ptratio invlstat #&gt; 6.6395 -0.1399 3.5960 -0.7113 61.4172 #step(res_lm5, direction = &quot;forward&quot;) # 変数増加法 (上限は初期モデル) #step(res_lm5, direction = &quot;both&quot;) # 変数増減法 (上限は初期モデル) # 採用する変数の上限・下限の指定 step(res_lm_all, scope = list(lower = ~ crim + rm)) # 下限のモデルを指定. 変数増減法 #&gt; Start: AIC=1589.64 #&gt; medv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + #&gt; tax + ptratio + b + lstat #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - age 1 0.06 11079 1587.7 #&gt; - indus 1 2.52 11081 1587.8 #&gt; &lt;none&gt; 11079 1589.6 #&gt; - chas 1 218.97 11298 1597.5 #&gt; - tax 1 242.26 11321 1598.6 #&gt; - zn 1 257.49 11336 1599.3 #&gt; - b 1 270.63 11349 1599.8 #&gt; - rad 1 479.15 11558 1609.1 #&gt; - nox 1 487.16 11566 1609.4 #&gt; - ptratio 1 1194.23 12273 1639.4 #&gt; - dis 1 1232.41 12311 1641.0 #&gt; - lstat 1 2410.84 13490 1687.3 #&gt; #&gt; Step: AIC=1587.65 #&gt; medv ~ crim + zn + indus + chas + nox + rm + dis + rad + tax + #&gt; ptratio + b + lstat #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - indus 1 2.52 11081 1585.8 #&gt; &lt;none&gt; 11079 1587.7 #&gt; - chas 1 219.91 11299 1595.6 #&gt; - tax 1 242.24 11321 1596.6 #&gt; - zn 1 260.32 11339 1597.4 #&gt; - b 1 272.26 11351 1597.9 #&gt; - rad 1 481.09 11560 1607.2 #&gt; - nox 1 520.87 11600 1608.9 #&gt; - ptratio 1 1200.23 12279 1637.7 #&gt; - dis 1 1352.26 12431 1643.9 #&gt; - lstat 1 2718.88 13798 1696.7 #&gt; #&gt; Step: AIC=1585.76 #&gt; medv ~ crim + zn + chas + nox + rm + dis + rad + tax + ptratio + #&gt; b + lstat #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; &lt;none&gt; 11081 1585.8 #&gt; - chas 1 227.21 11309 1594.0 #&gt; - zn 1 257.82 11339 1595.4 #&gt; - b 1 270.82 11352 1596.0 #&gt; - tax 1 273.62 11355 1596.1 #&gt; - rad 1 500.92 11582 1606.1 #&gt; - nox 1 541.91 11623 1607.9 #&gt; - ptratio 1 1206.45 12288 1636.0 #&gt; - dis 1 1448.94 12530 1645.9 #&gt; - lstat 1 2723.48 13805 1695.0 #&gt; #&gt; Call: #&gt; lm(formula = medv ~ crim + zn + chas + nox + rm + dis + rad + #&gt; tax + ptratio + b + lstat, data = housing) #&gt; #&gt; Coefficients: #&gt; (Intercept) crim zn chas nox rm #&gt; 36.341145 -0.108413 0.045845 2.718716 -17.376023 3.801579 #&gt; dis rad tax ptratio b lstat #&gt; -1.492711 0.299608 -0.011778 -0.946525 0.009291 -0.522553 step(res_lm_all_2, scope = list(upper = ~ crim + rm)) # 上限のモデルを指定. 変数増減法 #&gt; Start: AIC=2246.51 #&gt; medv ~ 1 #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; + rm 1 20654.4 22062 1914.2 #&gt; + crim 1 6440.8 36276 2165.8 #&gt; &lt;none&gt; 42716 2246.5 #&gt; #&gt; Step: AIC=1914.19 #&gt; medv ~ rm #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; + crim 1 2496.1 19566 1855.4 #&gt; &lt;none&gt; 22062 1914.2 #&gt; - rm 1 20654.4 42716 2246.5 #&gt; #&gt; Step: AIC=1855.43 #&gt; medv ~ rm + crim #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; &lt;none&gt; 19566 1855.4 #&gt; - crim 1 2496.1 22062 1914.2 #&gt; - rm 1 16709.7 36276 2165.8 #&gt; #&gt; Call: #&gt; lm(formula = medv ~ rm + crim, data = housing) #&gt; #&gt; Coefficients: #&gt; (Intercept) rm crim #&gt; -29.2447 8.3911 -0.2649 step(res_lm1, scope = list(upper = ~ crim + rm + tax + lstat + ptratio + b, lower = ~ crim + rm)) #&gt; Start: AIC=1722.43 #&gt; medv ~ crim + rm + tax + lstat #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; + ptratio 1 1324.2 13601 1677.4 #&gt; + b 1 255.6 14669 1715.7 #&gt; &lt;none&gt; 14925 1722.4 #&gt; - tax 1 203.1 15128 1727.3 #&gt; - lstat 1 3373.6 18298 1823.5 #&gt; #&gt; Step: AIC=1677.41 #&gt; medv ~ crim + rm + tax + lstat + ptratio #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; + b 1 306.0 13295 1667.9 #&gt; - tax 1 4.9 13606 1675.6 #&gt; &lt;none&gt; 13601 1677.4 #&gt; - ptratio 1 1324.2 14925 1722.4 #&gt; - lstat 1 3286.7 16887 1784.9 #&gt; #&gt; Step: AIC=1667.9 #&gt; medv ~ crim + rm + tax + lstat + ptratio + b #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - tax 1 3.06 13298 1666.0 #&gt; &lt;none&gt; 13295 1667.9 #&gt; - b 1 306.02 13601 1677.4 #&gt; - ptratio 1 1374.66 14669 1715.7 #&gt; - lstat 1 2849.76 16144 1764.2 #&gt; #&gt; Step: AIC=1666.01 #&gt; medv ~ crim + rm + lstat + ptratio + b #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; &lt;none&gt; 13298 1666.0 #&gt; + tax 1 3.06 13295 1667.9 #&gt; - b 1 307.85 13606 1675.6 #&gt; - ptratio 1 1478.71 14776 1717.4 #&gt; - lstat 1 3001.77 16299 1767.0 #&gt; #&gt; Call: #&gt; lm(formula = medv ~ crim + rm + lstat + ptratio + b, data = housing) #&gt; #&gt; Coefficients: #&gt; (Intercept) crim rm lstat ptratio b #&gt; 11.615006 -0.038921 4.788176 -0.495139 -0.877249 0.009593 # 上限・下限を同時に指定. 変数増減法 # ----------------------------------------# 多重共線性のチェックについて VIF (Variance Inflation Factor) によって, 説明変数間の多重共線性 (マルチコ) の 有無を確認することができる. VIFによる多重共線性への対応に関する慣用ルールとして, 5以上の値を持つ説明変数は要注意, 10以上の変数は除去するのが良いとされている. # VIF # install.packages(&quot;car&quot;) # or RStudio, Tools → Install packages, library(car) # &quot;Companion to Applied Regression&quot; package vif(res_lm_all) # 全13説明変数についてVIFを計算 #&gt; crim zn indus chas nox rm age dis #&gt; 1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 #&gt; rad tax ptratio b lstat #&gt; 7.484496 9.008554 1.799084 1.348521 2.941491 → 変数taxがVIF値が最大 (9.01) と10に近いことから, これを除いてVIF値を再計算してみる. vif(update(res_lm_all, . ~ . - tax)) # VIF値最大のtaxを除いてVIF値を再計算 #&gt; crim zn indus chas nox rm age dis #&gt; 1.791940 2.184240 3.226015 1.058220 4.369271 1.923075 3.098044 3.954446 #&gt; rad ptratio b lstat #&gt; 2.837494 1.788839 1.347564 2.940800 → taxの次に大きかったradのVIF値が大きく減少. 上でみたとおり, もともと, taxとradは相関が高かった (0.91). 線形回帰分析におけるマルチコを考慮した変数選択法としては, 例えば, Kariya, Kurata and Hayashi (2024) の提案した”Empirically Effective Modelling Methodology (EEM-M)“がある. 参考文献 Kariya, Kurata and Hayashi (2024). “A Modelling Framework for Regression with Collinearity”, Journal of Statistical Planning and Inference, 228 (1), Pages 95-115. EEM-Mアプリ 5.3 説明変数に質的変数を含む回帰 データセット#3: 高速道路事故データ - Hoffstedt’s Highway accident data - adt：1日の平均交通量（単位：千台) - trks：総交通量に占めるトラック交通量の割合 - lane：交通の総車線数 - acpt：1マイルあたりのアクセスポイント数 - sigs: 1マイルあたりの信号付きインターチェンジの数 - itg：1マイルあたりの高速道路型インターチェンジの数 - slim：1973年の制限速度 - lwid: 車線幅（フィート単位） - shld: 車道の外側路肩の幅（フィート単位) - htype: 道路の種類または道路の財源を示す指標: &quot;mc&quot;: メジャーコレクター (major collector), &quot;fai&quot;: 州間 (interstate) 高速道路, &quot;pa&quot;: 地域・都市間主要幹線 (principal arterial) 道路, &quot;ma&quot;; 地域・都市内主要幹線 (major arterial) 道路 - rate: 1973年の事故発生率（百万車両マイル当たり） - 注) htypeは4-水準因子 - 参考文献: Weisberg (2014), Applied Linear Regression, 4th Ed., Wiley. library(alr4) data(Highway) str(Highway) #&gt; &#39;data.frame&#39;: 39 obs. of 12 variables: #&gt; $ adt : int 69 73 49 61 28 30 46 25 43 23 ... #&gt; $ trks : int 8 8 10 13 12 6 8 9 12 7 ... #&gt; $ lane : int 8 4 4 6 4 4 4 4 4 4 ... #&gt; $ acpt : num 4.6 4.4 4.7 3.8 2.2 24.8 11 18.5 7.5 8.2 ... #&gt; $ sigs : num 0 0 0 0 0 1.84 0.7 0.38 1.39 1.21 ... #&gt; $ itg : num 1.2 1.43 1.54 0.94 0.65 0.34 0.47 0.38 0.95 0.12 ... #&gt; $ slim : int 55 60 60 65 70 55 55 55 50 50 ... #&gt; $ len : num 4.99 16.11 9.75 10.65 20.01 ... #&gt; $ lwid : int 12 12 12 12 12 12 12 12 12 12 ... #&gt; $ shld : int 10 10 10 10 10 10 8 10 4 5 ... #&gt; $ htype: Factor w/ 4 levels &quot;mc&quot;,&quot;fai&quot;,&quot;pa&quot;,..: 2 2 2 2 2 3 3 3 3 3 ... #&gt; $ rate : num 4.58 2.86 3.02 2.29 1.61 6.87 3.85 6.12 3.29 5.88 ... Highway %&gt;% head() #&gt; adt trks lane acpt sigs itg slim len lwid shld htype rate #&gt; 1 69 8 8 4.6 0.00 1.20 55 4.99 12 10 fai 4.58 #&gt; 2 73 8 4 4.4 0.00 1.43 60 16.11 12 10 fai 2.86 #&gt; 3 49 10 4 4.7 0.00 1.54 60 9.75 12 10 fai 3.02 #&gt; 4 61 13 6 3.8 0.00 0.94 65 10.65 12 10 fai 2.29 #&gt; 5 28 12 4 2.2 0.00 0.65 70 20.01 12 10 fai 1.61 #&gt; 6 30 6 4 24.8 1.84 0.34 55 5.97 12 10 pa 6.87 ライブラリcorrplotの関数corrplot.mixed()を使うと, 相関係数のヒートマップと相関係数の値を同時に表示するプロットを作成することができる. library(corrplot) cor_hw &lt;- cor(cbind(Highway$rate, Highway[, -(11:12)])) # htypeを除去 corrplot.mixed(cor_hw) # OLS回帰 res_lm &lt;- lm(rate ~ ., data = Highway) summary(res_lm) #&gt; #&gt; Call: #&gt; lm(formula = rate ~ ., data = Highway) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -1.99564 -0.62039 -0.05676 0.61741 2.54998 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 13.658212 6.872719 1.987 0.0579 . #&gt; adt -0.004038 0.033945 -0.119 0.9063 #&gt; trks -0.100150 0.114726 -0.873 0.3910 #&gt; lane 0.026675 0.283834 0.094 0.9259 #&gt; acpt 0.066588 0.042569 1.564 0.1303 #&gt; sigs 0.713644 0.525213 1.359 0.1864 #&gt; itg -0.475478 1.282742 -0.371 0.7140 #&gt; slim -0.123778 0.081683 -1.515 0.1422 #&gt; len -0.064751 0.033369 -1.940 0.0637 . #&gt; lwid -0.133813 0.597917 -0.224 0.8247 #&gt; shld 0.014113 0.162174 0.087 0.9313 #&gt; htypefai 0.543592 1.728270 0.315 0.7557 #&gt; htypepa -1.009777 1.105612 -0.913 0.3698 #&gt; htypema -0.548025 0.975623 -0.562 0.5793 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.198 on 25 degrees of freedom #&gt; Multiple R-squared: 0.7605, Adjusted R-squared: 0.636 #&gt; F-statistic: 6.107 on 13 and 25 DF, p-value: 5.733e-05 # 多重共線性のチェック (VIF) # install.packages(&quot;car&quot;) # or RStudio, Tools → Install packages, library(car) # &quot;Companion to Applied Regression&quot; package vif(res_lm) #&gt; GVIF Df GVIF^(1/(2*Df)) #&gt; adt 10.563911 1 3.250217 #&gt; trks 1.931277 1 1.389704 #&gt; lane 3.947949 1 1.986945 #&gt; acpt 4.164627 1 2.040742 #&gt; sigs 2.929007 1 1.711434 #&gt; itg 7.362521 1 2.713397 #&gt; slim 6.041264 1 2.457898 #&gt; len 1.706597 1 1.306368 #&gt; lwid 1.966483 1 1.402313 #&gt; shld 6.417952 1 2.533368 #&gt; htype 28.984452 3 1.752646 データセットHighwayは質的変数 (htype) を含んでいることから, VIFを拡張したGVIF (Generalized VIF) を算出している. GVIFを (変数間で比較できるように) 自由度調整した値GVIF\\(^{1/(2D_f)}\\)は, 量的変数の場合オリジナルのVIFの平方根を取ったものに対応している. そこで, この自由度調整済GVIF\\(^{1/(2D_f)}\\)は, \\(\\sqrt{5} \\approx 2.24\\)越えで要注意, \\(\\sqrt{10} \\approx 3.16\\)越えで除去を検討というのが一つの目安となる ただ, そもそもVIFの5, 10が慣用的な閾値に過ぎないことから, 2.24や3.16といった小数の値まで厳密に評価する合理性は乏しい. そこで, (使いやすくかつ覚えやすくするため) 数字を丸めて, 例えば, 「2までなら安全, 2〜5で要注意, 5越えたら除去を検討する」 等がより実践的である. (自主課題) ステップワイズを実行し, モデルの結果を解釈してみよう. 5.4 説明変数に質的変数を含む回帰 (2) 関数lm()の結果を関数anova()に入力することで, 分散分析表を作成する. 関数anova()は各説明変数 (量的変数, 質的変数どちらについても) ごとに変動性を分割して \\(F\\) 値および \\(p\\) 値を計算する詳細な分散分析表 (ANOVA table) を作成する. また, 二つ以上の包含関係 (ネスト) のある回帰モデルの適合結果オブジェクトを同時に引数として与えることで, 追加 (あるいは除去される) 変数群の持つ有意性を一括して調べることが出来る. データセット#5: 収入データ (仮想) - income.csv - 月収 (万円) - キャリア年数 (年) - 能力試験 (点) - 業種 (A/B) dat1 &lt;- read.csv(&quot;income.csv&quot;) # キャリア年数を説明変数とする単回帰 lm1_mod0 &lt;- lm(月収 ~ キャリア年数, dat = dat1) summary(lm1_mod0) #&gt; #&gt; Call: #&gt; lm(formula = 月収 ~ キャリア年数, data = dat1) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -16.8581 -3.4759 -0.7415 4.5299 13.6303 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 19.8952 1.8298 10.87 &lt;2e-16 *** #&gt; キャリア年数 1.9703 0.1362 14.46 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.495 on 98 degrees of freedom #&gt; Multiple R-squared: 0.681, Adjusted R-squared: 0.6778 #&gt; F-statistic: 209.2 on 1 and 98 DF, p-value: &lt; 2.2e-16 anova(lm1_mod0) # ANOVA表 #&gt; Analysis of Variance Table #&gt; #&gt; Response: 月収 #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; キャリア年数 1 6316.7 6316.7 209.23 &lt; 2.2e-16 *** #&gt; Residuals 98 2958.6 30.2 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # 業種 (質的変数) を説明変数に追加 # 交互作用項なしモデル # lm1_mod1 &lt;- lm(月収 ~ キャリア年数 + 業種, dat = dat1) lm1_mod1 &lt;- update(lm1_mod0, . ~ . + 業種) summary(lm1_mod1) #&gt; #&gt; Call: #&gt; lm(formula = 月収 ~ キャリア年数 + 業種, data = dat1) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -14.4436 -3.5628 -0.6921 3.5165 12.6536 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 20.7222 1.7727 11.690 &lt; 2e-16 *** #&gt; キャリア年数 1.9900 0.1306 15.233 &lt; 2e-16 *** #&gt; 業種B -3.5982 1.1499 -3.129 0.00232 ** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.264 on 97 degrees of freedom #&gt; Multiple R-squared: 0.7103, Adjusted R-squared: 0.7043 #&gt; F-statistic: 118.9 on 2 and 97 DF, p-value: &lt; 2.2e-16 anova(lm1_mod1) # ANOVA表 #&gt; Analysis of Variance Table #&gt; #&gt; Response: 月収 #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; キャリア年数 1 6316.7 6316.7 227.9980 &lt; 2.2e-16 *** #&gt; 業種 1 271.2 271.2 9.7906 0.002317 ** #&gt; Residuals 97 2687.4 27.7 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(lm1_mod1)の出力結果から, この回帰分析のベースラインは 業種Aであることが分かる. すなわち, ２つ目の回帰係数業種Bは, 業種Bに属することによる相対効果, すなわち, 業種BのAに対する目的変数月収の平均値の差分を表している. 具体的には, 回帰係数の切片項の値が \\(20.7222\\) が業種Aの切片となっていて, 一方, 業種Bは \\(20.7222-3.5982=17.1240\\) を切片に持つと読むことができる. 業種の違いによるキャリア年数の影響度 (傾き) の違い を調べるためには, 業種とキャリア年数の2つの項を持つモデルに両者の交互作用項を加え, その交互作用項の有意性 (\\(t\\)値に基づく\\(p\\)値) を確認すれば良い. # 交互作用項有りモデル lm1_mod1_2 &lt;- lm(月収 ~ キャリア年数 * 業種, dat = dat1) summary(lm1_mod1_2) #&gt; #&gt; Call: #&gt; lm(formula = 月収 ~ キャリア年数 * 業種, data = dat1) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -13.2175 -3.7691 -0.8021 3.4916 13.2028 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 19.3523 2.0795 9.306 4.6e-15 *** #&gt; キャリア年数 2.0980 0.1563 13.424 &lt; 2e-16 *** #&gt; 業種B 0.9930 3.8464 0.258 0.797 #&gt; キャリア年数:業種B -0.3537 0.2828 -1.250 0.214 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.248 on 96 degrees of freedom #&gt; Multiple R-squared: 0.7149, Adjusted R-squared: 0.706 #&gt; F-statistic: 80.24 on 3 and 96 DF, p-value: &lt; 2.2e-16 anova(lm1_mod1_2) #&gt; Analysis of Variance Table #&gt; #&gt; Response: 月収 #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; キャリア年数 1 6316.7 6316.7 229.3230 &lt; 2.2e-16 *** #&gt; 業種 1 271.2 271.2 9.8475 0.002259 ** #&gt; キャリア年数:業種 1 43.1 43.1 1.5637 0.214160 #&gt; Residuals 96 2644.3 27.5 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 回帰係数の結果表より, 交互作用項は有意な差があるとは言えない, すなわち, 業種の違いによる回帰係数の差は認められなかった (十分な証拠が得られなかった). 代替的に, 業種とキャリア年数の交互作用ありモデルとなしモデルについてlm()をそれぞれ走らせ, 二つの結果をanova()に同時に与えることで両者の変動性に有意な差があるかを調べても良い. anova(lm1_mod1, lm1_mod1_2) # ANOVA表 #&gt; Analysis of Variance Table #&gt; #&gt; Model 1: 月収 ~ キャリア年数 + 業種 #&gt; Model 2: 月収 ~ キャリア年数 * 業種 #&gt; Res.Df RSS Df Sum of Sq F Pr(&gt;F) #&gt; 1 97 2687.4 #&gt; 2 96 2644.3 1 43.073 1.5637 0.2142 先述の交互作用項の\\(t\\)値に基づいた\\(p\\)値と等価な結果が得られた. lm1_mod2 &lt;- update(lm1_mod1, . ~ . + 能力試験) summary(lm1_mod2) #&gt; #&gt; Call: #&gt; lm(formula = 月収 ~ キャリア年数 + 業種 + 能力試験, #&gt; data = dat1) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -14.8763 -3.9926 -0.6659 3.7814 12.0713 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 23.92898 3.52028 6.797 9e-10 *** #&gt; キャリア年数 1.98691 0.13060 15.214 &lt; 2e-16 *** #&gt; 業種B -3.77837 1.16192 -3.252 0.00158 ** #&gt; 能力試験 -0.06343 0.06017 -1.054 0.29444 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.261 on 96 degrees of freedom #&gt; Multiple R-squared: 0.7136, Adjusted R-squared: 0.7046 #&gt; F-statistic: 79.72 on 3 and 96 DF, p-value: &lt; 2.2e-16 anova(lm1_mod2) # ANOVA表 #&gt; Analysis of Variance Table #&gt; #&gt; Response: 月収 #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; キャリア年数 1 6316.7 6316.7 228.2596 &lt; 2e-16 *** #&gt; 業種 1 271.2 271.2 9.8018 0.00231 ** #&gt; 能力試験 1 30.8 30.8 1.1113 0.29444 #&gt; Residuals 96 2656.6 27.7 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 次に, 関数anova()により, ネスト関係にある二つのモデルlm1_mod0, lm1_mod2を比較する. # モデル比較 # anova(lm1_mod0, lm1_mod1, lm1_mod2) anova(lm1_mod0, lm1_mod2) #&gt; Analysis of Variance Table #&gt; #&gt; Model 1: 月収 ~ キャリア年数 #&gt; Model 2: 月収 ~ キャリア年数 + 業種 + 能力試験 #&gt; Res.Df RSS Df Sum of Sq F Pr(&gt;F) #&gt; 1 98 2958.6 #&gt; 2 96 2656.6 2 302 5.4566 0.005695 ** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 二つの変数, 業種, 能力試験は一括して, 偶然 (サンプリング・エラー) とはみなせないような体系的な (有意な) 変動を持つことを示している. すなわち, 業種と能力試験は説明変数に加えておいた方が良いと判断される. さらに, モデル選択規準AICによっても, これらを説明変数に持つlm1_mod2の方が望ましいことを示している. AIC(lm1_mod0, lm1_mod2) #&gt; df AIC #&gt; lm1_mod0 3 628.5190 #&gt; lm1_mod2 5 621.7522 “コントラスト”の設定変更* 関数options()のパラメータの一つであるコントラスト (contrasts) を変えることで, パラメータの持つ意味が, すなわち, 解釈が変わる. Rのデフォルトは, 処置対比 (“contr.treatment”). #options(&quot;contrasts&quot;) #options(&quot;contrasts&quot; = c(&quot;contr.treatment&quot;, &quot;contr.poly&quot;)) # デフォルト #options(&quot;contrasts&quot; = c(&quot;contr.sum&quot;, &quot;contr.poly&quot;)) 零和対比 (“contr.sum) に変更した場合について結果を, 上と比較してみよう. options(&quot;contrasts&quot; = c(&quot;contr.sum&quot;, &quot;contr.poly&quot;)) lm1_mod2_2 &lt;- update(lm1_mod1, . ~ . + 能力試験) summary(lm1_mod2_2) #&gt; #&gt; Call: #&gt; lm(formula = 月収 ~ キャリア年数 + 業種 + 能力試験, #&gt; data = dat1) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -14.8763 -3.9926 -0.6659 3.7814 12.0713 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 22.03980 3.45054 6.387 6.02e-09 *** #&gt; キャリア年数 1.98691 0.13060 15.214 &lt; 2e-16 *** #&gt; 業種1 1.88919 0.58096 3.252 0.00158 ** #&gt; 能力試験 -0.06343 0.06017 -1.054 0.29444 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.261 on 96 degrees of freedom #&gt; Multiple R-squared: 0.7136, Adjusted R-squared: 0.7046 #&gt; F-statistic: 79.72 on 3 and 96 DF, p-value: &lt; 2.2e-16 ここでは, 回帰係数業種1が業種Aに対応, 一方, 業種2 (業種B) は省略されている. 零和条件から, 業種2の係数は\\(-1.88919\\)であることが分かる. すなわち, 切片項の値が \\(22.03980\\) であることから, 業種Aの切片の値は, \\(22.03980+1.88919=23.92899\\), 一方, Bは, \\(23.92899-1.88919=20.15061\\) となると読める. すなわち, 先のデフォルトの処置対比の場合の切片の値と一致していることが確認される. "],["ロジットブロビット回帰分析.html", "6 ロジット/ブロビット回帰分析 6.1 ロジット回帰分析の基本操作 6.2 データ分析例 6.3 疑似R2の計算", " 6 ロジット/ブロビット回帰分析 6.1 ロジット回帰分析の基本操作 ロジットモデル: シミュレーションデータ 基本操作を確認するため, ロジット回帰モデルが想定するデータ生成メカニズムに従って, 人工データを生成する. その後, このデータを使って, ロジット回帰分析を行う関数であるglm()を呼び出して実行する. シミュレーションデータの生成 set.seed(1) n &lt;- 100 p &lt;- 2 a &lt;- 1.2; b &lt;- c(0.5, 1.5) X &lt;- matrix(runif(n * p, -5, 5), ncol = p) # 予測変数 (X1, X2) colnames(X) &lt;- paste0(&quot;X&quot;, 1:p) eta &lt;- a + X %*% b # 線形予測子 pi &lt;- exp(eta)/(1 + exp(eta)) # ロジスティック変換 y &lt;- rbinom(n, 1, pi) # 発生頻度 (ランダム) plot(X[, 1], y) plot(X[, 2], y) ロジット回帰分析の実行 ロジット回帰分析は, R関数glm()を用いて実行することができる. glm()は標準パッケージstatsに含まれているため, あらたなインストール作業は不要である. glm()を用いた分析の操作手順や出力結果の読み方は, おおむねlm()を踏襲すれば良い. glm()に与えるモデル式 (引数formula) のシンタックスは lm()と同様である. それに加えて, glm()では, 引数familyによってモデルの (分析データを生成していると想定する) 確率分布を指定する. ロジット回帰/プロビット回帰では, binomialを指定する. また, lm()と同様に, glm()の出力結果に対して summary()関数を適用することで, 推定結果が出力される. # ロジット回帰 res_glm &lt;- glm(y ~ X, family = binomial) summary(res_glm) #&gt; #&gt; Call: #&gt; glm(formula = y ~ X, family = binomial) #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 2.5815 0.8647 2.985 0.002831 ** #&gt; XX1 1.1494 0.3434 3.347 0.000818 *** #&gt; XX2 2.7630 0.7509 3.680 0.000233 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 133.750 on 99 degrees of freedom #&gt; Residual deviance: 34.478 on 97 degrees of freedom #&gt; AIC: 40.478 #&gt; #&gt; Number of Fisher Scoring iterations: 8 summary()の出力結果の主な相違は, 個別の回帰係数の有意性を見る指標が, lm()ではt値であったところが glm()ではz値になっている点 (それに伴い, 付随するp値の表記も変化), および, 全体の適合度の指標について, lm()のように\\(R^2\\), 補正\\(R^2\\)の値が表示されておらず, その代わりとして, “Null Deviance”, “Residual Deviance”が表示されている点である. (内挿) 予測 glm()の出力結果を関数predict()に与えることで, 内挿予測 (適合値の算出) を行うことができる. 注意点としては, 引数typeを指定しない場合 (デフォルト設定) には, 未知パラメータの線形結合である “線形予測子”\\(\\eta=\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_k x_k\\)の予測値が出力される. 関数predict(): - 予測値を返す関数 - 引数typeの選択肢: &quot;link&quot;(デフォルト), &quot;response&quot;, &quot;terms&quot; - &quot;link&quot;: 線形予測子のスケールにおける予測 - &quot;response&quot;: 反応変数のスケールにおける予測 - &quot;terms&quot;: 線形予測子のスケールにおける予測, モデル式の各項の適合値を格納した行列を返す ロジット回帰において, 確率の予測を得たい場合には, type=\"response\"と指定する必要がある. # 予測 (内挿) eta_hat &lt;- predict(res_glm) # 対数オッズ p_hat &lt;- predict(res_glm, type = &quot;response&quot;) # 確率 head(data.frame(y, p_hat)) #&gt; y p_hat #&gt; 1 1 0.9846543 #&gt; 2 0 0.0500027 #&gt; 3 0 0.0507452 #&gt; 4 1 1.0000000 #&gt; 5 1 0.9448650 #&gt; 6 0 0.3178520 plot(eta_hat, p_hat) 以下のように, 線形予測子の予測値に対して, ロジスティク変換 (リンク関数\\(g(\\cdot)\\)の逆関数) を適用すると, 確率の予測値p_hatと一致することが確認される. # 確認 # logistic_f &lt;- function(eta){ # exp(eta) / (1 + exp(eta)) # } # logistic_f(eta_hat) # 線形予測子のロジスティク変換 exp(eta_hat) / (1 + exp(eta_hat)) # 線形予測子のロジスティック変換 #&gt; 1 2 3 4 5 6 #&gt; 9.846543e-01 5.000270e-02 5.074520e-02 1.000000e+00 9.448650e-01 3.178520e-01 #&gt; 7 8 9 10 11 12 #&gt; 7.255619e-02 9.786489e-01 9.999999e-01 5.680028e-01 9.999957e-01 9.948751e-01 #&gt; 13 14 15 16 17 18 #&gt; 6.840476e-01 3.443645e-01 1.734430e-02 1.848314e-05 9.999839e-01 6.129494e-02 #&gt; 19 20 21 22 23 24 #&gt; 4.300994e-01 9.999350e-01 1.000000e+00 2.997055e-01 9.800110e-01 2.155297e-05 #&gt; 25 26 27 28 29 30 #&gt; 9.990392e-01 4.996798e-01 6.281495e-02 1.057488e-03 3.392529e-01 9.674261e-01 #&gt; 31 32 33 34 35 36 #&gt; 9.883911e-01 3.490784e-04 3.277648e-05 9.488525e-01 1.000000e+00 9.992746e-01 #&gt; 37 38 39 40 41 42 #&gt; 9.995223e-01 2.305139e-01 1.000000e+00 8.548202e-01 9.999879e-01 9.991567e-01 #&gt; 43 44 45 46 47 48 #&gt; 2.007423e-01 2.957535e-02 9.999048e-01 9.900165e-01 6.973787e-06 9.998923e-01 #&gt; 49 50 51 52 53 54 #&gt; 3.461442e-03 9.999997e-01 9.958972e-01 9.997546e-01 5.412342e-02 1.615667e-01 #&gt; 55 56 57 58 59 60 #&gt; 8.780960e-02 1.960509e-05 7.839028e-01 1.310930e-04 1.548464e-01 1.613948e-03 #&gt; 61 62 63 64 65 66 #&gt; 7.990981e-01 9.999853e-01 6.514770e-01 9.997732e-01 9.999996e-01 6.913302e-02 #&gt; 67 68 69 70 71 72 #&gt; 6.024214e-05 7.496756e-01 9.817408e-01 9.174800e-01 9.870712e-01 9.999999e-01 #&gt; 73 74 75 76 77 78 #&gt; 9.999765e-01 8.861010e-02 2.704708e-01 1.000000e+00 9.999787e-01 9.996573e-01 #&gt; 79 80 81 82 83 84 #&gt; 9.998298e-01 1.000000e+00 2.045796e-02 2.912945e-02 9.999945e-01 6.607180e-01 #&gt; 85 86 87 88 89 90 #&gt; 9.999999e-01 8.082135e-05 9.999947e-01 9.882999e-01 9.999933e-01 4.497550e-01 #&gt; 91 92 93 94 95 96 #&gt; 9.956770e-01 3.843901e-03 1.100383e-03 1.000000e+00 4.496549e-01 9.997968e-01 #&gt; 97 98 99 100 #&gt; 1.668414e-04 9.999826e-01 7.548861e-01 9.999909e-01 # → p_hatと同じ結果となることを確認 係数の信頼区間 同様に, 回帰係数の信頼区間は関数confint()を使えば良い. # 信頼区間 confint(res_glm) # 95%信頼区間 (デフォルト) #&gt; 2.5 % 97.5 % #&gt; (Intercept) 1.1914472 4.659417 #&gt; XX1 0.5901329 1.974012 #&gt; XX2 1.5989191 4.620681 confint(res_glm, level = 0.90) # 90%信頼区間 #&gt; 5 % 95 % #&gt; (Intercept) 1.3787136 4.269921 #&gt; XX1 0.6664132 1.818782 #&gt; XX2 1.7488603 4.265779 6.2 データ分析例 データセット (1): 企業パフォーマンス・データ (仮想) - firmperf.txt, 8件 - 企業規模 (size): H/L - 人材投資 (hr_invest): H/L - SDGs活動 (sdg): H/L - 対象企業数 (n_tot): 社 - 優良社数 (n_pos): 社 データ読み込み perf_dat1 &lt;- read.csv(&quot;firmperf.txt&quot;, skip = 2) # 注) デフォルトはstringsAsFactors = F (文字列を因子型変数に変換せずに読み込む) # size:sdgは, 関数read.csv()でそのまま読み込むと文字型変数となる. # 読み込み時に因子型にするには, stringsAsFactors = T # 執筆現在 (2024年5月10日)のglm()の仕様では, 文字型のままでもOK colnames(perf_dat1) &lt;- c(&quot;size&quot;, &quot;hr_invest&quot;, &quot;sdg&quot;, &quot;n_tot&quot;, &quot;n_pos&quot;) attach(perf_dat1) ロジット回帰実行 ロジット回帰の実行の際, 目的変数として glm()へは異なるデータ形式を与えることもできる. まず, 目的変数のデータ形式として, “成功回数”,“失敗回数” の2列を持つ行列をglm内でモデル式formulaの左辺に与える例である. # データ形式-1 # &quot;成功回数&quot;、&quot;失敗回数&quot;の2列 perf_tbl &lt;- cbind(n_pos, n_tot - n_pos) res_glm1 &lt;- glm(perf_tbl ~ size + hr_invest + sdg, family = binomial) summary(res_glm1) #&gt; #&gt; Call: #&gt; glm(formula = perf_tbl ~ size + hr_invest + sdg, family = binomial) #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -0.63288 0.25177 -2.514 0.01195 * #&gt; sizeL -0.07844 0.26926 -0.291 0.77082 #&gt; hr_investL -0.82333 0.27607 -2.982 0.00286 ** #&gt; sdgL -0.61585 0.35132 -1.753 0.07961 . #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 15.2547 on 7 degrees of freedom #&gt; Residual deviance: 2.3921 on 4 degrees of freedom #&gt; AIC: 36.198 #&gt; #&gt; Number of Fisher Scoring iterations: 4 線形回帰分析の時と同様, 二つのモデルの間に包含関係 (ネスト, “nested”) がある場合には, 分散分析を行う関数anova()を適用することで, 追加された項がまとめて有意かどうかを評価することができる. 下の例は, 切片しか持たないNullモデル (perf_tbl ~ 1) による モデルの当てはまりの悪さ, “Residual deviance” (残差逸脱度) を, hr_investとsdgの2変数を 持つモデルの”Residual deviance”と比較し, 後者の残差逸脱度がこれら2変数を加えることでどれだけ 改善したのかをみることことで, 有意性を評価している. res_glm2 &lt;- glm(perf_tbl ~ hr_invest + sdg, binomial) summary(res_glm2) #&gt; #&gt; Call: #&gt; glm(formula = perf_tbl ~ hr_invest + sdg, family = binomial) #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -0.6570 0.2380 -2.760 0.00578 ** #&gt; hr_investL -0.8235 0.2760 -2.983 0.00285 ** #&gt; sdgL -0.6100 0.3507 -1.739 0.08200 . #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 15.2547 on 7 degrees of freedom #&gt; Residual deviance: 2.4775 on 5 degrees of freedom #&gt; AIC: 34.284 #&gt; #&gt; Number of Fisher Scoring iterations: 4 res_glm0 &lt;- glm(perf_tbl ~ 1, binomial) # 切片項のみ (null model) summary(res_glm0) #&gt; #&gt; Call: #&gt; glm(formula = perf_tbl ~ 1, family = binomial) #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -1.3950 0.1205 -11.58 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 15.255 on 7 degrees of freedom #&gt; Residual deviance: 15.255 on 7 degrees of freedom #&gt; AIC: 43.061 #&gt; #&gt; Number of Fisher Scoring iterations: 4 # anova(res_glm2, test = &quot;Chisq&quot;) # カイ2乗検定 (test = &quot;LRT&quot;でも可) anova(res_glm0, res_glm2, test = &quot;Chisq&quot;) # 同 #&gt; Analysis of Deviance Table #&gt; #&gt; Model 1: perf_tbl ~ 1 #&gt; Model 2: perf_tbl ~ hr_invest + sdg #&gt; Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) #&gt; 1 7 15.2547 #&gt; 2 5 2.4775 2 12.777 0.001681 ** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 今回のケースでは, 2変数を加える (自由度2を使う) ことで, 残差逸脱度が15.2547から2.4775へと12.777減少すること, これにより, 帰無仮説 (二つの変数の回帰係数が同時にゼロ) の下で従う自由度2の\\(\\chi^2\\)分布に基づいて計算される\\(p\\)値が0.001681であること, したがって, 有意水準1%で有意である (帰無仮説が棄却される) ことを示している. 目的変数のデータ形式として, 上とは別の形式として, “成功率”を”試行回数”と共にglmに与える例を次に示す. # データ形式-2 # &quot;成功率&quot;の指定 prop_perf &lt;- n_pos / n_tot res_glm1_2 &lt;- glm(prop_perf ~ size + hr_invest + sdg, binomial, weights = n_tot) summary(res_glm1_2) #&gt; #&gt; Call: #&gt; glm(formula = prop_perf ~ size + hr_invest + sdg, family = binomial, #&gt; weights = n_tot) #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -0.63288 0.25177 -2.514 0.01195 * #&gt; sizeL -0.07844 0.26926 -0.291 0.77082 #&gt; hr_investL -0.82333 0.27607 -2.982 0.00286 ** #&gt; sdgL -0.61585 0.35132 -1.753 0.07961 . #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 15.2547 on 7 degrees of freedom #&gt; Residual deviance: 2.3921 on 4 degrees of freedom #&gt; AIC: 36.198 #&gt; #&gt; Number of Fisher Scoring iterations: 4 プロビット回帰については, glm()の引数family=binomial (link = \"probit\")と指定すれば, あとは ロジット回帰と同様に実行できる. # プロビット回帰 res_glm1_p &lt;- glm(perf_tbl ~ size + hr_invest + sdg, family = binomial(link = &quot;probit&quot;)) # probit summary(res_glm1_p) #&gt; #&gt; Call: #&gt; glm(formula = perf_tbl ~ size + hr_invest + sdg, family = binomial(link = &quot;probit&quot;)) #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -0.39738 0.15235 -2.608 0.0091 ** #&gt; sizeL -0.04947 0.15331 -0.323 0.7470 #&gt; hr_investL -0.48155 0.16439 -2.929 0.0034 ** #&gt; sdgL -0.33830 0.18901 -1.790 0.0735 . #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 15.2547 on 7 degrees of freedom #&gt; Residual deviance: 2.4259 on 4 degrees of freedom #&gt; AIC: 36.232 #&gt; #&gt; Number of Fisher Scoring iterations: 3 (内挿) 予測 関数predict()を用いて予測値を出力するが, 上述のように引数typeを指定しない場合には, 線形予測子 (のスケールでの) 予測を行う. 具体的には, ロジット回帰では対数オッズを, プロビット回帰では標準正規分布の分位点 (分位点関数は累積分布関数の逆関数) を返す. 一方, type=\"response\"とした場合には, どちらのモデルにおいても, 反応変数のスケール, すなわち, 確率の予測値を返す. # 予測 (ロジット回帰) predict(res_glm1_2) # 対数オッズ (∵ロジット回帰) #&gt; 1 2 3 4 5 6 7 #&gt; -2.0720496 -1.2487226 -1.4562023 -1.3271595 -1.5346392 -0.6328753 -0.7113122 #&gt; 8 #&gt; -2.1504865 p_hat &lt;- predict(res_glm1_2, type = &quot;response&quot;) # 確率 predict(res_glm1_2, type = &quot;terms&quot;) # 線形予測子の項ごとの適合値 #&gt; size hr_invest sdg #&gt; 1 0.03921846 -0.4116635 -0.3079237 #&gt; 2 0.03921846 0.4116635 -0.3079237 #&gt; 3 0.03921846 -0.4116635 0.3079237 #&gt; 4 -0.03921846 0.4116635 -0.3079237 #&gt; 5 -0.03921846 -0.4116635 0.3079237 #&gt; 6 0.03921846 0.4116635 0.3079237 #&gt; 7 -0.03921846 0.4116635 0.3079237 #&gt; 8 -0.03921846 -0.4116635 -0.3079237 #&gt; attr(,&quot;constant&quot;) #&gt; [1] -1.391681 # 同 (プロビット回帰) predict(res_glm1_p) # 標準正規分布の分位点 (∵プロビット回帰) #&gt; 1 2 3 4 5 6 7 #&gt; -1.2172398 -0.7356881 -0.8789356 -0.7851545 -0.9284021 -0.3973839 -0.4468504 #&gt; 8 #&gt; -1.2667063 p_hat_p &lt;- predict(res_glm1_p, type = &quot;response&quot;) # 確率 # 予測値の比較: ロジット vs プロビット data.frame(perf_dat1, logit = p_hat, probit = p_hat_p) #&gt; size hr_invest sdg n_tot n_pos logit probit #&gt; 1 H L L 60 7 0.1118433 0.1117565 #&gt; 2 H H L 8 2 0.2229213 0.2309603 #&gt; 3 H L H 186 36 0.1890489 0.1897181 #&gt; 4 L H L 3 0 0.2096296 0.2161815 #&gt; 5 L L H 86 14 0.1773159 0.1765995 #&gt; 6 H H H 50 16 0.3468589 0.3455422 #&gt; 7 L H H 22 9 0.3293090 0.3274915 #&gt; 8 L L L 18 2 0.1042858 0.1026302 # 参考) 説明変数が因子型(factor)でない場合, 以前はエラー発生 # → 文字型変数は因子型への変換が必要だった plot(factor(hr_invest), predict(res_glm2, type = &quot;response&quot;)) plot(factor(sdg), predict(res_glm2, type = &quot;response&quot;)) # 説明変数が数値型変数ならば、logistic曲線を描く 係数の信頼区間 # 信頼区間 confint(res_glm1_2) # 95%信頼区間 (デフォルト) #&gt; 2.5 % 97.5 % #&gt; (Intercept) -1.1392044 -0.14805877 #&gt; sizeL -0.6194614 0.43967083 #&gt; hr_investL -1.3592877 -0.27372310 #&gt; sdgL -1.3520115 0.03750972 confint(res_glm1_2, level = 0.90) # 90%信頼区間 #&gt; 5 % 95 % #&gt; (Intercept) -1.0558140 -0.22506829 #&gt; sizeL -0.5304547 0.35748113 #&gt; hr_investL -1.2736384 -0.36344375 #&gt; sdgL -1.2264225 -0.06339516 detach() (自主課題) “firmperf.txt”に対するロジット回帰分析の結果を解釈してみよう. データセット (2): 個人ローン・デフォルト・データ (仮想) - default.csv, 100件 - デフォルト (1/0) - ローン残高 (万円) - 収入 (万円) - 職種 (A/B) データ読み込み default &lt;- read.csv(&quot;default.csv&quot;) attach(default) ロジット回帰 # ロジット回帰 res_glm &lt;- glm(デフォルト ~ ローン残高 + 収入 + 職種, family = binomial) summary(res_glm) #&gt; #&gt; Call: #&gt; glm(formula = デフォルト ~ ローン残高 + 収入 + 職種, #&gt; family = binomial) #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -1.336e+01 3.505e+00 -3.811 0.000138 *** #&gt; ローン残高 4.708e-03 1.224e-03 3.846 0.000120 *** #&gt; 収入 -9.227e-04 2.345e-03 -0.394 0.693925 #&gt; 職種B 9.713e-01 1.445e+00 0.672 0.501434 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 102.791 on 99 degrees of freedom #&gt; Residual deviance: 46.971 on 96 degrees of freedom #&gt; AIC: 54.971 #&gt; #&gt; Number of Fisher Scoring iterations: 8 plot(ローン残高, predict(res_glm, type = &quot;response&quot;)) plot(収入, predict(res_glm, type = &quot;response&quot;)) # plot(職種, predict(res_glm, type = &quot;response&quot;)) プロビット回帰 # プロビット回帰 res_glm_p &lt;- glm(デフォルト ~ ローン残高 + 収入 + 職種, family = binomial(link = &quot;probit&quot;)) summary(res_glm_p) #&gt; #&gt; Call: #&gt; glm(formula = デフォルト ~ ローン残高 + 収入 + 職種, #&gt; family = binomial(link = &quot;probit&quot;)) #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -7.0233825 1.6903093 -4.155 3.25e-05 *** #&gt; ローン残高 0.0024920 0.0005893 4.229 2.35e-05 *** #&gt; 収入 -0.0006478 0.0013082 -0.495 0.620 #&gt; 職種B 0.6549554 0.8035424 0.815 0.415 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 102.79 on 99 degrees of freedom #&gt; Residual deviance: 47.81 on 96 degrees of freedom #&gt; AIC: 55.81 #&gt; #&gt; Number of Fisher Scoring iterations: 8 ## 結果の比較: glm vs lm # res_glm_normal &lt;- glm(デフォルト ~ ローン残高 + 収入 + 職種, family = gaussian) # summary(res_glm_normal) # res_lm &lt;- lm(デフォルト ~ ローン残高 + 収入 + 職種) # anova(res_lm) (外挿) 予測 外挿予測を行いたい場合には, 関数predict()に対して, glm()の出力と共に, 予測を行いたい予測変数の値の組をデータフレームとして, 引数newdataに与えれば良い. # 予測 (新しいデータセットに対して) newdat &lt;- data.frame(ローン残高 = c(100, 500, 1000, 10000), 収入 = 30000, 職種 = &quot;A&quot;) predict(res_glm, newdata = newdat, type = &quot;response&quot;) #&gt; 1 2 3 4 #&gt; 2.220446e-16 2.220446e-16 2.220446e-16 9.976404e-01 係数の信頼区間 # 信頼区間 confint(res_glm) # 95%信頼区間 (デフォルト) #&gt; 2.5 % 97.5 % #&gt; (Intercept) -21.713657994 -7.721969240 #&gt; ローン残高 0.002743950 0.007642011 #&gt; 収入 -0.005498298 0.003877354 #&gt; 職種B -1.917922786 3.862413416 detach() 便利なツール: パッケージepiDisplay # (参考) 便利なツール library(epiDisplay) logistic.display(res_glm, simplified = TRUE) #&gt; #&gt; OR lower95ci upper95ci Pr(&gt;|Z|) #&gt; ローン残高 1.0047194 1.0023118 1.007133 0.0001199072 #&gt; 収入 0.9990777 0.9944971 1.003679 0.6939246907 #&gt; 職種B 2.6413382 0.1555813 44.842586 0.5014338592 (自主課題) “default.csv”に対するロジット回帰分析の結果を解釈してみよう. 6.3 疑似R2の計算 線形回帰の場合と異なり, ロジット回帰では, \\(R^2\\)は計算されない. しかし, ロジット回帰 (より一般に, 一般化線形モデル) においては, 疑似的に\\(R^2\\)を計算する方法が 複数提案されている. # ロジット回帰 res_glm &lt;- glm(デフォルト ~ ローン残高 + 収入 + 職種, family = binomial, data = default) # summary(res_glm) 以下では, 3つのパッケージDescTools, pscl, performanceを使用した例を紹介する. パッケージDescToolsの利用 DescTools::PseudoR2() - usage: PseudoR2(x, which = NULL) - which: 計算したい疑似R2. 選択肢: &quot;McFadden&quot;(デフォルト), &quot;McFaddenAdj&quot;, &quot;CoxSnell&quot;, &quot;Nagelkerke&quot;, &quot;AldrichNelson&quot;, &quot;VeallZimmermann&quot;, &quot;Efron&quot;, &quot;McKelveyZavoina&quot;, &quot;Tjur&quot;, &quot;all&quot;. # 疑似R2の計算 library(DescTools) PseudoR2(res_glm) # McFadden (デフォルト) #&gt; McFadden #&gt; 0.5430468 PseudoR2(res_glm, which = &quot;CoxSnell&quot;) # Cox-Snell #&gt; CoxSnell #&gt; 0.4277647 PseudoR2(res_glm, which = &quot;Nagelkerke&quot;) # Nagelkerke #&gt; Nagelkerke #&gt; 0.6660436 PseudoR2(res_glm, which = &quot;all&quot;) #&gt; McFadden McFaddenAdj CoxSnell Nagelkerke AldrichNelson #&gt; 0.5430468 0.4652192 0.4277647 0.6660436 0.3582359 #&gt; VeallZimmermann Efron McKelveyZavoina Tjur AIC #&gt; 0.7067438 0.5736644 0.8515845 0.5589555 54.9708332 #&gt; BIC logLik logLik0 G2 #&gt; 65.3915139 -23.4854166 -51.3956671 55.8205010 疑似\\(R^2\\)は, 方法により相当異なることが確認される. パッケージpsclの利用 pscl::pR2() - 以下を出力: - llh: The log-likelihood from the fitted model - llhNull: The log-likelihood from the intercept-only restricted model - G2: Minus two times the difference in the log-likelihoods - McFadden: McFadden&#39;s pseudo r-squared - r2ML: Maximum likelihood pseudo r-squared - r2CU: Cragg and Uhler&#39;s pseudo r-squared # install.packages(&quot;pscl&quot;) library(pscl) # 疑似R2の計算 pscl::pR2(res_glm) #&gt; fitting null model for pseudo-r2 #&gt; llh llhNull G2 McFadden r2ML r2CU #&gt; -23.4854166 -51.3956671 55.8205010 0.5430468 0.4277647 0.6660436 パッケージperformanceの利用 performance::r2() - モデルに応じて適切な疑似R2を選んで出力: - Logistic models: Tjur&#39;s R2 - General linear models: Nagelkerke&#39;s R2 - Multinomial Logit: McFadden&#39;s R2 - Models with zero-inflation: R2 for zero-inflated models - Mixed models: Nakagawa&#39;s R2 - Bayesian models: R2 bayes # install.packages(&quot;performance&quot;) library(performance) # 疑似R2の計算 performance::r2(res_glm) #&gt; # R2 for Logistic Regression #&gt; Tjur&#39;s R2: 0.559 "],["判別分析.html", "7 判別分析 7.1 線形判別 &amp; 2次判別 - パッケージMASSの利用 7.2 kNN法 - パッケージclassの利用 7.3 データセット分割・学習・判別・評価 7.4 他パッケージの利用", " 7 判別分析 7.1 線形判別 &amp; 2次判別 - パッケージMASSの利用 手初めに, データセットを学習データ (訓練データ) とテストデータに分けずに, 標準パッケージMASS内の線形判別, 2次判別を実行する関数 lda, qdaを適用した結果 (内挿予測) を眺めてみる. データセット1: ビジネススクールの入学許可データ - admission.csv - GPA - GMAT - De (意思決定): &quot;admit&quot;, &quot;border&quot;, &quot;notadmit&quot; - source: https://rstudio-pubs-static.s3.amazonaws.com/35817_2552e05f1d4e4db8ba87b334101a43da.html #url &lt;- &#39;http://www.biz.uiowa.edu/faculty/jledolter/DataMining/admission.csv&#39; #admit &lt;- read.csv(url) admdat &lt;- read.csv(&quot;admission.csv&quot;, skip = 2, stringsAsFactors = T) # 注: stringsAsFactors = F(デフォルト) → エラー head(admdat) #admdat = data.frame(admdat) plot(admdat$GPA, admdat$GMAT, col = admdat$De) #&gt; GPA GMAT De #&gt; 1 2.96 596 admit #&gt; 2 3.14 473 admit #&gt; 3 3.22 482 admit #&gt; 4 3.29 527 admit #&gt; 5 3.69 505 admit #&gt; 6 3.46 693 admit 線形判別 (LDA) 線形判別の実行は, パッケージMASSの関数ldaにより行うことができる. library(MASS) # LDAの実行 adm_lda &lt;- lda(De ~ ., admdat) #adm_lda 線形判別 (lda) や2次判別 (qda) の出力オブジェクト (適合結果) を 関数predictに入力することで, 判別結果 (class), およびその判断に使用した 事後確率 (posterior) が出力される. これら (特に前者) が, 判別分析における主要な出力である. # 判別結果 (予測結果) prd_lda &lt;- predict(adm_lda) head(prd_lda$class) #&gt; [1] admit border border admit admit admit #&gt; Levels: admit border notadmit head(prd_lda$posterior) #&gt; admit border notadmit #&gt; 1 0.6274441 3.677500e-01 4.805886e-03 #&gt; 2 0.1400290 8.578453e-01 2.125682e-03 #&gt; 3 0.4070245 5.925494e-01 4.261541e-04 #&gt; 4 0.9111531 8.883071e-02 1.618774e-05 #&gt; 5 0.9989964 1.003605e-03 6.361722e-10 #&gt; 6 0.9999849 1.506768e-05 6.281005e-11 # → 出力: $class, $posterior, $x 外挿予測 &amp; 精度評価 predict(adm_lda, newdata = data.frame(GPA = 3.21, GMAT = 497)) (tbl_lda &lt;- table(predict(adm_lda)$class, admdat$De)) #&gt; $class #&gt; [1] admit #&gt; Levels: admit border notadmit #&gt; #&gt; $posterior #&gt; admit border notadmit #&gt; 1 0.5180421 0.4816015 0.0003563717 #&gt; #&gt; $x #&gt; LD1 LD2 #&gt; 1 1.252409 0.318194 #&gt; #&gt; #&gt; admit border notadmit #&gt; admit 28 1 0 #&gt; border 3 24 2 #&gt; notadmit 0 1 26 2次判別 (QDA) 線形判別の実行は, パッケージMASSの関数qdaにより行うことができる. # QDAの実行 adm_qda &lt;- qda(De ~ ., admdat) #adm_qda # 判別結果 (予測結果) prd_qda &lt;- predict(adm_qda) head(prd_qda$class) #&gt; [1] admit border admit admit admit admit #&gt; Levels: admit border notadmit head(prd_qda$posterior) #&gt; admit border notadmit #&gt; 1 0.9827310 5.740445e-03 1.152851e-02 #&gt; 2 0.3098756 6.880045e-01 2.119862e-03 #&gt; 3 0.8168026 1.826903e-01 5.070901e-04 #&gt; 4 0.9995494 4.307666e-04 1.985923e-05 #&gt; 5 1.0000000 2.322074e-09 4.254151e-10 #&gt; 6 1.0000000 3.974095e-20 1.887372e-10 # → 出力: $class, $posterior 外挿予測 &amp; 精度評価 predict(adm_qda, newdata = data.frame(GPA = 3.21, GMAT = 497)) (tbl_qda &lt;- table(predict(adm_qda)$class, admdat$De)) #&gt; $class #&gt; [1] admit #&gt; Levels: admit border notadmit #&gt; #&gt; $posterior #&gt; admit border notadmit #&gt; 1 0.9226763 0.0768693 0.0004544468 #&gt; #&gt; #&gt; admit border notadmit #&gt; admit 30 1 0 #&gt; border 1 25 1 #&gt; notadmit 0 0 27 なお, 学習済モデルのパラメータ推定値は ldaやqdaの出力オブジェクトに格納され, これらから 線形判別関数や2次判別関数や, 判別境界の形状を知ることができる (ここでは省略). 7.2 kNN法 - パッケージclassの利用 kNN法を実行する関数knnはパッケージclassに含まれている. 入学許可データに対して, 上と同様, 判別 (内挿予測) を試みる. # kNN法 library(class) pred_knn &lt;- knn(admdat[, 1:2], admdat[, 1:2], cl = admdat[, 3], k = 5) (tbl_knn &lt;- table(pred_knn, admdat[, 3])) # confusionMatrix(knn_tab, mode = &quot;prec_recall&quot;) #&gt; #&gt; pred_knn admit border notadmit #&gt; admit 26 2 1 #&gt; border 4 15 7 #&gt; notadmit 1 9 20 7.3 データセット分割・学習・判別・評価 実際の判別分析においては, まず, データセットを学習データ (訓練データ) とテストデータに分け, 前者でモデルを推定 (学習) し, 後者でパフォーマンスを評価する必要がある. データセット2: 個人ローン・デフォルト・データ (仮想) - default1.csv, 7500件 - ローン残高 (万円) - 収入 (万円) - 職種 (A/B) -デフォルト (1/0) dat &lt;- read.csv(&quot;default1.csv&quot;) nsize &lt;- nrow(dat) データセットをランダムに, 2/3を学習用, 残りをテスト用に分割する. # 学習用データ, テスト用データに分割 id_train &lt;- sample(1:nsize, round(nsize / 3 * 2)) dat_train &lt;- dat[id_train, ] dat_test &lt;- dat[- id_train, ] データセットの分割は通常ランダムに行うが, 学習済モデルの判別パフォーマンスはデータセットの分割の仕方に依存する. そこで, そのような分割への依存性を軽減し, モデルによる外挿予測の精度評価の信頼性を高めるために交互検証 (クロス・バリデーション) と呼ばれる方法を行うことが多い (ここでは省略する). 線形判別 # 線形判別 library(MASS) dat_lda &lt;- lda(デフォルト ~ ローン残高 + 収入, data = dat_train) # dat_lda # dat_lda$scaling # 線形判別関数の係数 # hist(predict(dat_lda)$x) # 判別得点 # dat_lda$means # グループ平均 # dat_lda$prior # 事前確率 学習済モデルのパフォーマンス評価は, テストデータを使って判別させ (外挿予測), 正解と比較することにより行う. # 予測 (テストデータに対する判別), パフォーマンス評価 lda_tab &lt;- table(predict(dat_lda, dat_test)$class, dat_test[, &quot;デフォルト&quot;]) # library(caret) # confusionMatrix(lda_tab, mode = &quot;prec_recall&quot;) kNN法 # kNN法 library(class) pred_knn &lt;- knn(dat_train[, c(&quot;ローン残高&quot;, &quot;収入&quot;)], dat_test[, c(&quot;ローン残高&quot;, &quot;収入&quot;)], cl = dat_train[, &quot;デフォルト&quot;], k = 3) (tbl_knn &lt;- table(pred_knn, dat_test[, &quot;デフォルト&quot;])) # confusionMatrix(knn_tab, mode = &quot;prec_recall&quot;) #&gt; #&gt; pred_knn 0 1 #&gt; 0 2394 76 #&gt; 1 18 12 7.4 他パッケージの利用 パッケージklaR 関数partimat()は, 判別分析やそれ以外の多様なクラス分類手法をサポートしている. - 引数methodで指定: “lda”, “qda”, “rpart”, “naiveBayes”, “rda”, “sknn”, “svmlight” # &quot;klaR&quot;パッケージの利用 #install.packages(&#39;klaR&#39;) library(klaR) partimat(De ~ ., data = admdat, method = &quot;lda&quot;) # 線形判別 partimat(De ~ ., data = admdat, method = &quot;qda&quot;) # 2次判別 partimat(De ~ ., data = admdat, method = &quot;sknn&quot;) # Simple kNN法 (デフォルトk = 3) partimat(De ~ ., data = admdat, method = &quot;naiveBayes&quot;) # ナイーブベイズ法 変数間のスケールを統一 (標準化) した後にkNN法を再実行してみる. adm2 &lt;- data.frame(GPA = scale(admdat$GPA), GMAT = scale(admdat$GMAT), De = admdat$De) partimat(De ~ ., data = adm2, method = &quot;sknn&quot;) # Simple kNN法 (デフォルトk = 3), 変数標準化後 パッケージklaR内には, ナイーブベイズ法を実行する関数NaiveBayes()も用意されている. # ナイーブベイズ法 adm_NB &lt;- NaiveBayes(De ~ ., data = admdat) # usekernel = F(デフォルト) → 正規分布 # predict(adm_NB) # # → 出力: $class, $posterior (tbl_nb &lt;- table(predict(adm_NB)$class, admdat$De)) #&gt; #&gt; admit border notadmit #&gt; admit 28 1 0 #&gt; border 3 23 2 #&gt; notadmit 0 2 26 "],["決定木分析.html", "8 決定木分析 8.1 導入: 回帰木 vs 線形回帰 8.2 回帰木 8.3 分類木", " 8 決定木分析 決定木 (decision tree) は, 学習データに含まれる説明変数空間を, 目的変数との関係に基づいて (目的変数がなるべく均質になるように) 階層的に分割する手法である. この分割は, 目的関数（回帰問題であればMSEなど, 分類問題であればGini不純度等）を最適化するように, 一回につき1変数, 学習データに基づいて逐次行われる. 逐次分割のルールは, 木構造で表現される. ルートノード： 木の最上部. データセット全体を表現 (内部) ノード： データセットの分割条件 (質問や条件)を表現. データセットが分割される リーフノード （末端ノード）: 分類されたデータセットの最終的な出力を表現. これ以上の分割は行われない 「決定木はデータセットの分割ルールを自動生成する」 という説明がなされることがあるが, 不正確であることに注意が必要である. 正確には, 生成されるのは「説明変数空間の分割規則」であり, 分割後の各領域に対応した予測値（分類ラベルや回帰適合値） が付与されているに過ぎない. 決定木分析を行う標準的な関数として, パッケージrpartの関数rpart()がある. 回帰木, 分類木どちらにも適用可能である. 8.1 導入: 回帰木 vs 線形回帰 データセット1: 自動車の制動距離 決定木の特徴を理解するため, 金明哲(2017)『Rによるデータサイエンス』でも紹介されている 例を取り上げる. Rに標準的に含まれているデータセットcarsを使い, ブレーキをかけて停止するまでの移動距離を目的変数, 自動車の速度を説明変数とする場合の, 線形回帰と決定木 (回帰木) の適合の結果を比較する. - cars - speed: 自動車の速度 (mph) - dist: ブレーキをかけて自動車が停止するまでの距離 (feet) - Rの標準データセット #cars head(cars); tail(cars) #&gt; speed dist #&gt; 1 4 2 #&gt; 2 4 10 #&gt; 3 7 4 #&gt; 4 7 22 #&gt; 5 8 16 #&gt; 6 9 10 #&gt; speed dist #&gt; 45 23 54 #&gt; 46 24 70 #&gt; 47 24 92 #&gt; 48 24 93 #&gt; 49 24 120 #&gt; 50 25 85 #cars[&quot;speed&quot;] plot(cars) cor(cars[&quot;speed&quot;], cars[&quot;dist&quot;]) #&gt; dist #&gt; speed 0.8068949 単回帰分析 すでに上で解説した通りの操作を行う. # 単回帰分析 cars_lm &lt;- lm(dist ~ speed, data = cars) summary(cars_lm) #&gt; #&gt; Call: #&gt; lm(formula = dist ~ speed, data = cars) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -29.069 -9.525 -2.272 9.215 43.201 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -17.5791 6.7584 -2.601 0.0123 * #&gt; speed 3.9324 0.4155 9.464 1.49e-12 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 15.38 on 48 degrees of freedom #&gt; Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 #&gt; F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12 # 回帰係数の取り出し cars_lm$coef #&gt; (Intercept) speed #&gt; -17.579095 3.932409 coefficients(cars_lm) #&gt; (Intercept) speed #&gt; -17.579095 3.932409 # 回帰直線の図示 plot(cars) abline(cars_lm) # モデル診断（残差プロット等) # plot(cars_lm) # 4枚表示: 残差vs Y適合値, 残差 vs Q - Qプロット, # 残差平方根 vs Y適合値, 残差 vs 影響力(てこ値とCook距離) 予測 # 学習データに対する適合値 (内挿予測) cars_pred &lt;- predict(cars_lm) # 残差 cars_resd &lt;- residuals(cars_lm) # 予測値 vs 残差 plot(cars_pred, cars_resd); abline(h = 0, lty = 2) data.frame(cars, cars_pred, cars_resd) #&gt; speed dist cars_pred cars_resd #&gt; 1 4 2 -1.849460 3.849460 #&gt; 2 4 10 -1.849460 11.849460 #&gt; 3 7 4 9.947766 -5.947766 #&gt; 4 7 22 9.947766 12.052234 #&gt; 5 8 16 13.880175 2.119825 #&gt; 6 9 10 17.812584 -7.812584 #&gt; 7 10 18 21.744993 -3.744993 #&gt; 8 10 26 21.744993 4.255007 #&gt; 9 10 34 21.744993 12.255007 #&gt; 10 11 17 25.677401 -8.677401 #&gt; 11 11 28 25.677401 2.322599 #&gt; 12 12 14 29.609810 -15.609810 #&gt; 13 12 20 29.609810 -9.609810 #&gt; 14 12 24 29.609810 -5.609810 #&gt; 15 12 28 29.609810 -1.609810 #&gt; 16 13 26 33.542219 -7.542219 #&gt; 17 13 34 33.542219 0.457781 #&gt; 18 13 34 33.542219 0.457781 #&gt; 19 13 46 33.542219 12.457781 #&gt; 20 14 26 37.474628 -11.474628 #&gt; 21 14 36 37.474628 -1.474628 #&gt; 22 14 60 37.474628 22.525372 #&gt; 23 14 80 37.474628 42.525372 #&gt; 24 15 20 41.407036 -21.407036 #&gt; 25 15 26 41.407036 -15.407036 #&gt; 26 15 54 41.407036 12.592964 #&gt; 27 16 32 45.339445 -13.339445 #&gt; 28 16 40 45.339445 -5.339445 #&gt; 29 17 32 49.271854 -17.271854 #&gt; 30 17 40 49.271854 -9.271854 #&gt; 31 17 50 49.271854 0.728146 #&gt; 32 18 42 53.204263 -11.204263 #&gt; 33 18 56 53.204263 2.795737 #&gt; 34 18 76 53.204263 22.795737 #&gt; 35 18 84 53.204263 30.795737 #&gt; 36 19 36 57.136672 -21.136672 #&gt; 37 19 46 57.136672 -11.136672 #&gt; 38 19 68 57.136672 10.863328 #&gt; 39 20 32 61.069080 -29.069080 #&gt; 40 20 48 61.069080 -13.069080 #&gt; 41 20 52 61.069080 -9.069080 #&gt; 42 20 56 61.069080 -5.069080 #&gt; 43 20 64 61.069080 2.930920 #&gt; 44 22 66 68.933898 -2.933898 #&gt; 45 23 54 72.866307 -18.866307 #&gt; 46 24 70 76.798715 -6.798715 #&gt; 47 24 92 76.798715 15.201285 #&gt; 48 24 93 76.798715 16.201285 #&gt; 49 24 120 76.798715 43.201285 #&gt; 50 25 85 80.731124 4.268876 # テストデータ(未学習データ)に対する予測 (外挿予測) testdat &lt;- data.frame(speed = c(5, 6, 21)) head(predict(cars_lm, newdata = testdat)) #&gt; 1 2 3 #&gt; 2.082949 6.015358 65.001489 8.1.1 基本操作: 回帰木 シンタックスはそのままで, lm()の代わりにrpart()と書き換えれば, 回帰木が適合される. library(rpart) cars_rp &lt;- rpart(dist ~ speed, data = cars) summary(cars_rp) # ==&gt; 葉3枚 #&gt; Call: #&gt; rpart(formula = dist ~ speed, data = cars) #&gt; n= 50 #&gt; #&gt; CP nsplit rel error xerror xstd #&gt; 1 0.4676398 0 1.0000000 1.0562244 0.2284800 #&gt; 2 0.1104944 1 0.5323602 0.7950838 0.1832721 #&gt; 3 0.0100000 2 0.4218658 0.5456963 0.1347149 #&gt; #&gt; Variable importance #&gt; speed #&gt; 100 #&gt; #&gt; Node number 1: 50 observations, complexity param=0.4676398 #&gt; mean=42.98, MSE=650.7796 #&gt; left son=2 (31 obs) right son=3 (19 obs) #&gt; Primary splits: #&gt; speed &lt; 17.5 to the left, improve=0.4676398, (0 missing) #&gt; #&gt; Node number 2: 31 observations, complexity param=0.1104944 #&gt; mean=29.32258, MSE=267.9605 #&gt; left son=4 (15 obs) right son=5 (16 obs) #&gt; Primary splits: #&gt; speed &lt; 12.5 to the left, improve=0.4328244, (0 missing) #&gt; #&gt; Node number 3: 19 observations #&gt; mean=65.26316, MSE=474.5097 #&gt; #&gt; Node number 4: 15 observations #&gt; mean=18.2, MSE=78.42667 #&gt; #&gt; Node number 5: 16 observations #&gt; mean=39.75, MSE=220.9375 plot(cars_rp, uniform = T, margin = 0.05) text(cars_rp, all = T, use.n = T) 決定木の主要な出力は, 分割ルールである. 今回は, 量的説明変数が一つ (speed) のみのケースであった. 適合の結果, speedの領域を 【分割1】speedが17.5未満か/以上か 【分割2】(speedが17.5未満の領域に対して) speedが12.5未満か/以上か のように2回分割して, 分割を終了した. 結果的に, 木の深さが2, リーフノード (末端ノード) が3個の小さな木である. 予測 予測値は線形回帰等と同様, 関数predict()にrpart()の結果オブジェクトを与えることで得られる. # 学習用データに対する適合値 (内挿予測) cars_rp_pred &lt;- predict(cars_rp) cars_rp_fitted &lt;- data.frame(cars$speed, cars_rp_pred) plot(cars$speed, cars$dist) lines(cars_rp_fitted, type = &quot;s&quot;) 図は, 得られた (説明変数空間の) 分割ルールでは, 横軸 (説明変数speed) は3つの領域に分割され (12.5, 17.5が境界点), それぞれの領域における予測値は, 一定である (左から順に, 18.2, 39.75, 65.26). この目的変数distに対する説明変数speedの区分一定 (piecewise constant) な形状は, 予測値を具体的に出力することでも確認できる. # 観測値, 線形回帰の適合値, 回帰木の適合値 data.frame(cars, cars_pred, cars_rp_fitted) #&gt; speed dist cars_pred cars.speed cars_rp_pred #&gt; 1 4 2 -1.849460 4 18.20000 #&gt; 2 4 10 -1.849460 4 18.20000 #&gt; 3 7 4 9.947766 7 18.20000 #&gt; 4 7 22 9.947766 7 18.20000 #&gt; 5 8 16 13.880175 8 18.20000 #&gt; 6 9 10 17.812584 9 18.20000 #&gt; 7 10 18 21.744993 10 18.20000 #&gt; 8 10 26 21.744993 10 18.20000 #&gt; 9 10 34 21.744993 10 18.20000 #&gt; 10 11 17 25.677401 11 18.20000 #&gt; 11 11 28 25.677401 11 18.20000 #&gt; 12 12 14 29.609810 12 18.20000 #&gt; 13 12 20 29.609810 12 18.20000 #&gt; 14 12 24 29.609810 12 18.20000 #&gt; 15 12 28 29.609810 12 18.20000 #&gt; 16 13 26 33.542219 13 39.75000 #&gt; 17 13 34 33.542219 13 39.75000 #&gt; 18 13 34 33.542219 13 39.75000 #&gt; 19 13 46 33.542219 13 39.75000 #&gt; 20 14 26 37.474628 14 39.75000 #&gt; 21 14 36 37.474628 14 39.75000 #&gt; 22 14 60 37.474628 14 39.75000 #&gt; 23 14 80 37.474628 14 39.75000 #&gt; 24 15 20 41.407036 15 39.75000 #&gt; 25 15 26 41.407036 15 39.75000 #&gt; 26 15 54 41.407036 15 39.75000 #&gt; 27 16 32 45.339445 16 39.75000 #&gt; 28 16 40 45.339445 16 39.75000 #&gt; 29 17 32 49.271854 17 39.75000 #&gt; 30 17 40 49.271854 17 39.75000 #&gt; 31 17 50 49.271854 17 39.75000 #&gt; 32 18 42 53.204263 18 65.26316 #&gt; 33 18 56 53.204263 18 65.26316 #&gt; 34 18 76 53.204263 18 65.26316 #&gt; 35 18 84 53.204263 18 65.26316 #&gt; 36 19 36 57.136672 19 65.26316 #&gt; 37 19 46 57.136672 19 65.26316 #&gt; 38 19 68 57.136672 19 65.26316 #&gt; 39 20 32 61.069080 20 65.26316 #&gt; 40 20 48 61.069080 20 65.26316 #&gt; 41 20 52 61.069080 20 65.26316 #&gt; 42 20 56 61.069080 20 65.26316 #&gt; 43 20 64 61.069080 20 65.26316 #&gt; 44 22 66 68.933898 22 65.26316 #&gt; 45 23 54 72.866307 23 65.26316 #&gt; 46 24 70 76.798715 24 65.26316 #&gt; 47 24 92 76.798715 24 65.26316 #&gt; 48 24 93 76.798715 24 65.26316 #&gt; 49 24 120 76.798715 24 65.26316 #&gt; 50 25 85 80.731124 25 65.26316 分割ルールを, 未学習のデータセットに対して適用してみる. speedの値が, それぞれ, 5,6,21 (mph) の時の予測値は 以下の通りである. # テストデータに対する予測値 (外挿予測) predict(cars_rp, newdata = testdat) #&gt; 1 2 3 #&gt; 18.20000 18.20000 65.26316 予測値は, 順に18.2, 18.2, 65.26である. スピードが5でも6でも同じ静止距離 (18.2) と予測している. すなわち, 制止までの距離が速度の変化に対して連続的に変化するような今回の事例においては, 区分一定な予測を行う回帰木の使用は明らかに不適切である. 8.2 回帰木 データセット2: ワイン品質データ - winequality-white.csv - fixed acidity: 酢酸濃度 - volitle acidity: 揮発酸濃度 - citric acidity: クエン酸濃度 - chlorides: 塩化物 - sulfur dioxide: 二酸化硫黄 - sulphate: 硫酸塩 - fixed acidity: 酒石酸含有量（g/dm3) - volatile acidity: 酢酸含有量（g/dm3) - citric acid: クエン酸含有量（g/dm3) - residual sugar: 残留糖分含有量（g/dm3） - chlorides: 塩化ナトリウム含有量（g/dm3) - free sulfur dioxide: 遊離亜硫酸含有量（mg/dm3） - total sulfur dioxide: 総亜硫酸含有量（mg/dm3） - density: 密度（g/dm3) - pH: pH - sulphates: 硫酸カリウム含有量（g/dm3） - alcohol: アルコール度数（% vol.） - quality: ワインの品質 (0 (very bad) -- 10 (excellent)) 全データセットを学習用とテスト用にランダムに分割. wine &lt;- read.csv(&quot;winequality-white.csv&quot;, sep = &quot;;&quot;, skip = 1, header = T) set.seed(100) smpl_idx &lt;- sample(1:nrow(wine), 3000) # 元データ(行番号)から3000件を非復元抽出 wine_train &lt;- wine[smpl_idx, ] # 学習用データセット wine_test &lt;- wine[ - smpl_idx, ] # テスト用データセット str(wine) #&gt; &#39;data.frame&#39;: 4898 obs. of 12 variables: #&gt; $ fixed.acidity : num 7 6.3 8.1 7.2 7.2 8.1 6.2 7 6.3 8.1 ... #&gt; $ volatile.acidity : num 0.27 0.3 0.28 0.23 0.23 0.28 0.32 0.27 0.3 0.22 ... #&gt; $ citric.acid : num 0.36 0.34 0.4 0.32 0.32 0.4 0.16 0.36 0.34 0.43 ... #&gt; $ residual.sugar : num 20.7 1.6 6.9 8.5 8.5 6.9 7 20.7 1.6 1.5 ... #&gt; $ chlorides : num 0.045 0.049 0.05 0.058 0.058 0.05 0.045 0.045 0.049 0.044 ... #&gt; $ free.sulfur.dioxide : num 45 14 30 47 47 30 30 45 14 28 ... #&gt; $ total.sulfur.dioxide: num 170 132 97 186 186 97 136 170 132 129 ... #&gt; $ density : num 1.001 0.994 0.995 0.996 0.996 ... #&gt; $ pH : num 3 3.3 3.26 3.19 3.19 3.26 3.18 3 3.3 3.22 ... #&gt; $ sulphates : num 0.45 0.49 0.44 0.4 0.4 0.44 0.47 0.45 0.49 0.45 ... #&gt; $ alcohol : num 8.8 9.5 10.1 9.9 9.9 10.1 9.6 8.8 9.5 11 ... #&gt; $ quality : int 6 6 6 6 6 6 6 6 6 6 ... hist(wine$quality) 回帰木の適合 library(rpart) wine_rp &lt;- rpart(quality ~ . , data = wine_train) # quality以外の変数を説明変数に使用 wine_rp #&gt; n= 3000 #&gt; #&gt; node), split, n, deviance, yval #&gt; * denotes terminal node #&gt; #&gt; 1) root 3000 2320.0400 5.880333 #&gt; 2) alcohol&lt; 10.85 1883 1106.7690 5.612852 #&gt; 4) volatile.acidity&gt;=0.2525 985 464.4934 5.358376 * #&gt; 5) volatile.acidity&lt; 0.2525 898 508.5223 5.891982 #&gt; 10) volatile.acidity&gt;=0.2075 437 217.9405 5.720824 * #&gt; 11) volatile.acidity&lt; 0.2075 461 265.6443 6.054230 #&gt; 22) residual.sugar&lt; 12.575 369 183.0244 5.926829 * #&gt; 23) residual.sugar&gt;=12.575 92 52.6087 6.565217 * #&gt; 3) alcohol&gt;=10.85 1117 851.4396 6.331244 #&gt; 6) free.sulfur.dioxide&lt; 11.5 66 67.5303 5.378788 * #&gt; 7) free.sulfur.dioxide&gt;=11.5 1051 720.2759 6.391056 #&gt; 14) alcohol&lt; 12.45 795 512.6717 6.275472 * #&gt; 15) alcohol&gt;=12.45 256 164.0000 6.750000 * 得られた回帰木の可視化 決定木は, 説明変数の空間を分割するルールを生成する手法であり, 解釈容易性が強みである. 解釈容易性を助けるため, 生成されたルールを表す木を表示するための関数が用意されている. # 可視化 library(rpart.plot) rpart.plot(wine_rp, digit = 3) rpart.plot(wine_rp, digit = 4, fallen.leaves = T, type = 3, extra = 101) CP表 (Complexity Parameter Table) plotcp() の横軸は CP（complexity parameter）で, CP が小さい (右に進む) ほど木は複雑になり, 交差検証誤差（縦軸）がどの程度変化するかを示す. 通常, 木を複雑にしていくと誤差は一度減少するが, 過度に複雑にすると過学習により誤差が悪化することがある. 「最良の cp」として, 標準的には以下のような方法が提案されている: 最小の 縦軸の値 (xerror) を与える cpを選ぶ “1-標準誤差ルール”: 最小の xerror の 1 標準誤差以内に収まる範囲で 最も単純な木 (＝最も大きい cp) を選ぶ なお, plotcp()は, “1-標準誤差ルール”にしたがって, 最小のxerrorプラス1標準誤差の値の大きさを点線で表される水平線で表現しており, これを下回る最初 (一番左) の cp を見つければ良い. # CP値 vs 交差検証 (CV) 予測誤差 printcp(wine_rp) #&gt; #&gt; Regression tree: #&gt; rpart(formula = quality ~ ., data = wine_train) #&gt; #&gt; Variables actually used in tree construction: #&gt; [1] alcohol free.sulfur.dioxide residual.sugar #&gt; [4] volatile.acidity #&gt; #&gt; Root node error: 2320/3000 = 0.77335 #&gt; #&gt; n= 3000 #&gt; #&gt; CP nsplit rel error xerror xstd #&gt; 1 0.155959 0 1.00000 1.00109 0.027711 #&gt; 2 0.057651 1 0.84404 0.84827 0.026277 #&gt; 3 0.027428 2 0.78639 0.80140 0.025794 #&gt; 4 0.018795 3 0.75896 0.77812 0.024591 #&gt; 5 0.011842 4 0.74017 0.75908 0.024127 #&gt; 6 0.010000 6 0.71648 0.74854 0.023784 plotcp(wine_rp) 手動による剪定例 関数prune()を使うことで, 事前に指定した停止ルールによるアルゴリズムによる自動的な木構造の決定のあと, 事後的に枝を切ることで木の大きさを小さく整える (剪定する) ことができる. 剪定には, 上述のcpの大きさを指定する. ここでは, 例示のため, cpを大きめに取ってみる (→ 結果的にリーフの数が3枚になる) # 手動による剪定例 (complex parameter(cp)の大きさに基づいて) wine_rp1 &lt;- prune(wine_rp, cp = 0.03) plot(wine_rp1, uniform = T, margin = 0.05) text(wine_rp1, all = T, use.n = T) 予測 内挿予測には, 関数predict()使用する. # 学習データによる予測の精度 wine_rp_train &lt;- predict(wine_rp) cor(wine_rp_train, wine_train$quality) #&gt; [1] 0.5324631 さらに, predict()の第二引数 (newdata) に対して, 先に取り分けて学習に使わなかったテスト用データセット wine_testを指定することで, 外挿予測を行うことができる. # パフォーマンス評価 wine_rp_pred &lt;- predict(wine_rp, wine_test) summary(wine_rp_pred) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 5.358 5.358 5.721 5.870 6.275 6.750 summary(wine_test$quality) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 3.000 5.000 6.000 5.874 6.000 9.000 summary(wine_rp_pred) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 5.358 5.358 5.721 5.870 6.275 6.750 cor(wine_rp_pred, wine_test$quality) #&gt; [1] 0.5093653 # plot(wine_test$quality, wine_rp_pred) 自主課題 上で得られた決定木を解釈してみよう. 8.3 分類木 データセット3: 自動車評価データ - car.data.txt - Class (評価, 4水準), unacc, acc, good, vgood - buying (価格帯, 4): vhigh, high, med, low. - maint (維持費, 4): vhigh, high, med, low. - doors (ドア数, 4): 2, 3, 4, 5more. - persons (乗車人数, 3): 2, 4, more. - lug_boot (収納性, 3): small, med, big. - safety (安全性, 3): low, med, high. - source: https://archive.ics.uci.edu/ml/datasets/Car+Evaluation - 注) factorの並び (Levels) はデフォルトではアルファベット順 library(rpart) cardata0 &lt;- read.csv(&quot;car.data.txt&quot;, skip = 6) attach(cardata0) # factorの並び(Levels)はデフォルトではアルファベット順 # → 好ましくない～好ましいの順に, 並べ替え 評価 &lt;- factor(Class, levels = c(&quot;unacc&quot;, &quot;acc&quot;, &quot;good&quot;, &quot;vgood&quot;)) 価格帯 &lt;- factor(buying, levels = c(&quot;vhigh&quot;, &quot;high&quot;, &quot;med&quot;, &quot;low&quot;)) 維持費 &lt;- factor(maint, levels = c(&quot;vhigh&quot;, &quot;high&quot;, &quot;med&quot;, &quot;low&quot;)) ドア数 &lt;- factor(doors, levels = c(&quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5more&quot;)) 乗車人数 &lt;- factor(persons, levels = c(&quot;2&quot;, &quot;4&quot;, &quot;more&quot;)) 収納性 &lt;- factor(lug_boot, levels = c(&quot;small&quot;, &quot;med&quot;, &quot;big&quot;)) 安全性 &lt;- factor(safety, levels = c(&quot;low&quot;, &quot;med&quot;, &quot;high&quot;)) # cardata1 &lt;- data.frame(評価, 価格帯, 維持費, ドア数, 乗車人数, 収納性, 安全性) head(cardata1) #&gt; 評価 価格帯 維持費 ドア数 乗車人数 収納性 安全性 #&gt; 1 unacc vhigh vhigh 2 2 small low #&gt; 2 unacc vhigh vhigh 2 2 small med #&gt; 3 unacc vhigh vhigh 2 2 small high #&gt; 4 unacc vhigh vhigh 2 2 med low #&gt; 5 unacc vhigh vhigh 2 2 med med #&gt; 6 unacc vhigh vhigh 2 2 med high データセットの分割 全データセットを学習用とテスト用にランダムに分割. # 元データを, 学習用データとテスト用データに分割 # 学習用データでモデルに学習させる set.seed(100) smpl_idx &lt;- sample(1:nrow(cardata1), 1500) # 元データ(行番号)から1500件を非復元抽出 car_train &lt;- cardata1[smpl_idx, ] # 学習用データセット, 1500件 car_test &lt;- cardata1[ - smpl_idx, ] # テスト用データセット, 残り 学習用データセットで訓練 (学習) する. cardata &lt;- car_train table(car_train[, c(&quot;評価&quot;, &quot;価格帯&quot;)]) #&gt; 価格帯 #&gt; 評価 vhigh high med low #&gt; unacc 307 281 239 221 #&gt; acc 65 92 94 79 #&gt; good 0 0 20 40 #&gt; vgood 0 0 24 38 table(car_train[, c(&quot;評価&quot;, &quot;維持費&quot;)]) #&gt; 維持費 #&gt; 評価 vhigh high med low #&gt; unacc 310 270 235 233 #&gt; acc 60 89 102 79 #&gt; good 0 0 22 38 #&gt; vgood 0 12 25 25 table(car_train[, c(&quot;評価&quot;, &quot;ドア数&quot;)]) #&gt; ドア数 #&gt; 評価 2 3 4 5more #&gt; unacc 282 263 255 248 #&gt; acc 74 82 86 88 #&gt; good 14 15 16 15 #&gt; vgood 10 13 19 20 table(car_train[, c(&quot;評価&quot;, &quot;乗車人数&quot;)]) #&gt; 乗車人数 #&gt; 評価 2 4 more #&gt; unacc 494 283 271 #&gt; acc 0 169 161 #&gt; good 0 31 29 #&gt; vgood 0 29 33 table(car_train[, c(&quot;評価&quot;, &quot;収納性&quot;)]) # balanced #&gt; 収納性 #&gt; 評価 small med big #&gt; unacc 393 341 314 #&gt; acc 94 112 124 #&gt; good 18 24 18 #&gt; vgood 0 24 38 分類木の適合 res_rp &lt;- rpart(評価 ~ ., data = car_train) summary(res_rp) #&gt; Call: #&gt; rpart(formula = 評価 ~ ., data = car_train) #&gt; n= 1500 #&gt; #&gt; CP nsplit rel error xerror xstd #&gt; 1 0.12389381 0 1.0000000 1.0000000 0.03931568 #&gt; 2 0.11283186 2 0.7522124 0.9181416 0.03833142 #&gt; 3 0.04535398 4 0.5265487 0.5265487 0.03130647 #&gt; 4 0.03982301 6 0.4358407 0.4889381 0.03037018 #&gt; 5 0.03097345 7 0.3960177 0.4137168 0.02830539 #&gt; 6 0.02876106 9 0.3340708 0.3805310 0.02730104 #&gt; 7 0.02101770 10 0.3053097 0.3429204 0.02608212 #&gt; 8 0.01880531 12 0.2632743 0.3163717 0.02516366 #&gt; 9 0.01327434 14 0.2256637 0.2389381 0.02214866 #&gt; 10 0.01000000 16 0.1991150 0.2212389 0.02137371 #&gt; #&gt; Variable importance #&gt; 安全性 乗車人数 維持費 収納性 価格帯 ドア数 #&gt; 30 28 20 10 10 1 #&gt; #&gt; Node number 1: 1500 observations, complexity param=0.1238938 #&gt; predicted class=unacc expected loss=0.3013333 P(node) =1 #&gt; class counts: 1048 330 60 62 #&gt; probabilities: 0.699 0.220 0.040 0.041 #&gt; left son=2 (508 obs) right son=3 (992 obs) #&gt; Primary splits: #&gt; 安全性 splits as LRR, improve=109.468500, (0 missing) #&gt; 乗車人数 splits as LRR, improve=104.970300, (0 missing) #&gt; 価格帯 splits as LLRR, improve= 17.172040, (0 missing) #&gt; 維持費 splits as LLRR, improve= 14.140400, (0 missing) #&gt; 収納性 splits as LRR, improve= 7.005849, (0 missing) #&gt; #&gt; Node number 2: 508 observations #&gt; predicted class=unacc expected loss=0 P(node) =0.3386667 #&gt; class counts: 508 0 0 0 #&gt; probabilities: 1.000 0.000 0.000 0.000 #&gt; #&gt; Node number 3: 992 observations, complexity param=0.1238938 #&gt; predicted class=unacc expected loss=0.4556452 P(node) =0.6613333 #&gt; class counts: 540 330 60 62 #&gt; probabilities: 0.544 0.333 0.060 0.062 #&gt; left son=6 (322 obs) right son=7 (670 obs) #&gt; Primary splits: #&gt; 乗車人数 splits as LRR, improve=155.34520, (0 missing) #&gt; 価格帯 splits as LLRR, improve= 26.05666, (0 missing) #&gt; 維持費 splits as LLRR, improve= 19.14227, (0 missing) #&gt; 安全性 splits as -LR, improve= 11.33566, (0 missing) #&gt; 収納性 splits as LRR, improve= 11.03210, (0 missing) #&gt; #&gt; Node number 6: 322 observations #&gt; predicted class=unacc expected loss=0 P(node) =0.2146667 #&gt; class counts: 322 0 0 0 #&gt; probabilities: 1.000 0.000 0.000 0.000 #&gt; #&gt; Node number 7: 670 observations, complexity param=0.1128319 #&gt; predicted class=acc expected loss=0.5074627 P(node) =0.4466667 #&gt; class counts: 218 330 60 62 #&gt; probabilities: 0.325 0.493 0.090 0.093 #&gt; left son=14 (333 obs) right son=15 (337 obs) #&gt; Primary splits: #&gt; 価格帯 splits as LLRR, improve=38.596470, (0 missing) #&gt; 維持費 splits as LLRR, improve=29.807250, (0 missing) #&gt; 収納性 splits as LRR, improve=17.341050, (0 missing) #&gt; 安全性 splits as -LR, improve=15.372940, (0 missing) #&gt; ドア数 splits as LRRR, improve= 4.288712, (0 missing) #&gt; Surrogate splits: #&gt; ドア数 splits as RLRR, agree=0.516, adj=0.027, (0 split) #&gt; 安全性 splits as -LR, agree=0.510, adj=0.015, (0 split) #&gt; 乗車人数 splits as -LR, agree=0.509, adj=0.012, (0 split) #&gt; 維持費 splits as RRRL, agree=0.507, adj=0.009, (0 split) #&gt; 収納性 splits as LRR, agree=0.504, adj=0.003, (0 split) #&gt; #&gt; Node number 14: 333 observations, complexity param=0.1128319 #&gt; predicted class=unacc expected loss=0.4714715 P(node) =0.222 #&gt; class counts: 176 157 0 0 #&gt; probabilities: 0.529 0.471 0.000 0.000 #&gt; left son=28 (162 obs) right son=29 (171 obs) #&gt; Primary splits: #&gt; 維持費 splits as LLRR, improve=51.712340, (0 missing) #&gt; 収納性 splits as LRR, improve=14.322380, (0 missing) #&gt; 安全性 splits as -LR, improve=12.358880, (0 missing) #&gt; 価格帯 splits as LR--, improve= 5.866990, (0 missing) #&gt; ドア数 splits as LRRR, improve= 2.000571, (0 missing) #&gt; Surrogate splits: #&gt; ドア数 splits as RRLR, agree=0.520, adj=0.012, (0 split) #&gt; 収納性 splits as LRR, agree=0.517, adj=0.006, (0 split) #&gt; #&gt; Node number 15: 337 observations, complexity param=0.04535398 #&gt; predicted class=acc expected loss=0.4866469 P(node) =0.2246667 #&gt; class counts: 42 173 60 62 #&gt; probabilities: 0.125 0.513 0.178 0.184 #&gt; left son=30 (166 obs) right son=31 (171 obs) #&gt; Primary splits: #&gt; 維持費 splits as LLRR, improve=30.432380, (0 missing) #&gt; 安全性 splits as -LR, improve=15.159790, (0 missing) #&gt; 収納性 splits as LRR, improve=10.848300, (0 missing) #&gt; 価格帯 splits as --LR, improve= 2.920132, (0 missing) #&gt; ドア数 splits as LRRR, improve= 2.729825, (0 missing) #&gt; Surrogate splits: #&gt; ドア数 splits as RRRL, agree=0.513, adj=0.012, (0 split) #&gt; 収納性 splits as LRR, agree=0.513, adj=0.012, (0 split) #&gt; 安全性 splits as -LR, agree=0.513, adj=0.012, (0 split) #&gt; #&gt; Node number 28: 162 observations, complexity param=0.0210177 #&gt; predicted class=unacc expected loss=0.1851852 P(node) =0.108 #&gt; class counts: 132 30 0 0 #&gt; probabilities: 0.815 0.185 0.000 0.000 #&gt; left son=56 (82 obs) right son=57 (80 obs) #&gt; Primary splits: #&gt; 価格帯 splits as LR--, improve=11.3888900, (0 missing) #&gt; 維持費 splits as LR--, improve=10.8401100, (0 missing) #&gt; 収納性 splits as LLR, improve= 1.9067460, (0 missing) #&gt; 安全性 splits as -LR, improve= 0.9437920, (0 missing) #&gt; ドア数 splits as LLRR, improve= 0.2358401, (0 missing) #&gt; Surrogate splits: #&gt; 収納性 splits as LLR, agree=0.531, adj=0.050, (0 split) #&gt; ドア数 splits as LRLR, agree=0.519, adj=0.025, (0 split) #&gt; #&gt; Node number 29: 171 observations, complexity param=0.03097345 #&gt; predicted class=acc expected loss=0.2573099 P(node) =0.114 #&gt; class counts: 44 127 0 0 #&gt; probabilities: 0.257 0.743 0.000 0.000 #&gt; left son=58 (57 obs) right son=59 (114 obs) #&gt; Primary splits: #&gt; 収納性 splits as LRR, improve=15.81287000, (0 missing) #&gt; 安全性 splits as -LR, improve=14.94250000, (0 missing) #&gt; ドア数 splits as LRRR, improve= 3.05237700, (0 missing) #&gt; 乗車人数 splits as -RL, improve= 0.05959792, (0 missing) #&gt; 価格帯 splits as LR--, improve= 0.05664293, (0 missing) #&gt; #&gt; Node number 30: 166 observations, complexity param=0.01880531 #&gt; predicted class=acc expected loss=0.2831325 P(node) =0.1106667 #&gt; class counts: 35 119 0 12 #&gt; probabilities: 0.211 0.717 0.000 0.072 #&gt; left son=60 (58 obs) right son=61 (108 obs) #&gt; Primary splits: #&gt; 収納性 splits as LRR, improve=7.922795, (0 missing) #&gt; 安全性 splits as -LR, improve=7.674699, (0 missing) #&gt; ドア数 splits as LRRR, improve=2.069439, (0 missing) #&gt; 維持費 splits as LR--, improve=1.602410, (0 missing) #&gt; 価格帯 splits as --LR, improve=1.284737, (0 missing) #&gt; #&gt; Node number 31: 171 observations, complexity param=0.04535398 #&gt; predicted class=good expected loss=0.6491228 P(node) =0.114 #&gt; class counts: 7 54 60 50 #&gt; probabilities: 0.041 0.316 0.351 0.292 #&gt; left son=62 (81 obs) right son=63 (90 obs) #&gt; Primary splits: #&gt; 安全性 splits as -LR, improve=22.499420, (0 missing) #&gt; 収納性 splits as LRR, improve=12.487990, (0 missing) #&gt; 維持費 splits as --LR, improve= 4.316887, (0 missing) #&gt; 価格帯 splits as --LR, improve= 3.767329, (0 missing) #&gt; ドア数 splits as LRRR, improve= 1.797828, (0 missing) #&gt; Surrogate splits: #&gt; ドア数 splits as RRLR, agree=0.538, adj=0.025, (0 split) #&gt; #&gt; Node number 56: 82 observations #&gt; predicted class=unacc expected loss=0 P(node) =0.05466667 #&gt; class counts: 82 0 0 0 #&gt; probabilities: 1.000 0.000 0.000 0.000 #&gt; #&gt; Node number 57: 80 observations, complexity param=0.0210177 #&gt; predicted class=unacc expected loss=0.375 P(node) =0.05333333 #&gt; class counts: 50 30 0 0 #&gt; probabilities: 0.625 0.375 0.000 0.000 #&gt; left son=114 (39 obs) right son=115 (41 obs) #&gt; Primary splits: #&gt; 維持費 splits as LR--, improve=21.40244000, (0 missing) #&gt; 収納性 splits as LLR, improve= 2.65723300, (0 missing) #&gt; 安全性 splits as -LR, improve= 1.91526000, (0 missing) #&gt; ドア数 splits as LLRR, improve= 0.56441530, (0 missing) #&gt; 乗車人数 splits as -LR, improve= 0.05639098, (0 missing) #&gt; Surrogate splits: #&gt; 収納性 splits as RLR, agree=0.562, adj=0.103, (0 split) #&gt; ドア数 splits as RLRR, agree=0.525, adj=0.026, (0 split) #&gt; #&gt; Node number 58: 57 observations, complexity param=0.03097345 #&gt; predicted class=unacc expected loss=0.4385965 P(node) =0.038 #&gt; class counts: 32 25 0 0 #&gt; probabilities: 0.561 0.439 0.000 0.000 #&gt; left son=116 (28 obs) right son=117 (29 obs) #&gt; Primary splits: #&gt; 安全性 splits as -LR, improve=21.17362000, (0 missing) #&gt; ドア数 splits as LRRR, improve= 1.20350900, (0 missing) #&gt; 乗車人数 splits as -RL, improve= 1.03815600, (0 missing) #&gt; 維持費 splits as --RL, improve= 0.09980507, (0 missing) #&gt; 価格帯 splits as LR--, improve= 0.05032432, (0 missing) #&gt; Surrogate splits: #&gt; 価格帯 splits as LR--, agree=0.526, adj=0.036, (0 split) #&gt; 維持費 splits as --RL, agree=0.526, adj=0.036, (0 split) #&gt; ドア数 splits as RRLR, agree=0.526, adj=0.036, (0 split) #&gt; 乗車人数 splits as -RL, agree=0.526, adj=0.036, (0 split) #&gt; #&gt; Node number 59: 114 observations #&gt; predicted class=acc expected loss=0.1052632 P(node) =0.076 #&gt; class counts: 12 102 0 0 #&gt; probabilities: 0.105 0.895 0.000 0.000 #&gt; #&gt; Node number 60: 58 observations, complexity param=0.01880531 #&gt; predicted class=acc expected loss=0.4482759 P(node) =0.03866667 #&gt; class counts: 26 32 0 0 #&gt; probabilities: 0.448 0.552 0.000 0.000 #&gt; left son=120 (29 obs) right son=121 (29 obs) #&gt; Primary splits: #&gt; 安全性 splits as -LR, improve=13.7931000, (0 missing) #&gt; 維持費 splits as LR--, improve= 2.2068970, (0 missing) #&gt; ドア数 splits as LRRR, improve= 1.9956380, (0 missing) #&gt; 価格帯 splits as --LR, improve= 0.8277504, (0 missing) #&gt; 乗車人数 splits as -RL, improve= 0.3325123, (0 missing) #&gt; Surrogate splits: #&gt; 維持費 splits as LR--, agree=0.552, adj=0.103, (0 split) #&gt; ドア数 splits as LLRR, agree=0.534, adj=0.069, (0 split) #&gt; #&gt; Node number 61: 108 observations #&gt; predicted class=acc expected loss=0.1944444 P(node) =0.072 #&gt; class counts: 9 87 0 12 #&gt; probabilities: 0.083 0.806 0.000 0.111 #&gt; #&gt; Node number 62: 81 observations, complexity param=0.02876106 #&gt; predicted class=acc expected loss=0.4444444 P(node) =0.054 #&gt; class counts: 3 45 33 0 #&gt; probabilities: 0.037 0.556 0.407 0.000 #&gt; left son=124 (28 obs) right son=125 (53 obs) #&gt; Primary splits: #&gt; 収納性 splits as LRR, improve=12.1816400, (0 missing) #&gt; 価格帯 splits as --LR, improve= 2.6916900, (0 missing) #&gt; 維持費 splits as --LR, improve= 2.2466560, (0 missing) #&gt; ドア数 splits as LLRR, improve= 1.5350200, (0 missing) #&gt; 乗車人数 splits as -RL, improve= 0.5724932, (0 missing) #&gt; #&gt; Node number 63: 90 observations, complexity param=0.03982301 #&gt; predicted class=vgood expected loss=0.4444444 P(node) =0.06 #&gt; class counts: 4 9 27 50 #&gt; probabilities: 0.044 0.100 0.300 0.556 #&gt; left son=126 (28 obs) right son=127 (62 obs) #&gt; Primary splits: #&gt; 収納性 splits as LRR, improve=18.2472100, (0 missing) #&gt; ドア数 splits as LLRR, improve= 3.1671560, (0 missing) #&gt; 維持費 splits as --LR, improve= 1.8000000, (0 missing) #&gt; 価格帯 splits as --LR, improve= 1.4888890, (0 missing) #&gt; 乗車人数 splits as -RL, improve= 0.6274484, (0 missing) #&gt; #&gt; Node number 114: 39 observations #&gt; predicted class=unacc expected loss=0 P(node) =0.026 #&gt; class counts: 39 0 0 0 #&gt; probabilities: 1.000 0.000 0.000 0.000 #&gt; #&gt; Node number 115: 41 observations #&gt; predicted class=acc expected loss=0.2682927 P(node) =0.02733333 #&gt; class counts: 11 30 0 0 #&gt; probabilities: 0.268 0.732 0.000 0.000 #&gt; #&gt; Node number 116: 28 observations #&gt; predicted class=unacc expected loss=0 P(node) =0.01866667 #&gt; class counts: 28 0 0 0 #&gt; probabilities: 1.000 0.000 0.000 0.000 #&gt; #&gt; Node number 117: 29 observations #&gt; predicted class=acc expected loss=0.137931 P(node) =0.01933333 #&gt; class counts: 4 25 0 0 #&gt; probabilities: 0.138 0.862 0.000 0.000 #&gt; #&gt; Node number 120: 29 observations #&gt; predicted class=unacc expected loss=0.2068966 P(node) =0.01933333 #&gt; class counts: 23 6 0 0 #&gt; probabilities: 0.793 0.207 0.000 0.000 #&gt; #&gt; Node number 121: 29 observations #&gt; predicted class=acc expected loss=0.1034483 P(node) =0.01933333 #&gt; class counts: 3 26 0 0 #&gt; probabilities: 0.103 0.897 0.000 0.000 #&gt; #&gt; Node number 124: 28 observations #&gt; predicted class=acc expected loss=0.1071429 P(node) =0.01866667 #&gt; class counts: 3 25 0 0 #&gt; probabilities: 0.107 0.893 0.000 0.000 #&gt; #&gt; Node number 125: 53 observations, complexity param=0.01327434 #&gt; predicted class=good expected loss=0.3773585 P(node) =0.03533333 #&gt; class counts: 0 20 33 0 #&gt; probabilities: 0.000 0.377 0.623 0.000 #&gt; left son=250 (25 obs) right son=251 (28 obs) #&gt; Primary splits: #&gt; 価格帯 splits as --LR, improve=4.69137500, (0 missing) #&gt; 維持費 splits as --LR, improve=2.97708900, (0 missing) #&gt; ドア数 splits as LRRR, improve=2.68221700, (0 missing) #&gt; 収納性 splits as -LR, improve=1.42290200, (0 missing) #&gt; 乗車人数 splits as -LR, improve=0.04851752, (0 missing) #&gt; Surrogate splits: #&gt; 維持費 splits as --RL, agree=0.547, adj=0.04, (0 split) #&gt; #&gt; Node number 126: 28 observations #&gt; predicted class=good expected loss=0.3571429 P(node) =0.01866667 #&gt; class counts: 4 6 18 0 #&gt; probabilities: 0.143 0.214 0.643 0.000 #&gt; #&gt; Node number 127: 62 observations #&gt; predicted class=vgood expected loss=0.1935484 P(node) =0.04133333 #&gt; class counts: 0 3 9 50 #&gt; probabilities: 0.000 0.048 0.145 0.806 #&gt; #&gt; Node number 250: 25 observations, complexity param=0.01327434 #&gt; predicted class=acc expected loss=0.4 P(node) =0.01666667 #&gt; class counts: 0 15 10 0 #&gt; probabilities: 0.000 0.600 0.400 0.000 #&gt; left son=500 (12 obs) right son=501 (13 obs) #&gt; Primary splits: #&gt; 維持費 splits as --LR, improve=7.38461500, (0 missing) #&gt; ドア数 splits as LLRR, improve=0.63636360, (0 missing) #&gt; 収納性 splits as -LR, improve=0.11688310, (0 missing) #&gt; 乗車人数 splits as -LR, improve=0.05194805, (0 missing) #&gt; Surrogate splits: #&gt; ドア数 splits as RRLR, agree=0.56, adj=0.083, (0 split) #&gt; 収納性 splits as -RL, agree=0.56, adj=0.083, (0 split) #&gt; #&gt; Node number 251: 28 observations #&gt; predicted class=good expected loss=0.1785714 P(node) =0.01866667 #&gt; class counts: 0 5 23 0 #&gt; probabilities: 0.000 0.179 0.821 0.000 #&gt; #&gt; Node number 500: 12 observations #&gt; predicted class=acc expected loss=0 P(node) =0.008 #&gt; class counts: 0 12 0 0 #&gt; probabilities: 0.000 1.000 0.000 0.000 #&gt; #&gt; Node number 501: 13 observations #&gt; predicted class=good expected loss=0.2307692 P(node) =0.008666667 #&gt; class counts: 0 3 10 0 #&gt; probabilities: 0.000 0.231 0.769 0.000 CP表 (Complexity Parameter Table) # CP値 vs 交差検証 (CV) 予測誤差 plotcp(res_rp) 可視化 ここでは, cp=0.1に指定して適合. library(rpart.plot) res.rp &lt;- rpart(評価 ~ ., data = car_train, control = rpart.control(cp = 0.1)) rpart.plot(res_rp, digit = 3) #, col = c(&quot;red&quot;, &quot;orange&quot;, &quot;yellow&quot;, &quot;blue&quot;)) rpart.plot(res_rp, digit = 4, fallen.leaves = T, type = 3, extra = 101) パフォーマンス評価 学習済モデルを使い, テスト用データで外挿予測を行う. car_rp_pred &lt;- predict(res_rp, car_test, type = &quot;class&quot;) # クラス分類の予測結果出力 #car_rp_pred &lt;- predict(res_rp, car_test, type = &quot;prob&quot;) # 確率の出力 ここでは, パッケージcaretの関数confusionMatrix()を使い, 混同行列に基づいて各種パフォーマンス評価指標を計算する. confusionMatrix()では, 2種類の表示モードを選択することができる: 適合率 (precision) - 再現率 (recall) 表示 感度(sensitivity) - 特異度 (specificity) 表示 # 混同行列 tbl_rp &lt;- table(car_rp_pred, car_test$評価) library(caret) # 適合率 (precision) - 再現率 (recall) 表示 confusionMatrix(tbl_rp, mode = &quot;prec_recall&quot;) #&gt; Confusion Matrix and Statistics #&gt; #&gt; #&gt; car_rp_pred unacc acc good vgood #&gt; unacc 159 1 0 0 #&gt; acc 3 51 0 1 #&gt; good 0 2 9 0 #&gt; vgood 0 0 0 2 #&gt; #&gt; Overall Statistics #&gt; #&gt; Accuracy : 0.9693 #&gt; 95% CI : (0.9378, 0.9876) #&gt; No Information Rate : 0.7105 #&gt; P-Value [Acc &gt; NIR] : &lt; 2.2e-16 #&gt; #&gt; Kappa : 0.9306 #&gt; #&gt; Mcnemar&#39;s Test P-Value : NA #&gt; #&gt; Statistics by Class: #&gt; #&gt; Class: unacc Class: acc Class: good Class: vgood #&gt; Precision 0.9938 0.9273 0.81818 1.000000 #&gt; Recall 0.9815 0.9444 1.00000 0.666667 #&gt; F1 0.9876 0.9358 0.90000 0.800000 #&gt; Prevalence 0.7105 0.2368 0.03947 0.013158 #&gt; Detection Rate 0.6974 0.2237 0.03947 0.008772 #&gt; Detection Prevalence 0.7018 0.2412 0.04825 0.008772 #&gt; Balanced Accuracy 0.9832 0.9607 0.99543 0.833333 # 感度(sensitivity) - 特異度 (specificity) 表示 confusionMatrix(tbl_rp) # mode = &quot;sens_spec&quot; (デフォルト) #&gt; Confusion Matrix and Statistics #&gt; #&gt; #&gt; car_rp_pred unacc acc good vgood #&gt; unacc 159 1 0 0 #&gt; acc 3 51 0 1 #&gt; good 0 2 9 0 #&gt; vgood 0 0 0 2 #&gt; #&gt; Overall Statistics #&gt; #&gt; Accuracy : 0.9693 #&gt; 95% CI : (0.9378, 0.9876) #&gt; No Information Rate : 0.7105 #&gt; P-Value [Acc &gt; NIR] : &lt; 2.2e-16 #&gt; #&gt; Kappa : 0.9306 #&gt; #&gt; Mcnemar&#39;s Test P-Value : NA #&gt; #&gt; Statistics by Class: #&gt; #&gt; Class: unacc Class: acc Class: good Class: vgood #&gt; Sensitivity 0.9815 0.9444 1.00000 0.666667 #&gt; Specificity 0.9848 0.9770 0.99087 1.000000 #&gt; Pos Pred Value 0.9938 0.9273 0.81818 1.000000 #&gt; Neg Pred Value 0.9559 0.9827 1.00000 0.995575 #&gt; Prevalence 0.7105 0.2368 0.03947 0.013158 #&gt; Detection Rate 0.6974 0.2237 0.03947 0.008772 #&gt; Detection Prevalence 0.7018 0.2412 0.04825 0.008772 #&gt; Balanced Accuracy 0.9832 0.9607 0.99543 0.833333 正答率/正解率 (accuracy) を取り出す場合: (confusionMatrix(tbl_rp)$overall[&quot;Accuracy&quot;]) #&gt; Accuracy #&gt; 0.9692982 自主課題 上で得られた決定木 (分類規則) を解釈してみよう. 関数confusionMatrix()を収録しているパッケージcaretは, 多種多様な機械学習アルゴリズムを統一的な環境で 実行し比較できる環境を提供する. サポートベクターマシン (SVM), 勾配ブースティング, ランダムフォレスト, ニューラルネット, …, など多様な手法・アルゴリズムをサポートする. "],["主成分分析-pca.html", "9 主成分分析 (PCA) 9.1 基本操作 9.2 2種類のPCA: 相関行列ベース vs 分散共分散行列ベース", " 9 主成分分析 (PCA) Rの標準パッケージ内には, PCAを実行する関数として, princomp()の他にprcomp()が用意されている. 9.1 基本操作 データセット1: 従業員スキル評価データ(仮想) 文字化けする場合には, 英語版”testdat_eng.csv”を使用しても良い. &quot;testdat_30_jap.csv&quot; (日本語版/英語版) - 氏名/Name - 専門性/Expertise (0-100) - 分析力/Analytics (同) - リーダーシップ/Leadership (同) - プレゼン力/Presentation (同) - コミュ力/Communication (同) - n = 9, p = 5 tokuten &lt;- read.csv(&quot;testdat_jap.csv&quot;, header = T, row.names = 1, skip = 1) # colnames(tokuten) &lt;- c(&quot;name&quot;, &quot;E&quot;, &quot;A&quot;, &quot;L&quot;, &quot;P&quot;, &quot;C&quot;) tokuten # 散布図 pairs(tokuten) #&gt; 専門性 分析力 リーダーシップ プレゼン力 コミュ力 #&gt; 山田 80 87 70 65 67 #&gt; 鈴木 85 83 65 68 63 #&gt; 田中 75 70 80 79 75 #&gt; 中村 70 72 85 83 79 #&gt; 大野 88 85 85 90 91 #&gt; 松井 75 78 79 85 80 #&gt; 高木 80 80 75 75 80 #&gt; 三浦 83 72 85 80 83 #&gt; 佐藤 69 82 77 70 80 分散共分散行列に対するPCA（デフォルト) # 分散共分散行列に対するPCA（デフォルト) tokuten_pc &lt;- princomp(tokuten) summary(tokuten_pc) #&gt; Importance of components: #&gt; Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 #&gt; Standard deviation 12.4807420 7.1424787 4.73037885 2.9724343 1.57269337 #&gt; Proportion of Variance 0.6477709 0.2121478 0.09305346 0.0367422 0.01028558 #&gt; Cumulative Proportion 0.6477709 0.8599188 0.95297222 0.9897144 1.00000000 自主課題: 結果の解釈をしてみよう. スクリープロット (エルボープロット) plot(tokuten_pc) # Scree plot = barplot of PC variances (eigenvalues) # screeplot(tokuten_pc) # same as above 主成分負荷量 # 主成分負荷量(係数) tokuten_pc$loadings # t(tokuten_pc$loadings) %*% tokuten_pc$loadings # 直交性の確認 # 注) 下のprcompの結果と, 符号が逆転 #&gt; #&gt; Loadings: #&gt; Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 #&gt; 専門性 0.775 0.536 0.332 #&gt; 分析力 0.227 0.554 -0.634 -0.325 0.366 #&gt; リーダーシップ -0.510 -0.137 0.323 0.786 #&gt; プレゼン力 -0.591 0.153 0.275 -0.741 #&gt; コミュ力 -0.582 0.223 -0.486 0.362 -0.496 #&gt; #&gt; Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 #&gt; SS loadings 1.0 1.0 1.0 1.0 1.0 #&gt; Proportion Var 0.2 0.2 0.2 0.2 0.2 #&gt; Cumulative Var 0.2 0.4 0.6 0.8 1.0 自主課題: 結果の解釈をしてみよう. 主成分得点 # 主成分得点(データ毎) tokuten_pc$scores plot(tokuten_pc$scores[, 1], tokuten_pc$scores[, 2], type = &quot;n&quot;, main = &quot;PC scores: PC1 vs PC2&quot;) # text(tokuten_pc$scores[, 1], tokuten_pc$scores[, 2], text(tokuten_pc$scores[, 1], tokuten_pc$scores[, 2], rownames(tokuten), family = &quot;HiraKakuProN-W3&quot;) # 日本語文字化け対応 (mac) #&gt; Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 #&gt; 山田 19.304343 2.7048291 -2.4411238 0.5754498 2.6750248 #&gt; 鈴木 21.655520 4.6155395 5.6073003 -1.7490457 -0.7368287 #&gt; 田中 -2.739766 -8.0346039 5.4765050 0.1887216 -0.4687677 #&gt; 中村 -9.682974 -9.9826824 0.6190434 -2.0247358 1.8769769 #&gt; 大野 -17.277425 14.9224986 -1.8787424 -1.1322419 0.8666327 #&gt; 松井 -6.867523 -1.4328205 -0.3577567 -5.3750278 -1.0885751 #&gt; 高木 1.694979 2.5684468 -1.6399325 1.7537323 -2.8802837 #&gt; 三浦 -9.824535 0.5291295 4.8161616 5.9565572 0.4056066 #&gt; 佐藤 3.737381 -5.8903368 -10.2014547 1.8065903 -0.6497859 バイプロット 合成変数 (主成分) を縦横軸に取った平面上に, 各データ点の主成分得点を散布すると共に, 元の変数を矢印 (ベクトル) で表現したプロット. # バイプロット biplot(tokuten_pc, family = &quot;HiraKakuProN-W3&quot;) # 日本語文字化け対応 (mac) # biplot(tokuten_pc) # plot PC &amp; PC scores ===&gt; display first 2 PCs only... 自主課題: 結果の解釈をしてみよう. # # 計算結果確認 # (yama &lt;- tokuten[&quot;山田&quot;, ]-apply(tokuten, 2, mean)) # demeaned score # sum(tokuten_pc$loadings[, &quot;Comp.1&quot;] * yama) # Yamada&#39;s PC1 score # sum(tokuten_pc$loadings[, &quot;Comp.2&quot;] * yama) # Yamada&#39;s PC2 score # # compare this with tokuten_pc$scores[&quot;Yamada&quot;, ] 最初の2つの主成分を選択する場合の操作例 # 第1, 2主成分 tokuten_pc$scores[, 1:2] tokuten_pc$loadings[, 1:2] # biplot(tokuten_pc$scores[, 1:2], tokuten_pc$loadings[, 1:2]) # loadings accompany &quot;arrows&quot; &lt;-- Looks more useful!! biplot(tokuten_pc$scores[, 1:2], tokuten_pc$loadings[, 1:2], family = &quot;HiraKakuProN-W3&quot;) # 日本語文字化け対応 (mac) #&gt; Comp.1 Comp.2 #&gt; 山田 19.304343 2.7048291 #&gt; 鈴木 21.655520 4.6155395 #&gt; 田中 -2.739766 -8.0346039 #&gt; 中村 -9.682974 -9.9826824 #&gt; 大野 -17.277425 14.9224986 #&gt; 松井 -6.867523 -1.4328205 #&gt; 高木 1.694979 2.5684468 #&gt; 三浦 -9.824535 0.5291295 #&gt; 佐藤 3.737381 -5.8903368 #&gt; Comp.1 Comp.2 #&gt; 専門性 0.03162775 0.7753770 #&gt; 分析力 0.22710092 0.5541994 #&gt; リーダーシップ -0.50974398 -0.1365068 #&gt; プレゼン力 -0.59111858 0.1530044 #&gt; コミュ力 -0.58151936 0.2227311 主成分得点と元の変数の表示方法を入れ替えたもの. # biplot(tokuten_pc$loadings[, 1:2], tokuten_pc$scores[, 1:2]) # scores accompany &quot;arrows&quot; biplot(tokuten_pc$loadings[, 1:2], tokuten_pc$scores[, 1:2], family = &quot;HiraKakuProN-W3&quot;) # 日本語文字化け対応 (mac) # 元の変数 (評価項目) ペアごとの, 各従業員のスコア散布図 #plot(tokuten) # par(mfrow = c(2, 2)) plot(tokuten[, c(1, 2)], type = &quot;n&quot;, main = &quot;item 1 vs item 2&quot;) # plot only axis # text(tokuten[, c(1, 2)], rownames(tokuten)) text(tokuten[, c(1, 2)], rownames(tokuten), family = &quot;HiraKakuProN-W3&quot;) # 日本語文字化け対応 (mac) plot(tokuten[, c(2, 3)], type = &quot;n&quot;, main = &quot;item 2 vs item 3&quot;) # plot only axis # text(tokuten[, c(2, 3)], rownames(tokuten)) text(tokuten[, c(2, 3)], rownames(tokuten), family = &quot;HiraKakuProN-W3&quot;) # 日本語文字化け対応 (mac) # plot(tokuten[, c(3, 4)], type = &quot;n&quot;) # plot only axis # text(tokuten[, c(3, 4)], rownames(tokuten)) # plot(tokuten[, c(4, 5)], type = &quot;n&quot;) # plot only axis # text(tokuten[, c(4, 5)], rownames(tokuten)) 相関行列に対するPCA # 相関行列に対するPCA # tokuten_pc2 &lt;- princomp(tokuten, cor = T) # summary(tokuten_pc2) # tokuten_pc2$loadings # tokuten_pc2$scores[, 1:2] # tokuten_pc2$loadings[, 1:2] PCAを実行するためのprincomp()に代わる標準関数として, prcomp()がある. # 代替的関数 # prcomp() # 不偏分散共分散行列の使用 (÷(n-1)) tokuten_prcomp &lt;- prcomp(tokuten) # tokuten_prcomp &lt;- prcomp(tokuten, scale = T) summary(tokuten_prcomp) #&gt; Importance of components: #&gt; PC1 PC2 PC3 PC4 PC5 #&gt; Standard deviation 13.2378 7.5757 5.01732 3.15274 1.66809 #&gt; Proportion of Variance 0.6478 0.2122 0.09305 0.03674 0.01029 #&gt; Cumulative Proportion 0.6478 0.8599 0.95297 0.98971 1.00000 tokuten_prcomp$rotation # PC負荷量 #&gt; PC1 PC2 PC3 PC4 PC5 #&gt; 専門性 -0.03162775 0.7753770 0.53571555 0.3316583 -0.02831625 #&gt; 分析力 -0.22710092 0.5541994 -0.63357070 -0.3251925 -0.36623249 #&gt; リーダーシップ 0.50974398 -0.1365068 -0.01367592 0.3227645 -0.78559724 #&gt; プレゼン力 0.59111858 0.1530044 0.27483158 -0.7411911 0.04766342 #&gt; コミュ力 0.58151936 0.2227311 -0.48567230 0.3615403 0.49561793 tokuten_prcomp$x # PC得点 #&gt; PC1 PC2 PC3 PC4 PC5 #&gt; 山田 -19.304343 2.7048291 -2.4411238 0.5754498 -2.6750248 #&gt; 鈴木 -21.655520 4.6155395 5.6073003 -1.7490457 0.7368287 #&gt; 田中 2.739766 -8.0346039 5.4765050 0.1887216 0.4687677 #&gt; 中村 9.682974 -9.9826824 0.6190434 -2.0247358 -1.8769769 #&gt; 大野 17.277425 14.9224986 -1.8787424 -1.1322419 -0.8666327 #&gt; 松井 6.867523 -1.4328205 -0.3577567 -5.3750278 1.0885751 #&gt; 高木 -1.694979 2.5684468 -1.6399325 1.7537323 2.8802837 #&gt; 三浦 9.824535 0.5291295 4.8161616 5.9565572 -0.4056066 #&gt; 佐藤 -3.737381 -5.8903368 -10.2014547 1.8065903 0.6497859 # biplot(tokuten_prcomp) biplot(tokuten_prcomp, family = &quot;HiraKakuProN-W3&quot;) 9.2 2種類のPCA: 相関行列ベース vs 分散共分散行列ベース データセット2: USArrests data USArrests - 全米50州, 人口10万人当たり - 1973年の凶悪犯罪(assault, murder, rape)件数, 及び都市部の人口割合(%) - n = 50, p = 4 head(USArrests) summary(USArrests) pairs(USArrests) #&gt; Murder Assault UrbanPop Rape #&gt; Alabama 13.2 236 58 21.2 #&gt; Alaska 10.0 263 48 44.5 #&gt; Arizona 8.1 294 80 31.0 #&gt; Arkansas 8.8 190 50 19.5 #&gt; California 9.0 276 91 40.6 #&gt; Colorado 7.9 204 78 38.7 #&gt; Murder Assault UrbanPop Rape #&gt; Min. : 0.800 Min. : 45.0 Min. :32.00 Min. : 7.30 #&gt; 1st Qu.: 4.075 1st Qu.:109.0 1st Qu.:54.50 1st Qu.:15.07 #&gt; Median : 7.250 Median :159.0 Median :66.00 Median :20.10 #&gt; Mean : 7.788 Mean :170.8 Mean :65.54 Mean :21.23 #&gt; 3rd Qu.:11.250 3rd Qu.:249.0 3rd Qu.:77.75 3rd Qu.:26.18 #&gt; Max. :17.400 Max. :337.0 Max. :91.00 Max. :46.00 自主課題: 以下の2つのやり方のどちらが良いか考えてみよう. # どちらが良いか? # (pc_cr1 &lt;- princomp(USArrests)) # (pc_cr2 &lt;- princomp(USArrests, cor = TRUE)) (pc_cr3 &lt;- prcomp(USArrests)) #&gt; Standard deviations (1, .., p=4): #&gt; [1] 83.732400 14.212402 6.489426 2.482790 #&gt; #&gt; Rotation (n x k) = (4 x 4): #&gt; PC1 PC2 PC3 PC4 #&gt; Murder 0.04170432 -0.04482166 0.07989066 -0.99492173 #&gt; Assault 0.99522128 -0.05876003 -0.06756974 0.03893830 #&gt; UrbanPop 0.04633575 0.97685748 -0.20054629 -0.05816914 #&gt; Rape 0.07515550 0.20071807 0.97408059 0.07232502 (pc_cr4 &lt;- prcomp(USArrests, scale = TRUE)) #&gt; Standard deviations (1, .., p=4): #&gt; [1] 1.5748783 0.9948694 0.5971291 0.4164494 #&gt; #&gt; Rotation (n x k) = (4 x 4): #&gt; PC1 PC2 PC3 PC4 #&gt; Murder -0.5358995 -0.4181809 0.3412327 0.64922780 #&gt; Assault -0.5831836 -0.1879856 0.2681484 -0.74340748 #&gt; UrbanPop -0.2781909 0.8728062 0.3780158 0.13387773 #&gt; Rape -0.5434321 0.1673186 -0.8177779 0.08902432 plot(pc_cr3) summary(pc_cr3) #&gt; Importance of components: #&gt; PC1 PC2 PC3 PC4 #&gt; Standard deviation 83.7324 14.21240 6.4894 2.48279 #&gt; Proportion of Variance 0.9655 0.02782 0.0058 0.00085 #&gt; Cumulative Proportion 0.9655 0.99335 0.9991 1.00000 plot(pc_cr4) summary(pc_cr4) #&gt; Importance of components: #&gt; PC1 PC2 PC3 PC4 #&gt; Standard deviation 1.5749 0.9949 0.59713 0.41645 #&gt; Proportion of Variance 0.6201 0.2474 0.08914 0.04336 #&gt; Cumulative Proportion 0.6201 0.8675 0.95664 1.00000 自主課題: 最初のやり方 (pc_cr3) は. 一見PCAの実行結果として望ましいように見えるが, 実はナンセンスである. 出力結果を観察し, その理由を考えてみよう. 二番目のやり方 (pc_cr4) は, 相関行列に対するPCAである. 関数princomp()では, 引数cor=TRUEを, 関数prcomp()では, 引数scale=TRUEを指定すればよい. # PCAにかける変数の絞り込み # prcomp(~ Murder + Assault + Rape, data = USArrests, scale = TRUE) "],["探索的因子分析.html", "10 探索的因子分析 10.1 基本操作 10.2 パッケージpsychの利用 10.3 因子分析に有用なツール", " 10 探索的因子分析 10.1 基本操作 データセット1: 従業員評価データ (仮想) &quot;selfeval30_jp.csv&quot; (日本語版) - 氏名/Name - 専門性/Expertise (0-100) - 分析力/Analytics (同) - リーダーシップ/Leadership (同) - プレゼン力/Presentation (同) - コミュ力/Communication (同) - n = 30, p = 5 - ※ 英語版は, &quot;selfeval30_eg.csv&quot; tokuten &lt;- read.csv(&quot;selfeval30_jp.csv&quot;, header = T, row.names = 1) # tokuten &lt;- read.csv(&quot;selfeval30_eg.csv&quot;, header = T, row.names = 1) # 散布図 pairs(tokuten) Rでは, 因子分析を実行する関数としてfactanal()が標準的に用意されている. factanal()は, 最尤法を使ってモデル推定を行う. 他の推定法を試したい場合には, 後述のパッケージpsych内のfa()を使用すると良い. # (1) factanal: 最尤法only tokuten_fac &lt;- factanal(tokuten, factors = 2) # default varimax回転 tokuten_fac #&gt; #&gt; Call: #&gt; factanal(x = tokuten, factors = 2) #&gt; #&gt; Uniquenesses: #&gt; 専門性 分析力 リーダーシップ プレゼン力 コミュ力 #&gt; 0.312 0.124 0.429 0.340 0.150 #&gt; #&gt; Loadings: #&gt; Factor1 Factor2 #&gt; 専門性 0.180 0.810 #&gt; 分析力 0.223 0.909 #&gt; リーダーシップ 0.714 0.246 #&gt; プレゼン力 0.811 #&gt; コミュ力 0.866 0.315 #&gt; #&gt; Factor1 Factor2 #&gt; SS loadings 2.001 1.644 #&gt; Proportion Var 0.400 0.329 #&gt; Cumulative Var 0.400 0.729 #&gt; #&gt; Test of the hypothesis that 2 factors are sufficient. #&gt; The chi square statistic is 1.13 on 1 degree of freedom. #&gt; The p-value is 0.288 tokuten_fac$loading #&gt; #&gt; Loadings: #&gt; Factor1 Factor2 #&gt; 専門性 0.180 0.810 #&gt; 分析力 0.223 0.909 #&gt; リーダーシップ 0.714 0.246 #&gt; プレゼン力 0.811 #&gt; コミュ力 0.866 0.315 #&gt; #&gt; Factor1 Factor2 #&gt; SS loadings 2.001 1.644 #&gt; Proportion Var 0.400 0.329 #&gt; Cumulative Var 0.400 0.729 # 確認(各因子のloadingsベクトルの2乗和--&gt; SS loadings)) sum(tokuten_fac$loading[, 1]^2) #&gt; [1] 2.000541 得られた因子負荷量の解釈をしやすくするために, 棒グラフを用いることも多い. # 因子負荷量barplot(factor loadings) par(mfrow = c(1, 2)) barplot(tokuten_fac$loading[, 1], col = &quot;orange&quot;) barplot(tokuten_fac$loading[, 2], col = &quot;lightblue&quot;) Tips: プロット時, 日本語が文字化けする場合の対応 (特に, macユーザー): &gt; par(family = &quot;HiraKakuProN-W3&quot;) または, &gt; par(family = &quot;HG明朝E&quot;) 自主課題: 因子数を3に変えてfa()を実行し (引数factor=3), どのような出力になるかを確認してみよう. 因子行列の回転 # 因子（負荷量）行列の回転 # scores = &quot;regression&quot; ==&gt; Thomson&#39;s score返す （デフォルト, 戻り値無) tokuten_fac2 &lt;- factanal(tokuten, factors = 2, rotation = &quot;none&quot;, scores = &quot;regression&quot;) # 回転なし tokuten_fac3 &lt;- factanal(tokuten, factors = 2, rotation = &quot;promax&quot;, scores = &quot;regression&quot;) # promax回転（斜交回転) tokuten_fac4 &lt;- factanal(tokuten, factors = 2, scores = &quot;regression&quot;) # varimax回転（直交回転) 因子負荷量 (factor loadings) # 因子負荷量 (factor loadings) tokuten_fac2$loading #&gt; #&gt; Loadings: #&gt; Factor1 Factor2 #&gt; 専門性 0.722 -0.408 #&gt; 分析力 0.825 -0.442 #&gt; リーダーシップ 0.660 0.367 #&gt; プレゼン力 0.582 0.568 #&gt; コミュ力 0.813 0.434 #&gt; #&gt; Factor1 Factor2 #&gt; SS loadings 2.638 1.007 #&gt; Proportion Var 0.528 0.201 #&gt; Cumulative Var 0.528 0.729 tokuten_fac3$loading #&gt; #&gt; Loadings: #&gt; Factor1 Factor2 #&gt; 専門性 0.830 #&gt; 分析力 0.926 #&gt; リーダーシップ 0.717 #&gt; プレゼン力 0.869 -0.154 #&gt; コミュ力 0.866 0.114 #&gt; #&gt; Factor1 Factor2 #&gt; SS loadings 2.019 1.589 #&gt; Proportion Var 0.404 0.318 #&gt; Cumulative Var 0.404 0.722 tokuten_fac4$loading #&gt; #&gt; Loadings: #&gt; Factor1 Factor2 #&gt; 専門性 0.180 0.810 #&gt; 分析力 0.223 0.909 #&gt; リーダーシップ 0.714 0.246 #&gt; プレゼン力 0.811 #&gt; コミュ力 0.866 0.315 #&gt; #&gt; Factor1 Factor2 #&gt; SS loadings 2.001 1.644 #&gt; Proportion Var 0.400 0.329 #&gt; Cumulative Var 0.400 0.729 # tokuten_fac$loading 自主課題: 回転の掛け方の違いによる, 因子負荷量の違いを比較してみよう. 上の3つの方法の中では, どれが最も望ましいと言えるか. バイプロット # 因子得点と因子負荷量のbiplot # biplot(tokuten_fac2$scores, tokuten_fac2$loading) # biplot(tokuten_fac3$scores, tokuten_fac3$loading) # biplot(tokuten_fac4$scores, tokuten_fac4$loading) biplot(tokuten_fac2$scores, tokuten_fac2$loading, family = &quot;HiraKakuProN-W3&quot;, main = &quot;無回転&quot;) # 日本語文字化け対応 (mac) biplot(tokuten_fac3$scores, tokuten_fac3$loading, family = &quot;HiraKakuProN-W3&quot;, main = &quot;promax回転&quot;) # 日本語文字化け対応 (mac) biplot(tokuten_fac4$scores, tokuten_fac4$loading, family = &quot;HiraKakuProN-W3&quot;, main = &quot;varimax回転&quot;) # 日本語文字化け対応 (mac) 独自因子 # 独自因子（共通因子で説明出来なかった変動の割合) tokuten_fac$uniquenesses # specific variance #&gt; 専門性 分析力 リーダーシップ プレゼン力 コミュ力 #&gt; 0.3122369 0.1238380 0.4294971 0.3395409 0.1502496 因子得点 (factor scores) # 因子得点(factor scores) tokuten_fac2$scores #&gt; Factor1 Factor2 #&gt; 金 -0.35345980 -0.608456362 #&gt; 加藤 0.17945003 0.124563978 #&gt; 伊藤 1.45174808 0.575307941 #&gt; 山田 -0.42356834 0.498259301 #&gt; 鈴木 0.55483188 0.312476238 #&gt; 馬 1.26102603 0.583836680 #&gt; 王 1.54109037 0.083053116 #&gt; 劉 -0.69689211 -1.114352717 #&gt; 趙 -0.25800514 -0.210834640 #&gt; 呉 -0.70810831 -0.750978809 #&gt; 尹 1.04536346 -0.254942740 #&gt; 佐藤 0.09700760 -0.475101973 #&gt; ルイス 0.28088421 -1.810422048 #&gt; 徐 0.14982464 2.469057128 #&gt; スミス -1.21124653 0.675513625 #&gt; 木村 2.25572532 -0.242759239 #&gt; 大野 0.31595701 -0.075091179 #&gt; ロペス -2.05457156 0.093522072 #&gt; 陳 0.60199657 0.605075906 #&gt; 李 -0.54192120 -0.949786482 #&gt; 張 -0.51872657 0.831038283 #&gt; 黄 0.22507377 -0.077584334 #&gt; 朴 -1.49527174 0.009933262 #&gt; 池田 -0.34529174 1.359825911 #&gt; 藤田 -0.71155444 -1.333684989 #&gt; 周 -1.40767534 0.451195043 #&gt; ハリス 0.58928507 -1.976037558 #&gt; 田中 0.01337208 0.250000173 #&gt; 朱 -0.61845621 0.768688699 #&gt; 楊 0.78211294 0.188685714 tokuten_fac3$scores #&gt; Factor1 Factor2 #&gt; 金 -0.783798224 0.335754006 #&gt; 加藤 0.218967961 -0.001191321 #&gt; 伊藤 1.351662948 0.387409265 #&gt; 山田 0.252293375 -0.724666432 #&gt; 鈴木 0.606480686 0.063030681 #&gt; 馬 1.255742444 0.259283499 #&gt; 王 0.922595728 0.895755121 #&gt; 劉 -1.462553130 0.583660747 #&gt; 趙 -0.345637420 0.030858914 #&gt; 呉 -1.115919132 0.242931675 #&gt; 尹 0.323632449 0.893436307 #&gt; 佐藤 -0.408227394 0.497430641 #&gt; ルイス -1.604090543 1.839515824 #&gt; 徐 2.478806745 -2.172621111 #&gt; スミス -0.005973431 -1.384237779 #&gt; 木村 0.996736144 1.645663657 #&gt; 大野 0.099723541 0.268234009 #&gt; ロペス -1.031715491 -1.381757963 #&gt; 陳 0.916303486 -0.175889845 #&gt; 李 -1.218125203 0.530299056 #&gt; 張 0.523364581 -1.090247794 #&gt; 黄 0.047649527 0.213200254 #&gt; 朴 -0.807292119 -0.952237133 #&gt; 池田 1.131464854 -1.466396505 #&gt; 藤田 -1.683490718 0.775806481 #&gt; 周 -0.331059046 -1.302159553 #&gt; ハリス -1.596375924 2.186104544 #&gt; 田中 0.250004937 -0.221119206 #&gt; 朱 0.408348994 -1.095900308 #&gt; 楊 0.610479374 0.320050269 tokuten_fac4$scores #&gt; Factor1 Factor2 #&gt; 金 -0.68882679 0.14377283 #&gt; 加藤 0.21259422 0.05022145 #&gt; 伊藤 1.39824818 0.69530836 #&gt; 山田 0.08750758 -0.64808556 #&gt; 鈴木 0.60326556 0.20383935 #&gt; 馬 1.27711654 0.54774562 #&gt; 王 1.09181566 1.09077766 #&gt; 劉 -1.29466551 0.22645486 #&gt; 趙 -0.32926874 -0.05099018 #&gt; 呉 -1.03187860 -0.02476107 #&gt; 尹 0.50907297 0.94795848 #&gt; 佐藤 -0.28855058 0.38970620 #&gt; ルイス -1.15888407 1.41898261 #&gt; 徐 1.93666662 -1.53883492 #&gt; スミス -0.30711875 -1.35244773 #&gt; 木村 1.32712114 1.84010279 #&gt; 大野 0.15532636 0.28520385 #&gt; ロペス -1.30367841 -1.59073357 #&gt; 陳 0.85243022 0.04335245 #&gt; 李 -1.06867838 0.23173147 #&gt; 張 0.27143201 -0.94128983 #&gt; 黄 0.09272705 0.21926975 #&gt; 朴 -0.99202641 -1.11884756 #&gt; 池田 0.78067414 -1.16571900 #&gt; 藤田 -1.46760861 0.36214683 #&gt; 周 -0.60526047 -1.34862397 #&gt; ハリス -1.07594164 1.75907104 #&gt; 田中 0.19489202 -0.15714961 #&gt; 朱 0.15839769 -0.97379699 #&gt; 楊 0.66309899 0.45563437 10.2 パッケージpsychの利用 パッケージpsych内に収録されている関数fa()では, モデル推定方法や因子負荷行列の回転方法等に選択肢がある. - モデル推定方法 (引数fm=): 最尤法 (&quot;ml&quot;), 一般化最小2乗法 (&quot;gls&quot;), 重み付き最小2乗法 (&quot;gls&quot;), 最小残差法 (&quot;mires&quot;). デフォルトは&quot;mires&quot; - 回転方法 (引数rotate=): オブリミン (&quot;oblimin&quot;), バリマックス(&quot;varimax&quot;)など. 主因子法で因子負荷量行列を推定し, 回転の掛け方の違いによる結果の比較を行う. library(psych) # 主因子法 tokuten_fa &lt;- fa(r = tokuten, nfactors = 2 , rotate = &quot;none&quot;, fm = &quot;pa&quot;, scores = T) # 回転なし tokuten_fa2 &lt;- fa(r = tokuten, nfactors = 2 , rotate = &quot;oblimin&quot;, fm = &quot;pa&quot;, scores = T) # # tokuten_fa &lt;- fa(r = tokuten, nfactors = 2 , rotate = &quot;none&quot;, fm = &quot;ml&quot;, scores = T) # デフォルト: rotate = &quot;oblimin&quot;, fm = &quot;minres&quot; # 最尤法, fm = &quot;ml&quot;, 一般化最小2乗法, &quot;gls&quot;, 重み付き最小2乗法&quot;gls&quot;, 最小残差法&quot;mires&quot; # PCAとの比較 # tokuten_fa # standardized loadings (pattern matrix)表示 無回転のケース summary(tokuten_fa) #&gt; #&gt; Factor analysis with Call: fa(r = tokuten, nfactors = 2, rotate = &quot;none&quot;, scores = T, fm = &quot;pa&quot;) #&gt; #&gt; Test of the hypothesis that 2 factors are sufficient. #&gt; The degrees of freedom for the model is 1 and the objective function was 0.05 #&gt; The number of observations was 30 with Chi Square = 1.17 with prob &lt; 0.28 #&gt; #&gt; The root mean square of the residuals (RMSA) is 0.02 #&gt; The df corrected root mean square of the residuals is 0.06 #&gt; #&gt; Tucker Lewis Index of factoring reliability = 0.971 #&gt; RMSEA index = 0.067 and the 10 % confidence intervals are 0 0.506 #&gt; BIC = -2.23 tokuten_fa$loadings #&gt; #&gt; Loadings: #&gt; PA1 PA2 #&gt; 専門性 0.653 0.559 #&gt; 分析力 0.733 0.531 #&gt; リーダーシップ 0.702 -0.277 #&gt; プレゼン力 0.664 -0.459 #&gt; コミュ力 0.878 -0.291 #&gt; #&gt; PA1 PA2 #&gt; SS loadings 2.669 0.967 #&gt; Proportion Var 0.534 0.193 #&gt; Cumulative Var 0.534 0.727 tokuten_fa$scores #&gt; PA1 PA2 #&gt; 金 -0.50097984 0.42995050 #&gt; 加藤 0.20061318 -0.15893523 #&gt; 伊藤 1.51349352 -0.39976949 #&gt; 山田 -0.34154665 -0.58422408 #&gt; 鈴木 0.54024756 -0.47035034 #&gt; 馬 1.35988787 -0.29538763 #&gt; 王 1.51778205 0.02683852 #&gt; 劉 -0.90563580 0.79837065 #&gt; 趙 -0.26181220 0.23944774 #&gt; 呉 -0.78988691 0.88028830 #&gt; 尹 0.96526732 0.39864415 #&gt; 佐藤 -0.02613999 0.23031360 #&gt; ルイス -0.01428384 1.86567555 #&gt; 徐 0.53763371 -2.38807728 #&gt; スミス -1.06973521 -0.80247967 #&gt; 木村 2.21278411 0.68791627 #&gt; 大野 0.32716812 0.20696590 #&gt; ロペス -2.00637270 -0.38991235 #&gt; 陳 0.66251396 -0.53105722 #&gt; 李 -0.71971022 0.75052896 #&gt; 張 -0.33955651 -0.83954685 #&gt; 黄 0.22505320 0.11009970 #&gt; 朴 -1.44406138 -0.12219658 #&gt; 池田 -0.09426028 -1.30113136 #&gt; 藤田 -0.93230480 1.09856826 #&gt; 周 -1.30063403 -0.69601446 #&gt; ハリス 0.25974096 2.09973398 #&gt; 田中 0.04686464 -0.19837776 #&gt; 朱 -0.47461910 -0.80750067 #&gt; 楊 0.85248925 0.16161887 oblimin回転のケース summary(tokuten_fa2) #&gt; #&gt; Factor analysis with Call: fa(r = tokuten, nfactors = 2, rotate = &quot;oblimin&quot;, scores = T, #&gt; fm = &quot;pa&quot;) #&gt; #&gt; Test of the hypothesis that 2 factors are sufficient. #&gt; The degrees of freedom for the model is 1 and the objective function was 0.05 #&gt; The number of observations was 30 with Chi Square = 1.17 with prob &lt; 0.28 #&gt; #&gt; The root mean square of the residuals (RMSA) is 0.02 #&gt; The df corrected root mean square of the residuals is 0.06 #&gt; #&gt; Tucker Lewis Index of factoring reliability = 0.971 #&gt; RMSEA index = 0.067 and the 10 % confidence intervals are 0 0.506 #&gt; BIC = -2.23 #&gt; With factor correlations of #&gt; PA1 PA2 #&gt; PA1 1.00 0.43 #&gt; PA2 0.43 1.00 tokuten_fa2$loadings #&gt; #&gt; Loadings: #&gt; PA1 PA2 #&gt; 専門性 0.872 #&gt; 分析力 0.883 #&gt; リーダーシップ 0.728 #&gt; プレゼン力 0.859 -0.143 #&gt; コミュ力 0.862 0.127 #&gt; #&gt; PA1 PA2 #&gt; SS loadings 2.015 1.580 #&gt; Proportion Var 0.403 0.316 #&gt; Cumulative Var 0.403 0.719 tokuten_fa2$scores #&gt; PA1 PA2 #&gt; 金 -0.63719993 -0.12126216 #&gt; 加藤 0.24946885 0.05684763 #&gt; 伊藤 1.53828937 0.92948821 #&gt; 山田 -0.05705539 -0.63216871 #&gt; 鈴木 0.69002640 0.12657088 #&gt; 馬 1.35472194 0.87511839 #&gt; 王 1.35867014 1.20002358 #&gt; 劉 -1.16097601 -0.20597088 #&gt; 趙 -0.33934743 -0.05413001 #&gt; 呉 -1.09171502 -0.06443027 #&gt; 尹 0.69995489 1.00216931 #&gt; 佐藤 -0.12266000 0.12387160 #&gt; ルイス -0.81535148 1.15737049 #&gt; 徐 1.51251121 -1.07657321 #&gt; スミス -0.62056974 -1.33653847 #&gt; 木村 1.70175981 2.15586958 #&gt; 大野 0.20633950 0.38467617 #&gt; ロペス -1.64359318 -1.80831273 #&gt; 陳 0.82651628 0.18386404 #&gt; 李 -0.97254981 -0.09099336 #&gt; 張 0.05455960 -0.79053044 #&gt; 黄 0.15581651 0.24440165 #&gt; 朴 -1.25110204 -1.20227775 #&gt; 池田 0.47454136 -0.88840395 #&gt; 藤田 -1.31417180 -0.03874208 #&gt; 周 -0.87481150 -1.44985872 #&gt; ハリス -0.66864130 1.51758637 #&gt; 田中 0.12763357 -0.08771334 #&gt; 朱 -0.08115492 -0.87574980 #&gt; 楊 0.70009011 0.76579801 par(mfrow = c(1, 2)) # plot(tokuten_fa) # plot(tokuten_fa2) biplot(tokuten_fa, main = &quot;無回転&quot;) biplot(tokuten_fa2, main = &quot;oblimin回転&quot;) par(mfrow = c(1, 1)) 自主課題: 因子数や回転の掛け方を色々と変えてみて, 最も望ましい組合せをみつけてみよう. 10.3 因子分析に有用なツール 因子数の決定 パッケージpsychの関数fa.parallel(), vss()は, 因子数の決定に有用である. 平行分析 (parallel analysis) # library(psych) # 平行分析(parallel analysis) # デフォルト: fm = &quot;minres&quot; (res_parallel &lt;- fa.parallel(tokuten)) # minres法(デフォルト), PCA &amp; 因子分析 # fa.parallel(tokuten, fm = &quot;wls&quot;) # fa.parallel(tokuten, fm = &quot;ml&quot;, fa = &quot;fa&quot;) # 最尤法+因子分析(のみ)実行 # サンプルデータから作られるscreeプロットと, シミュレーションデータの行列(サンプルと同じサイズ)のscreeプロットとを比較 # → 因子数をsuggest # fa = &quot;both&quot; (デフォルト): # PCA, 主因子法の固有値を同時に表示 #&gt; Parallel analysis suggests that the number of factors = 2 and the number of components = 1 #&gt; Call: fa.parallel(x = tokuten) #&gt; Parallel analysis suggests that the number of factors = 2 and the number of components = 1 #&gt; #&gt; Eigen Values of #&gt; Original factors Resampled data Simulated data Original components #&gt; 1 2.48 1.15 1.12 2.93 #&gt; 2 0.63 0.28 0.30 1.23 #&gt; Resampled components Simulated components #&gt; 1 1.58 1.52 #&gt; 2 1.21 1.23 VSS (Very Simple Structure) 基準 # VSS (Very Simple Structure) 基準 # Very Simple Structure criterion ( VSS) for estimating the optimal number of factors # 最大となる因子数を探す # (tokuten_vss &lt;- vss(tokuten, n = 5, rotate = &quot;oblimin&quot;, fm = &quot;wls&quot;) ) # n: Number of factors to extract (&gt; (初期仮説の)因子数) # デフォルト: # 因子数 n = 8, 回転 rotate = &quot;varimax&quot; # → VSS, MAP, その他の因子数決定基準の数値を表示 tmp_vss &lt;- vss(tokuten, n = 4, fm = &quot;ml&quot;) # VSS.plot(tmp_vss) print(tmp_vss) #&gt; #&gt; Very Simple Structure #&gt; Call: vss(x = tokuten, n = 4, fm = &quot;ml&quot;) #&gt; VSS complexity 1 achieves a maximimum of 0.81 with 3 factors #&gt; VSS complexity 2 achieves a maximimum of 0.96 with 2 factors #&gt; #&gt; The Velicer MAP achieves a minimum of 0.19 with 2 factors #&gt; BIC achieves a minimum of -2.27 with 2 factors #&gt; Sample Size adjusted BIC achieves a minimum of 0.84 with 2 factors #&gt; #&gt; Statistics by number of factors #&gt; vss1 vss2 map dof chisq prob sqresid fit RMSEA BIC SABIC complex #&gt; 1 0.78 0.00 0.22 5 2.2e+01 0.00057 2.31 0.78 0.333 4.8 20.36 1.0 #&gt; 2 0.80 0.96 0.19 1 1.1e+00 0.28840 0.44 0.96 0.056 -2.3 0.84 1.1 #&gt; 3 0.81 0.92 0.38 -2 1.2e-08 NA 0.31 0.97 NA NA NA 1.2 #&gt; 4 0.49 0.83 1.00 -4 3.0e-13 NA 0.23 0.98 NA NA NA 1.8 #&gt; eChisq SRMR eCRMS eBIC #&gt; 1 2.1e+01 1.9e-01 0.265 4.0 #&gt; 2 2.7e-01 2.1e-02 0.067 -3.1 #&gt; 3 2.6e-09 2.1e-06 NA NA #&gt; 4 1.7e-13 1.7e-08 NA NA 因子間の関係性の探索 パッケージpsych内の関数fa.plot()やfa.diagram()は, 因子間の関係性 (相関構造や階層構造) を調べるのに有用である. fa.plot(tokuten_fa, cut = 0.5) # fa.plot(tokuten_fa2, cut = 0.5) fa.diagram(tokuten_fa) # fa.diagram(tokuten_fa2) また, パッケージqgraphの関数qgraph()も同様な目的で有用である. # 相関ネットワーク(参考) library(qgraph) qgraph(cor(tokuten), edge.labels = T, minimum = .2, edge.color = &quot;black&quot;) 確証的因子分析 # 確証的因子分析 # library(lavaan) # cfa()関数 "],["クラスター分析.html", "11 クラスター分析 11.1 基本操作 11.2 データ分析例 11.3 発展的なクラスター分析", " 11 クラスター分析 11.1 基本操作 クラスター分析の基本操作を学ぶため, 主成分分析の章でも使用した, 従業員スキル評価データ \\((n=9,p=5)\\) をここでも使用する. tokuten &lt;- read.csv(&quot;testdat_eng.csv&quot;, skip = 1, header = T, row.names = 1) # tokuten &lt;- read.csv(&quot;testdat_jap.csv&quot;, skip = 1, header = T, row.names = 1) 階層型クラスタリング 階層型クラスタリングは, Rの標準パッケージの一つstatsに含まれる 関数hclust()を用いて実行することができる. hclust()への入力として, クラスタリング対象間の距離行列を与える必要がある. もし入力データセット (tokuten) の個体間 (行) のクラスタリングを行うのであれば そのまま関数dist()を適用すれば良い. 一方, 変数間 (列) のクラスタリングであれば, 一旦データセットを転置 (行と列の入れ替え) する必要がある. tokuten_dist &lt;- dist(tokuten) # method = &quot;binary&quot;, &quot;canberra&quot;, &quot;maximum&quot;, &quot;manhattan&quot; # 距離行列 round(tokuten_dist, 2) #&gt; yamada suzuki tanaka nakamura ohno Matsui takagi miura #&gt; suzuki 9.54 #&gt; tanaka 25.96 27.55 #&gt; nakamura 31.91 35.03 9.27 #&gt; ohno 38.65 41.00 28.21 26.19 #&gt; Matsui 27.50 29.98 11.22 10.10 20.00 #&gt; takagi 18.52 21.73 13.82 18.14 23.13 12.04 #&gt; miura 30.66 32.70 12.57 13.93 18.92 13.04 14.39 #&gt; sato 19.72 26.34 17.18 18.30 30.90 16.76 12.41 21.66 # 距離の近い人を集めて、クラスターを形成 # 1:C1 = {tanaka, nakamura} # 2:C2 = {yamada, suzuki}, or C2 = {C1, matsui} or else?? # --&gt; どちらが優先される? hclust()のデフォルト設定では, 距離尺度はユークリッド距離, クラスター結合方法は最遠隣法/完全連結法となっている. # 最遠隣法/完全連結法 (tokuten_hc_1 &lt;- hclust(tokuten_dist)) # method = &quot;complete&quot; #&gt; #&gt; Call: #&gt; hclust(d = tokuten_dist) #&gt; #&gt; Cluster method : complete #&gt; Distance : euclidean #&gt; Number of objects: 9 #summary(tokuten_hc_1) 出力オブジェクトの要素$mergeにクラスター形成過程が格納されている. # クラスター形成履歴 # マイナス付き =&gt; 個体番号. マイナス無し =&gt; クラスター番号 tokuten_hc_1$merge #&gt; [,1] [,2] #&gt; [1,] -3 -4 #&gt; [2,] -1 -2 #&gt; [3,] -6 1 #&gt; [4,] -7 -9 #&gt; [5,] -8 3 #&gt; [6,] 4 5 #&gt; [7,] -5 6 #&gt; [8,] 2 7 また, 出力オブジェクトの要素$heightには, クラスター間の結合が行われる際の距離が格納されている. # デンドログラム (樹形図) の枝の高さ tokuten_hc_1$height #&gt; [1] 9.273618 9.539392 11.224972 12.409674 13.928388 21.656408 30.903074 #&gt; [8] 41.000000 このクラスター形成過程と結合距離を使うことでデンドログラムが作成される. デンドログラムは階層的クラスタリングの結果 (クラスターの形成過程) を視覚化するための有効なツールである. # デンドログラム par(mfrow = c(1, 2)) plot(tokuten_hc_1) plot(tokuten_hc_1, hang = -1) # 葉の位置(高さ)を揃える 階層クラスタリングを実施しデンドログラムを作成したあと, cutreeにクラスター数を指定することで, 木を剪定し (細かい枝を切り落とし) て望ましい数のクラスターに絞り込んで, 個体がそれぞれ所属するクラスターに分類することができる. # 各個体の属するクラスター番号 cutree(tokuten_hc_1, k = 3) # k: クラスター数 #&gt; yamada suzuki tanaka nakamura ohno Matsui takagi miura #&gt; 1 1 2 2 3 2 2 2 #&gt; sato #&gt; 2 次に, クラスター結合方法の相違によるデンドログラムの形状の違いをみてみよう. # 代替的手法 (tokuten_hc_2 &lt;- hclust(tokuten_dist, method = &quot;single&quot;)) # 最近隣法 #&gt; #&gt; Call: #&gt; hclust(d = tokuten_dist, method = &quot;single&quot;) #&gt; #&gt; Cluster method : single #&gt; Distance : euclidean #&gt; Number of objects: 9 (tokuten_hc_3 &lt;- hclust(tokuten_dist, method = &quot;ward.D2&quot;)) # ウォード法 #&gt; #&gt; Call: #&gt; hclust(d = tokuten_dist, method = &quot;ward.D2&quot;) #&gt; #&gt; Cluster method : ward.D2 #&gt; Distance : euclidean #&gt; Number of objects: 9 (tokuten_hc_4 &lt;- hclust(tokuten_dist, method = &quot;average&quot;)) # 群平均法 #&gt; #&gt; Call: #&gt; hclust(d = tokuten_dist, method = &quot;average&quot;) #&gt; #&gt; Cluster method : average #&gt; Distance : euclidean #&gt; Number of objects: 9 (tokuten_hc_5 &lt;- hclust(tokuten_dist, method = &quot;centroid&quot;)) # 中心点法/重心法 #&gt; #&gt; Call: #&gt; hclust(d = tokuten_dist, method = &quot;centroid&quot;) #&gt; #&gt; Cluster method : centroid #&gt; Distance : euclidean #&gt; Number of objects: 9 (tokuten_hc_6 &lt;- hclust(tokuten_dist, method = &quot;median&quot;)) # メジアン法 #&gt; #&gt; Call: #&gt; hclust(d = tokuten_dist, method = &quot;median&quot;) #&gt; #&gt; Cluster method : median #&gt; Distance : euclidean #&gt; Number of objects: 9 par(mfrow = c(2, 2)) plot(tokuten_hc_1); plot(tokuten_hc_2); plot(tokuten_hc_3) plot(tokuten_hc_4); par(mfrow = c(1, 2)) plot(tokuten_hc_5); plot(tokuten_hc_6) 距離尺度として中心点法 (重心法) やメジアン法を選択した場合には, 「inversion (逆転) 現象」 の発生が確認される. 「inversion現象」とは、階層的クラスタリングのプロセスにおいて, 本来ならばステップが進むにつれてより”遠く”にあるクラスターと結合していくべきところ, 実際にはより”近く”のクラスターが後に統合されるという状況を指す. すなわち, 結合の順序が距離に関する単調性を失い, 距離の近い順に結合するという直感的な (本来あるべき) 順序に反する状態であり, クラスタリングがうまくいっていないことを示す. この現象が生じた場合には, 異なる距離尺度や統合方法を検討する必要がある. k-means法 非階層型クラスタリングの主要な方法であるk-means法は, Rの標準パッケージの一つstatsに含まれる 関数kmeans()を用いて実行することができる. # k-means法 (tokuten_hc_k &lt;- kmeans(tokuten, 3)) #&gt; K-means clustering with 3 clusters of sizes 2, 1, 6 #&gt; #&gt; Cluster means: #&gt; Expertise Analytics Leadership Presentation Communication #&gt; 1 82.50000 85.00000 67.50000 66.50000 65.0 #&gt; 2 88.00000 85.00000 85.00000 90.00000 91.0 #&gt; 3 75.33333 75.66667 80.16667 78.66667 79.5 #&gt; #&gt; Clustering vector: #&gt; yamada suzuki tanaka nakamura ohno Matsui takagi miura #&gt; 1 1 3 3 2 3 3 3 #&gt; sato #&gt; 3 #&gt; #&gt; Within cluster sum of squares by cluster: #&gt; [1] 45.5000 0.0000 540.3333 #&gt; (between_SS / total_SS = 72.9 %) #&gt; #&gt; Available components: #&gt; #&gt; [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; #&gt; [6] &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; tokuten_hc_k$cluster #&gt; yamada suzuki tanaka nakamura ohno Matsui takagi miura #&gt; 1 1 3 3 2 3 3 3 #&gt; sato #&gt; 3 (tokuten_hc_k &lt;- kmeans(tokuten, 2)) #&gt; K-means clustering with 2 clusters of sizes 7, 2 #&gt; #&gt; Cluster means: #&gt; Expertise Analytics Leadership Presentation Communication #&gt; 1 77.14286 77 80.85714 80.28571 81.14286 #&gt; 2 82.50000 85 67.50000 66.50000 65.00000 #&gt; #&gt; Clustering vector: #&gt; yamada suzuki tanaka nakamura ohno Matsui takagi miura #&gt; 2 2 1 1 1 1 1 1 #&gt; sato #&gt; 1 #&gt; #&gt; Within cluster sum of squares by cluster: #&gt; [1] 996.0 45.5 #&gt; (between_SS / total_SS = 51.9 %) #&gt; #&gt; Available components: #&gt; #&gt; [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; #&gt; [6] &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; tokuten_hc_k$cluster #&gt; yamada suzuki tanaka nakamura ohno Matsui takagi miura #&gt; 2 2 1 1 1 1 1 1 #&gt; sato #&gt; 1 変数に対するクラスタリング tokuten_dist3 &lt;- dist(t(tokuten)) tokuten_hc3 &lt;- hclust(tokuten_dist3, method = &quot;ward.D2&quot;) tokuten_hc3$merge # クラスター形成履歴 #&gt; [,1] [,2] #&gt; [1,] -3 -5 #&gt; [2,] -4 1 #&gt; [3,] -1 -2 #&gt; [4,] 2 3 tokuten_hc3$height # デンドログラム(樹形図)の枝の高さ #&gt; [1] 12.20656 14.61734 19.74842 44.74967 par(mfrow = c(1, 2)); plot(tokuten_hc3); plot(tokuten_hc3, hang = -1) # デンドログラム cutree(tokuten_hc3, k = 3) # 各個体の属するクラスター番号 (k: クラスター数) #&gt; Expertise Analytics Leadership Presentation Communication #&gt; 1 2 3 3 3 par(mfrow = c(1, 1)) 11.2 データ分析例 データセット (1): ワイン品質データ (再掲) 回帰木で使用したワイン品質データを使用する. ここでは, 回帰問題において目的変数であったワイン品質quality は除き, ワインの化学的特性を表す11の変数のみを用いて, 白ワインのデータセットをクラスター分類することを試みる. - winequality-white.csv - fixed acidity: 酢酸濃度 - volitle acidity: 揮発酸濃度 - citric acidity: クエン酸濃度 - chlorides: 塩化物 - sulfur dioxide: 二酸化硫黄 - sulphate: 硫酸塩 - fixed acidity: 酒石酸含有量（g/dm3) - volatile acidity: 酢酸含有量（g/dm3) - citric acid: クエン酸含有量（g/dm3) - residual sugar: 残留糖分含有量（g/dm3） - chlorides: 塩化ナトリウム含有量（g/dm3) - free sulfur dioxide: 遊離亜硫酸含有量（mg/dm3） - total sulfur dioxide: 総亜硫酸含有量（mg/dm3） - density: 密度（g/dm3) - pH: pH - sulphates: 硫酸カリウム含有量（g/dm3） - alcohol: アルコール度数（% vol.） - quality: ワインの品質 (0 (very bad) -- 10 (excellent)) winedat &lt;- read.csv(&quot;winequality-white.csv&quot;, sep = &quot;;&quot;, skip = 1, header = T) wine &lt;- winedat[, -12] # qualityを除く wine_s &lt;- scale(wine) # 標準化 レコードのクラスタリング 個別のワイン (行) を, 11個の化学的特性 (列) に関する類似性によりクラスタリングする. #wine_dist &lt;- dist(wine) wine_dist &lt;- dist(wine_s) # method = &quot;binary&quot;, &quot;canberra&quot;, &quot;maximum&quot;, &quot;manhattan&quot; (wine_hc &lt;- hclust(wine_dist, method = &quot;ward.D2&quot;)) # ウォード法 #&gt; #&gt; Call: #&gt; hclust(d = wine_dist, method = &quot;ward.D2&quot;) #&gt; #&gt; Cluster method : ward.D2 #&gt; Distance : euclidean #&gt; Number of objects: 4898 plot(wine_hc, labels = F, main = &quot;wineデータ (標準化後): レコードのクラスタリング\\nEuclidean + Ward&quot;, family = &quot;HiraKakuProN-W3&quot;); #plot(wine_hc, labels=F, main=&quot;wineデータ (標準化前): レコードのクラスタリング\\nEuclidean + Ward&quot;) # 指定したクラスター数kで, デンドログラムを切断 rect.hclust(wine_hc, k = 5, border = &quot;red&quot;) rect.hclust(wine_hc, k = 9, border = &quot;blue&quot;) 変数のクラスタリング 11個の化学的特性 (列) を, ワインに関する類似性によりクラスタリングする. #wine_dist &lt;- dist(t(wine)) wine_dist &lt;- dist(t(wine_s)) (wine_hc &lt;- hclust(wine_dist, method = &quot;ward.D2&quot;)) # ウォード法 #&gt; #&gt; Call: #&gt; hclust(d = wine_dist, method = &quot;ward.D2&quot;) #&gt; #&gt; Cluster method : ward.D2 #&gt; Distance : euclidean #&gt; Number of objects: 11 plot(wine_hc, main = &quot;wineデータ (標準化後): 変数のクラスタリング\\nEuclidean + Ward&quot;, family = &quot;HiraKakuProN-W3&quot;) #plot(wine_hc, main=&quot;wineデータ (標準化前): 変数のクラスタリング\\nEuclidean + Ward&quot;) クラスター間距離による結果の相違 (wine_hc &lt;- hclust(wine_dist, method = &quot;complete&quot;)) #&gt; #&gt; Call: #&gt; hclust(d = wine_dist, method = &quot;complete&quot;) #&gt; #&gt; Cluster method : complete #&gt; Distance : euclidean #&gt; Number of objects: 11 plot(wine_hc, main = &quot;wineデータ (標準化後): レコードのクラスタリング\\nEuclidean + complete&quot;, family = &quot;HiraKakuProN-W3&quot;) (wine_hc &lt;- hclust(wine_dist, method = &quot;single&quot;)) #&gt; #&gt; Call: #&gt; hclust(d = wine_dist, method = &quot;single&quot;) #&gt; #&gt; Cluster method : single #&gt; Distance : euclidean #&gt; Number of objects: 11 plot(wine_hc, main = &quot;wineデータ (標準化後): レコードのクラスタリング\\nEuclidean + single&quot;, family = &quot;HiraKakuProN-W3&quot;) (wine_hc &lt;- hclust(wine_dist, method = &quot;average&quot;)) #&gt; #&gt; Call: #&gt; hclust(d = wine_dist, method = &quot;average&quot;) #&gt; #&gt; Cluster method : average #&gt; Distance : euclidean #&gt; Number of objects: 11 plot(wine_hc, main = &quot;wineデータ (標準化後): レコードのクラスタリング\\nEuclidean + average&quot;, family = &quot;HiraKakuProN-W3&quot;) (wine_hc &lt;- hclust(wine_dist, method = &quot;centroid&quot;)) #&gt; #&gt; Call: #&gt; hclust(d = wine_dist, method = &quot;centroid&quot;) #&gt; #&gt; Cluster method : centroid #&gt; Distance : euclidean #&gt; Number of objects: 11 plot(wine_hc, main = &quot;wineデータ (標準化後): レコードのクラスタリング\\nEuclidean + centroid&quot;, family = &quot;HiraKakuProN-W3&quot;) 重心法 (中心点法)において, inversion現象の発生が確認される. データセット (2): (ID無し) POSデータ 地方のサービスエリアの売店のレシートデータ500件 (実際のデータに加工を加えた上でランダムにサブセット化. オリジナルの地域・店舗が特定されないように工夫). &quot;pos_sample500_dist.csv&quot; - n = 500, p = 16 - 行: レシート (顧客の一回当り購入バスケット) - 列: &quot;洋菓子土産&quot;〜&quot;デザート類&quot;までの16分類の商品群 - 値: 対応するレシートに含まれる対応する商品の売上個数 (0,1,2,...) データ読み込み &amp; 距離行列の計算 # posデータの読み込み posdat &lt;- read.csv(file(&quot;pos_sample500_dist.csv&quot;, encoding = &#39;Shift_JIS&#39;)) # 500件 階層クラスタリング 16個の商品分類 (列) を, それらの買われ方 (レシートへの反映のされ方) の類似性によって, クラスタリングする. # 商品分類間の距離行列 pos_dist &lt;- dist(t(posdat)) # カテゴリー間の距離 #pos_dist &lt;- dist(posdat) # 顧客間の距離 pos_hc &lt;- hclust(pos_dist, method = &quot;ward.D2&quot;) pos_hc$merge # クラスター形成履歴 #&gt; [,1] [,2] #&gt; [1,] -9 -10 #&gt; [2,] -6 -13 #&gt; [3,] -7 1 #&gt; [4,] 2 3 #&gt; [5,] -8 4 #&gt; [6,] -3 5 #&gt; [7,] -16 6 #&gt; [8,] -11 7 #&gt; [9,] -5 8 #&gt; [10,] -12 9 #&gt; [11,] -14 10 #&gt; [12,] -4 11 #&gt; [13,] -15 12 #&gt; [14,] -1 13 #&gt; [15,] -2 14 pos_hc$height # デンドログラム(樹形図)の高さ #&gt; [1] 7.280110 8.062258 8.103497 8.644844 9.014803 9.549370 9.702724 #&gt; [8] 10.785793 12.548572 13.754999 15.099669 18.753461 30.374223 41.487118 #&gt; [15] 65.724108 plot(pos_hc, hang = -1, family = &quot;HiraKakuProN-W3&quot;) # 問題点? デンドログラム内に「chaining現象」が発生していることが分かる. 「chaining現象」は, クラスターが連鎖的に長く伸びる形で形成される現象を指す. この現象は, 階層クラスタリングの過程で, 一つのクラスターが次々に近くのデータ点を吸収して成長していくことで生する. chaining現象は, 特に, 単連結法で生じやすいと考えられる. 本来異なるクラスターに属すべきデータポイントが, 同一のクラスターに統合されてしまうことで, データの真の構造を見落としてしまう可能性がある. 対処法として, クラスター間の結合方法を変える (単連結法から他の方法) ことでchaining現象の影響を軽減できる場合がある. また, 距離尺度を変更することで軽減できる場合がある. このposデータの取る値は, 各レシートごとの商品群の購買個数を表している. 同時購買されやすい・されにくいが商品群間の距離に反映されてしまっている. ここでは, データに加工を施し, 値が購買個数 (頻度) → 購買有無 (0/1)に変換してみる. posdat2 &lt;- posdat posdat2[posdat2&gt;= 2] &lt;- 1 pos_dist2 &lt;- dist(t(posdat2)) # pos_hc2 &lt;- hclust(pos_dist2, method = &quot;ward.D2&quot;) pos_hc2$merge # クラスター形成履歴 #&gt; [,1] [,2] #&gt; [1,] -7 -9 #&gt; [2,] -10 1 #&gt; [3,] -16 2 #&gt; [4,] -3 -13 #&gt; [5,] 3 4 #&gt; [6,] -6 5 #&gt; [7,] -5 6 #&gt; [8,] -11 -14 #&gt; [9,] -8 7 #&gt; [10,] 8 9 #&gt; [11,] -12 10 #&gt; [12,] -4 11 #&gt; [13,] -1 -2 #&gt; [14,] -15 12 #&gt; [15,] 13 14 pos_hc2$height # デンドログラム(樹形図)の高さ #&gt; [1] 4.472136 4.760952 4.983305 5.099020 5.369668 5.550633 6.989788 #&gt; [8] 7.348469 7.348469 8.011356 9.468448 9.536032 15.874508 16.016132 #&gt; [15] 20.502178 plot(pos_hc2, hang = -1, family = &quot;HiraKakuProN-W3&quot;) 更新されたデンドログラムより, chainingが緩和されたのが確認される. 次に, 距離尺度およびクラスター結合方法の変更を試みる. ここでは, 距離尺度としてコサイン距離を, クラスター結合方法としてウォード法を採用する. # マンハッタン距離 #pos_dist &lt;- dist(t(posdat), method = &quot;manhattan&quot;) # cosine距離の使用 library(proxy) pos_dist &lt;- proxy::dist(t(posdat), method = &quot;cosine&quot;) # cosine距離 pos_hc &lt;- hclust(pos_dist, method = &quot;ward.D2&quot;) plot(pos_hc, hang = -1, family = &quot;HiraKakuProN-W3&quot;) pos_dist2 &lt;- proxy::dist(t(posdat2), method = &quot;cosine&quot;) # cosine距離 pos_hc2 &lt;- hclust(pos_dist2, method = &quot;ward.D2&quot;) plot(pos_hc2, hang = -1, family = &quot;HiraKakuProN-W3&quot;) 以上, データ間の距離やクラスター結合方法の選択が結果に大きく影響することが確認される. Hartiganルールによるクラスター数Kの決定 Hartiganルールでは, (k+1)番目のクラスターを加えるか否かの判定を逐次行い, 最適なクラスター数\\(K\\)を決定する. library(useful) ## pos_km &lt;- FitKMeans(posdat, max.cluster = 20, seed = 1) # 客 pos_km_item &lt;- FitKMeans(t(posdat), seed = 1) # 商品 pos_km_item #&gt; Clusters Hartigan AddCluster #&gt; 1 2 14.359657 TRUE #&gt; 2 3 8.985054 FALSE #&gt; 3 4 7.062050 FALSE #&gt; 4 5 3.181427 FALSE #&gt; 5 6 2.307692 FALSE #&gt; 6 7 2.131698 FALSE #&gt; 7 8 1.964241 FALSE #&gt; 8 9 1.112276 FALSE #&gt; 9 10 1.706499 FALSE #&gt; 10 11 1.190476 FALSE #&gt; 11 12 1.142857 FALSE # (クラスター数, Hartigan数, クラスターを追加するべきか否か) PlotHartigan(pos_km_item) # 閾値=10(デフォルト) 上記出力より, クラスター数\\(K=2\\)が選択される. 次元縮約 → 顧客 (レシート) のクラスタリング つぎに, 顧客 (レシート) のクラスタリングを行う. 変数の次元が大きい場合には, クラスタリングの実行に先立って, 類似性を測る変数の次元を落とすのが効果的なことが多い. 本データセットの商品分類数 (16) は大きいとは言えないが, 手順例を紹介する. ここでは, PCAによる次元縮約によって 事前に5つに絞り込んでおく. (なお, “超高次元”の場合には, 通常のPCAはうまく機能しないことが 理論的に示されているため, 別途対応が必要となる.) # 変数(商品)に関して次元縮約pca実行 #posdat_pc &lt;- prcomp(posdat) posdat_pc &lt;- prcomp(posdat, scale. = T) # 標準化 summary(posdat_pc) #&gt; Importance of components: #&gt; PC1 PC2 PC3 PC4 PC5 PC6 PC7 #&gt; Standard deviation 1.26384 1.22037 1.10340 1.07873 1.03901 1.0245 0.99523 #&gt; Proportion of Variance 0.09983 0.09308 0.07609 0.07273 0.06747 0.0656 0.06191 #&gt; Cumulative Proportion 0.09983 0.19291 0.26901 0.34174 0.40921 0.4748 0.53672 #&gt; PC8 PC9 PC10 PC11 PC12 PC13 PC14 #&gt; Standard deviation 0.9911 0.9847 0.96511 0.93339 0.91726 0.89735 0.8772 #&gt; Proportion of Variance 0.0614 0.0606 0.05821 0.05445 0.05259 0.05033 0.0481 #&gt; Cumulative Proportion 0.5981 0.6587 0.71693 0.77138 0.82397 0.87430 0.9224 #&gt; PC15 PC16 #&gt; Standard deviation 0.8070 0.76844 #&gt; Proportion of Variance 0.0407 0.03691 #&gt; Cumulative Proportion 0.9631 1.00000 plot(posdat_pc, family = &quot;HiraKakuProN-W3&quot;) posdat_score5 &lt;- posdat_pc$x[, 1:5] posdat_pc$rotation[, 1:5] #&gt; PC1 PC2 PC3 PC4 PC5 #&gt; 洋菓子土産 0.19801045 -0.41680964 0.07830286 0.204893941 -0.15189427 #&gt; 和菓子土産 0.31926545 -0.17429488 0.29451622 0.024368667 0.06823826 #&gt; 地域限定菓子 0.11093983 -0.21464500 0.08853735 0.122379290 -0.51464240 #&gt; 水産加工品 0.38402139 0.47345139 0.12406842 -0.018704524 0.05301032 #&gt; 地元名産品 0.28312609 0.25920695 0.07566671 -0.432182990 -0.21885500 #&gt; 畜産加工品 0.31510689 0.36152933 0.04970947 0.404289871 -0.10659114 #&gt; 農産加工品 0.14914158 -0.03604771 0.16126513 -0.142280575 0.30221851 #&gt; ご当地グロッサリー 0.20348286 0.20049364 0.03892426 0.150258935 0.38320847 #&gt; 玩具土産 -0.18783969 0.06869904 0.48245433 0.205983425 -0.07184299 #&gt; 雑貨土産 -0.08771897 0.09893981 0.46577309 0.185847154 0.18455564 #&gt; 雑貨類 -0.13980819 0.13804035 -0.38617293 0.166604346 0.17361526 #&gt; 菓子類 -0.33688987 0.10484451 0.32900498 0.001601135 -0.21715666 #&gt; 麺類 0.21239584 0.14992291 -0.10107137 0.106430151 -0.49276377 #&gt; パン.弁当類 -0.14576690 0.29113765 -0.32482427 0.311169546 -0.14077896 #&gt; 飲料 -0.46157964 0.32927414 0.15896997 -0.041046373 -0.11399644 #&gt; デザート等 -0.02334359 0.16142448 0.01003826 -0.578340358 -0.12251255 posdat_dist5 &lt;- dist(posdat_score5) # レコード間 #posdat_dist5 &lt;- dist(t(posdat_score5)) # 合成変数間 posdat_hc5 &lt;- hclust(posdat_dist5, method = &quot;ward.D2&quot;) plot(posdat_hc5, family = &quot;HiraKakuProN-W3&quot;) rect.hclust(posdat_hc5, k = 8, border = &quot;blue&quot;) # pos_km5 &lt;- FitKMeans(posdat_score5, max.cluster = 20, seed = 1) # PlotHartigan(pos_km5) 上では, 顧客 (レシート) に対して階層クラスタリングを実施したが, 件数の多い場合には, 注意が必要である. 主な問題点は計算コストと可視化の困難さである. 階層的クラスタリングにおいて, データ点の数が非常に多い場合, 計算時間やメモリ消費が膨大になる可能性がある (全てのデータ点間で距離を計算し, これらの距離に基づいて段階的にクラスターを形成する必要性) . また, データポイントの数が多い場合、デンドログラムは非常に密集してしまい、個々のクラスターやクラスタリングの構造を解釈することが困難で, 有益な情報を得られにくくなる. 11.3 発展的なクラスター分析 先に使用した従業員評価データ (拡張版) をここでも使用する. 距離としては, 相関係数 (類似度) をベースにしたcosine距離 (=1-相関係数) を使用する. tokuten &lt;- read.csv(&quot;selfeval30_jp.csv&quot;, skip = 1, header = T, row.names = 1) 関数agnes パッケージcluster内にある関数agnes (AGglomerative NESting) を使うことで, 階層型クラスタリングをより高度に制御しながら実行することが可能. library(cluster) dist_matrix &lt;- as.dist(1 - cor(tokuten)) res_agnes &lt;- agnes(dist_matrix, method = &quot;average&quot;) plot(res_agnes) dist_matrix &lt;- as.dist(1 - cor(t(tokuten))) res_agnes &lt;- agnes(dist_matrix, method = &quot;average&quot;) plot(res_agnes) 関数pheatmap パッケージpheatmapは, 主にヒートマップの作成に使用されるが, 相関に基づく階層型クラスタリングを行い, ヒートマップとして可視化する機能もあり. 遺伝子発現データなどのbioinformatics分野で活用. library(pheatmap) pheatmap(tokuten, clustering_distance_rows = as.dist(1 - cor(t(tokuten))), clustering_distance_cols = as.dist(1 - cor(tokuten))) 関数pam メドイド (medoid) は, クラスタ内の全ての点に対する距離の合計が最小となるような, クラスタ内に存在するデータ点であり, K-medoids法は, メドイドを中心としてクラスタリングを行う手法である. PAM (Partitioning Around Medoids) は, K-medoids法の考え方を実装した具体的アルゴリズムの一種である. Rでは, パッケージcluster内の関数pamを使用することができる. 一方, K-medoids法に類似したクラスタリング手法として, K-median法がある. K-median法は, 各クラスタ内のデータポイントの中央値に基づいて「中央点」決定し, クラスタ内の全データポイントと「中央点」との距離の合計を最小化するようにクラスタリングを行う. これらの手法を使うことで, 外れ値に強い, ロバストなクラスタリング結果が得られる. # pam関数によるK-medoids法の実行 k &lt;- 3 # クラスタの数 res_pam &lt;- pam(tokuten, k) print(res_pam) #&gt; Medoids: #&gt; ID X4 X6 X5 X5.1 X4.1 #&gt; 黄 21 5 6 4 4 7 #&gt; 馬 5 7 7 8 6 9 #&gt; スミス 14 2 2 3 4 5 #&gt; Clustering vector: #&gt; 加藤 伊藤 山田 鈴木 馬 王 劉 趙 呉 尹 佐藤 #&gt; 1 2 1 2 2 2 1 1 1 2 1 #&gt; ルイス 徐 スミス 木村 大野 ロペス 陳 李 張 黄 朴 #&gt; 1 2 3 2 1 3 2 1 3 1 3 #&gt; 池田 藤田 周 ハリス 田中 朱 楊 #&gt; 3 1 3 1 1 3 2 #&gt; Objective function: #&gt; build swap #&gt; 3.17531 3.17531 #&gt; #&gt; Available components: #&gt; [1] &quot;medoids&quot; &quot;id.med&quot; &quot;clustering&quot; &quot;objective&quot; &quot;isolation&quot; #&gt; [6] &quot;clusinfo&quot; &quot;silinfo&quot; &quot;diss&quot; &quot;call&quot; &quot;data&quot; plot(res_pam) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
